%Tipo de documento
    \documentclass{article}
%Temas de matematica
    %Usar expresiones matematica
    \usepackage{amsmath}
    %Encuadrar enunciados (teoremas, corolarios, lemas)
    \usepackage{mdframed}
    %Paquete para cambiar los estilos de teoremas.
    \usepackage{amsthm}
    %dibujar matrices
    \usepackage{tikz}
    \usetikzlibrary{matrix}
    %colorboxexs
    \usepackage{tcolorbox}
    \tcbuselibrary{theorems}
    %overset in aligned
    \usepackage{aligned-overset}
    %Images
    \usepackage{graphicx}
    %Tikz
    \usepackage{tikz}
    %Para usar mathbb
    \usepackage{amsfonts}
    %Alinear matrices
    \usepackage{mathtools}
    %enumitem
    \usepackage{enumitem}
    %Differential
    \usepackage[thinc]{esdiff}
    %color rows and columns in matrices 
    \usepackage{nicematrix}
    %\circ default item
    \setlist[itemize]{label=$\circ$}
%Temas de idioma
    % Set the font (output) encodings
    \usepackage[T1]{fontenc}
    % Spanish-specific commands
    \usepackage[spanish,es-lcroman]{babel}
    % Simbolos
    \usepackage{amssymb}
    %Fracciones extra
    \usepackage{nicefrac}
    \usepackage{xfrac}
    %includegraphics
\usepackage{graphicx} % to be able to use \includegraphics
%Usando mdframed para dar un contorno negro a los enunciados
    \theoremstyle{definition}
      \newmdtheoremenv{teo}{Teorema}
      \newmdtheoremenv{lema}{Lema}
      \newmdtheoremenv{defi}{Definición}
      \newmdtheoremenv{corol}{Corolario}
      \newmdtheoremenv{axi}{Axioma}
      \newmdtheoremenv{corol2}{Corolario}
%Para poder usar Enunciados
    %El estilo dentro del los enunciados (de letra, espaciado, etc)
    \theoremstyle{definition}
    %Todos estos estan con el \theoremstyle{definition}
    \newtheorem*{obs}{Observación}
    \newtheorem{prop}[teo]{Proposición}
    \newtheorem*{ej}{Por ejemplo}
    \newtheorem*{notacion}{Notación}
    %
    \theoremstyle{remark}
    \newtheorem*{demo}{Demostración}

%Estilos de letra
    % \textbf{ }               Negrita.
    % \underline{ }            Subrayado.
    % \textit{ }               Cursiva.
%Enunciados: Teoremas, lemas, corolarios.
    % \begin{theorem}[ ]       Teorema. [nombre del teorema](opcional)
    % \begin{corollary}[ ]     Corolario
% These are the fancy headers
    \usepackage{fancyhdr}
    \pagestyle{fancy}
    % LE: left even
    % RO: right odd
    % CE, CO: center even, center odd
    \fancyhead[R]{\today}%Numero de clase} % Right odd,  Left even
    \fancyfoot[R]{\thepage}  % Right odd,  Left even
    \fancyfoot[C]{\leftmark}     % Center
    \makeatother
%No indentacion al iniciar un parrafo.
\setlength{\parindent}{0cm}
%Shortcuts para simbolos de matematica
    \newcommand\N{\ensuremath{\mathbb{N}}}
    \newcommand\R{\ensuremath{\mathbb{R}}}
    \newcommand\Z{\ensuremath{\mathbb{Z}}}
    \renewcommand\O{\ensuremath{\emptyset}}
    \newcommand\Q{\ensuremath{\mathbb{Q}}}
    \newcommand\C{\ensuremath{\mathbb{C}}}
    \newcommand\fun{$f$\;}
    \newcommand\I{$I$\;}
    \newcommand\e{$\in$\:}
    \newcommand\bl{$\bullet\;$}
    \newcommand\U{\cup}
    \newcommand\ok{\checkmark}
    \newcommand\qtilde{\overset{\sim}{q}}
    \newcommand\ptilde{\overset{\sim}{p}}
    \newcommand\infi{\infty}
%Imagenes con inkscape
    \usepackage{graphicx}
    \graphicspath{{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras}}    %Folder en el mismo directorio que .tex donde estan las imagenes
    %Para escalar la imagen
    %Para omitir warnings boludas
   %Para meter el archivo con latex adentro de golpe
    %\input{<filename>.pdf_tex}
    %Paquetes para que ande el tema este
    \usepackage{color}
    \usepackage{import}
    %iz y der imagen
    \usepackage{subfigure}
%Algunos colores
    \usepackage{xcolor}
    %Colores pastel
    \definecolor{verdep}{RGB}{61, 184, 143}
    \definecolor{rojop}{RGB}{227, 93, 133}
    \definecolor{azulp}{RGB}{22, 69, 122}
    %v2
    \definecolor{verdep2}{RGB}{78, 204, 190}
    \definecolor{rojop2}{RGB}{225, 131, 48}
    \definecolor{azulp2}{RGB}{36, 115, 171}
    %v2prima
    \definecolor{azulp2prima}{RGB}{150,186,255}
    \definecolor{rojop2prima}{RGB}{225,131,140}
%Mostrar toda 'math' en grande.
    \everymath{\displaystyle} 
%Para graficar 
    \usepackage{pgfplots}
    \def\FunctionF(#1){(#1)^3- 3*(#1)}%
    \decimalpoint %2.5 se queda con el punto y no con la comma
\usepackage{transparent}
%Extensiones to amsmath package
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother
%Bordermatrix with brackets redefinitions
\usepackage{etoolbox}
\let\bbordermatrix\bordermatrix
\patchcmd{\bbordermatrix}{8.75}{4.75}{}{}
\patchcmd{\bbordermatrix}{\left(}{\left[}{}{}
\patchcmd{\bbordermatrix}{\right)}{\right]}{}{}
%Rom in text
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{document}

  \section{Números complejos.}
  En este archivo introduciremos el conjunto $\mathbb{C}$ de numeros complejos junto a sus operaciones de suma y multiplicacion. Ademas, \\\\
  \bl Definiremos los conceptos de ``conjugado'', ``argumento'' y ``modulo'' de un numero complejo; \\
  \bl Aprenderemos a calcular el inverso de un numero complejo; \\ 
  \bl Veremos como representar graficamente los numeros complejos.
\\\\
¿La ecuacion $x^2+1=0$ tiene solucion? \\\\
En $\mathbb{R}$ no: $x^2 \geq 0$ y por lo tanto $x^2+1>0$.\\\\

Podemos extender $\mathbb{R}$ a otro cuerpo, de tal forma que \emph{toda} ecuacion polinomica con coeficientes en $\mathbb{R}$ tenga solucion.

\begin{defi}
  Los numeros complejos es el conjunto $\mathbb{C}$ de los pares ordenados $(a,b)$ denotados $a+ib$, con $a,b$ en $\mathbb{R}$, con las operaciones $`+`$ y $`-`$, definidas\\
  $$\begin{array}{llll}
    (a+ib)+(c+id) := (a+c) + i(b+d), & & (1) \\
    (a+ib) \phantom{\cdot}\cdot\phantom{\cdot}(c+id) := (ac-bd)+i(ad+bc). & & (2)
  \end{array}$$
\end{defi}
\bl La definicion de la suma de los numeros complejos es ``coordenada a coordenada''. \\
\bl Por la definicion de producto: \[
i^2 = (0+i \cdot 1)(0+i \cdot 1) = (0 \cdot 0 - 1 \cdot 1 ) + i(0 \cdot 1 + 1 \cdot 0) = -1.
\]
Luego, $i^2=-1$ e $i$ es la solucion de la ecuacion polinomica $x^2+1=0$.
\\\\
\bl Sabiendo que $i^2=-1$ y la propiedad distributiva, no necesitamos memorizar la formula del producto:  
  \begin{alignat*}{2}
    (a+ib)(c+id) &= ac+iad &&+ ibc+i^2bd \\
                 &= ac+iad &&+ ibc-bd \\
                 &= (ac - bc) &&+ i(ad+bc).
  \end{alignat*}
\bl Al numero complejo $i=0+1\cdot i$ lo llamamos el imaginario puro.\\\\
\bl Si $z=a+ib$ es un numero complejo, diremos que $a$ es la parte real de $z$ y la denotamos $a=\mathfrak{Re}z$. Por otro lado, $b$ es la parte imaginaria de $z$ que es denotada $b=\mathfrak{Im}z$.\\\\
\bl Es claro que \[
a+bi=c+di \quad \Leftrightarrow \quad a=c \; \land \; b=d.
\]
\bl $\mathbb{R} \in \mathbb{C}$, con la correspondencia $a \to a+i\cdot 0$ y observamos que si nos restringimos a $\mathbb{R}$, tenemos las reglas de adicion y multiplicacion usuales.
\\\\
Con la definicion de suma y producto los numeros complejos forman un anillo conmutativo con $1$.\\\\ Es decir, cumplen todas las propiedades de los numeros enteros, salvo las de orden.
\\\\ En paraticular la suma y el producto son conmutativos, asociativos, se cumplen las propiedades distributivas, etc.
\\\\ Mas aun, como veremos mas delante, $\mathbb{C}$ es un cuerpo (como lo es $\mathbb{R}$): es decir todo numero complejo no nulo tiene un inverso multiplicativo.\\\\ 
Simbolicamente \[
  z \neq 0 \quad \Rightarrow \quad \exists w \text{ tal que } zw=1.
\]
($w$ se denota $z^{-1}$).
\\\\
\bl La definicion de la suma de dos numeros complejos es ``coordenada a coordenada''.\\\\
\bl $0=0+i\cdot0 \in \mathbb{C}$, es el elemento neutro de la suma. \[
  (a+ib)+(0+i\cdot 0)=(a+0)+i \cdot (b+0)=a+i\cdot b.
\]
\bl Si $z=a+ib,$ entonces $-z= -a-ib$ es el opuesto aditivo de $z$. \[
  (a+ib)+(-a-i\cdot b)=(a-a)+i\cdot(b-b)=0+i\cdot 0.
\]
\bl $1=1+i\cdot 0 \in \mathbb{C}$, es el elemento neutro del producto. \[
  (1+i\cdot 0)(a+ib) = (1\cdot a - 0 \cdot b) + i (0 \cdot a + 1 \cdot b) = a+ib. \pagebreak
\] \begin{center}
\textbf{Representacion grafica de los numeros complejos.}
\end{center}
Hemos definido los numeros complejos como pares ordenados y como tales es posible representarlos en el plano $\mathbb{R} \times \mathbb{R}$:
\begin{figure}[h]
\centering
\def\svgwidth{0.45\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v1.pdf_tex}
\end{figure}\\
Representemos graficamente los numeros $2+i1,-1+i 2.5$ y $-2.5-i2.5$:
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v2.pdf_tex}
\end{figure}\\
Con esta representacion grafica la definicion de la suma de dos numeros complejos es la suma ``coordenada a coordenada''. \\

\pagebreak
Ejemplificamos la suma de numeros complejos en el plano:\\
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v3.pdf_tex}
\end{figure}\\
 \begin{center}
\textbf{Modulo y conjugado de un numero complejo.}
\end{center}
\begin{defi}
  Sea $z=a+ib \in \mathbb{C}$. \\\\ $\begin{array}{llll}
    \bullet \text{ El modulo de $z$ es} & |z|=\sqrt{a^2+b^2}. \\
    \bullet \text{ El conjugado de $z$ es} & \bar{z}=a-ib.
\end{array}$
\end{defi}
Graficamente:
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v4.pdf_tex}
\end{figure}\\
\begin{prop}
  Sean $z$ y $w$ numeros complejos. Entonces \\\\
  1. $|z|=0 \Leftrightarrow z=0$.\\
  2.  $z\bar{z}=|z|^2$.\\
  3. $\overline{z+w}= \bar{z}+\bar{w}$. \\
  4. $\overline{zw}=\bar{z}\bar{w}$.
\end{prop}
\begin{demo} \; \\
  1. Si $z=a+ib$, \[
\begin{aligned}
\;|z|=0 &\Leftrightarrow \sqrt{a^2+b^2} \Leftrightarrow a^2+b^2 \Leftrightarrow a^2=0 \land b^2 =0 \\
& \Leftrightarrow a=0 \land b=0 \Leftrightarrow z=0.
\end{aligned}
  \]
  2. Si $z=a+ib$, entonces $\bar{z}=a-ib$, y \[
z\bar{z}=(a+ib)(a-ib)=a^2+iab-iba-i^2b^2=a^2-(-1)b^2=a^2+b^2.
  \]
  3. Si $z=a+ib$ y $w=c+id$, entonces \[\begin{aligned}
\overline{z+w}&=\overline{(a+c)+i(b+d)}&=(a+c)-i(b+d),\\
\bar{z}+\bar{w}&=a-ib\phantom{)}+c-id&=(a+c)-i(b+d).
\end{aligned}
  \]
  Por lo tanto $\overline{z+w}=\bar{z}+\bar{w}$.\\\\
  4. Si $z=a+ib$ y $w=c+id$, entonces \[\begin{aligned}
\overline{zw}&=\overline{(a+ib)(c+id)}=\overline{(ac-bd)+i(ad+bc)}=(ac-bd)-i(ad+bc),\\
\bar{z}\bar{w}&=(a-ib)(c-id)=(ac-bd)-i(ad+bc).\end{aligned}
  \]
  Por lo tanto $\overline{zw}=\bar{z}\bar{w}$. \qed
\end{demo}
\begin{center}
\textbf{Inverso de un numero complejo.}
\end{center}
\begin{prop}
  Sea $z$ un numero complejo no nulo. Entonces, $z^{-1}=\frac{\bar{z}}{|z|^2}$.
\end{prop}
\begin{demo}
  Ya vimos que $z\bar{z}=|z|^2$. Como $z \neq 0$, tenemos que $|z|\neq 0$, luego \[
z\frac{\bar{z}}{|z|^2}=\frac{z\bar{z}}{|z|^2}=\frac{|z|^2}{|z|^2}=1.
  \]\qed
\end{demo}
\begin{obs}
  Multiplicar por un numero real es ``coordenada a coordenada'', luego si $z=a+ib$, tenemos que $z^{-1}=\frac{1}{|z|^2}\bar{z}$, es decir\[
(a+ib)^{-1}=\frac{a}{a^2+b^2}-i\frac{b}{a^2+b^2}.
  \]
\end{obs}
\begin{ej}
  Escribir el inverso de $-1+2i$ en la forma $a+bi$.
\end{ej}
Primero averiguamos el modulo al cuadrado: \[
|-1+2i|^2=(-1)^2+2^2=1+4+5.
\]
Como $\overline{-1+2i}=-1-2i$,\[
(-1+2i)^{-1}=-\frac{2}{5}-i\frac{2}{5}.
\]
Nunca esta de mas comprobar el resultado:
\[
(-1+2i)\left(-\frac{1}{5}-i\frac{2}{5}\right)=\frac{1}{5}+\frac{2}{5}i-\frac{2}{5}i+\frac{4}{5}=\frac{1}{5}+\frac{4}{5}=1
\] \qed
\begin{center}
\textbf{Soluciones de ecuaciones polinomiales a coeficientes en $\mathbb{C}$.}
\end{center}
Extendiendo los numeros reales a los complejos, encontramos un coinjunto de numeros en donde la ecuacion $x^2+1=0$ tiene solucion. \\\\
¿Tendremos que extender $\mathbb{R}$ aun mas para encontrar soluciones a ecuaciones polinomiales mas complejas? \\\\
La respuesta es: \begin{teo}[Teorema fundamental del algebra]\;\\
  La ecuacion polinomica \[
x^n+a_{n-1}x^{n-1}+\cdots+a_2x^2+a_1x+a_0=0,
  \]con $a_i \in \mathbb{C}\; (0 \leq i \leq n-1)$ tiene solucion en $\mathbb{C}$.
\end{teo}
\section{Vectores en $\mathbb{R}^2$ y $\mathbb{R}^3$.}
\begin{center}
\textbf{Algebra lineal en $\mathbb{R}^2$ y $\mathbb{R}^3$.}
\end{center}
Sabemos que se puede usar un numero para presentar un punto en una linea, una vez que se selecciona la longitud de una unidad:
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v5.pdf_tex}
\end{figure}\\
Se puede usar un par de numeros $(x,y)$ para representar un punto en el plano.
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v2.pdf_tex}
\end{figure} \pagebreak

Ahora observamos que un triple de numeros $(x,y,z)$ se puede usar para arepresentar un punto en el espacio.
\begin{figure}[h]
\centering
\def\svgwidth{0.5\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v6.pdf_tex}
\end{figure}\\
En lugar de usar $(x,y,z)$, tambien suele usarse la notacion $(x_1,x_2,x_3)$.
  \begin{defi}
    Sea $\mathbb{R}$ el cuerpo de los numeros reales, entonces \[
      \mathbb{R}^n:=\big\{(x_1,x_2,\dots, x_n):x_i \in \mathbb{R},1\leq i \leq n\big\}.
    \]
    Todo $v$ en $\mathbb{R}^n$ sera llamado punto. Alternativamente, tambien podemos decir que $v$ es un vector en el origen o simplemente un vector.
  \end{defi}
La mayoria de nuestros ejemplos tendran lugar cuando $n=2$ o $n=3$.\\\\
 Para ello usaremos el sistema de coordenadas cartesianas para representar los elementos de $\mathbb{R}^2$ y $\mathbb{R}^3$.
\begin{ej}
  El ministerio de economia quiere representar la inversion anual en $6$ ramas de la industria: 1. acero, 2. automotriz, 3. productos agricolas, 4. productos quimicos, 5. indumentaria y 6. transporte.
\end{ej}
Se puede representar esta situacion por una 6-upla donde caada coordenada representa la inversion anual de las industrias correspondientes. \\\\ 
Por ejemplo, si la 6-upla correspondiente al año 2019 es \[
  (1200,700,600,300,900,250),
\]
significa que la industria del acero invirtio 1200 en ese año, la automotriz 700, etc.
\\\\
Recordemos que a los numeros complejos se los puede representar en el plano y que la suma es coordenada a coordenada.
\\\\
En el ejemplo anterior veamos que tambien es natural definir la suma coordenada a coordenada.\\\\
Por ejemplo si las inversiones en los años 2018 y 2019 fueron
\[
  \begin{array}{llll}
    2018 & \to & (1000,800,550,300,700,200) \\
    2019 & \to & (1200,700,600,300,900,250)
  \end{array}
\]
Las inversiones totales, por rubro, en los dos años fueron:\[
(1000,800,550,300,700,200)+(1200,700,600,300,900,250)
\]
$=(1000+1200,800+700,550+600,300+300,700+900,200+250)$\\
$=(2200,1500,1350,600,1600,450).$
\begin{center}
  \textbf{Suma en $\mathbb{R}^n$.}
\end{center}
\begin{defi}
  Si $(x_1,\dots,x_n),(y_1,\dots,y_n)\in \mathbb{R}^n$, definimos \[
    (x_1,\dots,x_n)+(y_1,\dots,y_n)=(x_1,+y_1,\dots,x_n+y_n),
  \] es decir sumamos ``coordenada a coordenada''.
\end{defi}
\begin{ej}
  En $\mathbb{R}^5$ tenemos que \[
    \begin{aligned}
      (1,2,3,4,5)+(6,7,8,9,0)&=(1+6,2+7,3+8,4+9,5+0) \\
                             &=(7,9,11,13,5).
    \end{aligned}
  \]
\end{ej}
\textbf{Propiedades:}\\\\
La suma de vectores en $\mathbb{R}^n$ satisface que \\\\
\textcolor{azulp2}{1.} Es asociativa: \[
  u+(v+w)=(u+v)+w\quad \forall u,v,w, \in \mathbb{R}^n
\]
\textcolor{azulp2}{2.} Es conmutativa: \[
  v+w=w+v \quad \forall v,w \in \mathbb{R}^n
\]
\textcolor{azulp2}{3.} El vector $\textcolor{rojop2}{0}:=(0,\dots,0)$, es el elemento \textcolor{rojop2}{neutro}:
\[
  v+0=0+v=v \quad \forall v \in \mathbb{R}^n
\]
\textcolor{azulp2}{4.} El vector \textcolor{rojop2}{$-v:=(-x_1,\dots,-x_n)$} es el \textcolor{rojop2}{opuesto} de $v=(x_1,\dots,x_n)$:\[
v+(-v)=(-v+v)=0.
\]
Estas propiedades son consecuencias de las propiedades analogas de la suma de numeros reales. Pues la suma de vectores es coordenada a coordenada y las coordenadas son numeros reales.
\\\\
Por ejemplo, si $u=(u_1,\dots,u_n), v=(v_1,\dots,v_n),w=(w_1,\dots,w_n)$ \[
  \begin{aligned}
    u+(v+w)&=(u_1,\dots,u_n)+\big((v_1,\dots,v_n)+(w_1,\dots,w_n)\big) \\
           &=(u_1,\dots,u_n)+(v_1,+w_1,\dots,v_n+w_n) \\
           &= \Big(u_1+(v_1+w_1),\dots,u_n+(v_n+w_n)\Big)\\
           &= \Big((u_1+v_1)+w_1,\dots,(u_n+v_n)+w_n\Big) \\
           &=(u_1+v_1,\dots,u_n+v_n)+(w_1,\dots,w_n) \\
           &=\Big((u_1,\dots,u_n)+(v_1,\dots,v_n)\Big)+(w_1,\dots,w_n)\\
           &=(u+v)+w
  \end{aligned}
\]
\begin{center}
\textbf{Ley del paralelogramo.}
\end{center}
Sea $v=(2,3)$ y $w=(-1,1)$. Entonces $v+w=(1,4)$. En el dibujo de los puntos involucrados aparece un paralelogrmo.
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v7.pdf_tex}
\end{figure}\\ \pagebreak \\
En general, la suma de dos vectores se puede representar geometricamente con la ley del paralelogramo: 
\begin{figure}[h]
\centering
\def\svgwidth{0.45\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v8.pdf_tex}
\end{figure}\\
\begin{center}
\textbf{El opuesto de un vector.}
\end{center}
El opuesto de un vector $v$ en el plano es $-v$ y geometricamente es el vectore reflejado respecto al centro.
\begin{figure}[h]
\centering
\def\svgwidth{0.45\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v9.pdf_tex}
\end{figure}\\ \pagebreak
\begin{center}
\textbf{La resta de vectores.}
\end{center}
Dados dos vectores $v,w$ en el plano, podemos representar la resta como la suma $v+(-w)=v-w$.\\\\ 
Como $(v-w)+w=v$, la ley del paralelogramo tambien nos sirve para visualizar la resta.
\begin{figure}[h]
\centering
\def\svgwidth{0.45\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v10.pdf_tex}
\end{figure}\\
\begin{center}
\textbf{Producto de un vector por un escalar.}
\end{center}
\begin{ej}
  Sea $v=(1,2)$, podemos representar los ``multiplos'' de $v$ en forma natural: 
\end{ej}
\begin{figure}[h]
\centering
\def\svgwidth{0.45\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v11.pdf_tex}
\end{figure}
\begin{defi}
  Sea $v=(x_1,\dots,x_n) \in \mathbb{R}^n$ y $\lambda \in \mathbb{R}$, entonces \[
\lambda \cdot v := (\lambda x_1,\dots,\lambda x_n).
  \] Tambien denotmos a esta multiplicacion $\lambda v.$
\end{defi}
\begin{ej}
  Si $v=(2,-1.5)$ y $\lambda = 7$, entonces $\lambda v=(14,-10.5).$
\end{ej}
\pagebreak
\textbf{Propiedades:}\\\\
La multiplicacion por escalares satisface que \\\\
\textcolor{azulp2}{1.} Es asociativa \[
  (\lambda \mu)v=\lambda(\mu v) \quad \forall v \in \mathbb{R}^n, \forall \lambda,\mu \in \mathbb{R}.
\]
\textcolor{azulp2}{2.} Es distributiva\[
  \begin{aligned}
    \lambda(v+w)=\lambda v+\lambda w \quad &\forall v,w \in \mathbb{R}^n, \lambda \in \mathbb{R} \\
    (\lambda+\mu)v=\lambda v+\mu v \quad & \forall v \in \mathbb{R}^n, \lambda,\mu \in \mathbb{R}.
  \end{aligned}
\]
Al igual que las propiedades de la suma, estas tambien se deducen de las propiedades de los numeros.\\
Similarmente, multiplicando por $(-1)$ obtenemos el opuesto: \[
  (-1)v=-v\quad \forall v \in \mathbb{R}^n.
\]
\begin{defi}
  Dado $i \in \{1,\dots,n\}$, se denota $e_i \in \mathbb{R}^n$ al vector cuyas coordenadas son todas $0$ excepto la coordenada $i$ que es un $1$. \[
e_i=(0,\dots,1,\dots,0)
  \]
  El Conjunto $\{e_1,\dots,e_n\}$ se llama base canonica de $\mathbb{R}^n$. 
\end{defi}
\begin{ej}
  En $\mathbb{R}^3$ los vectores son $e_1=(1,0,0),e_2=(0,1,0),e_3=(0,0,1).$\\\\ Estos vectores jugaran un rol central en la materia. \\\\ Principalmente por la siguiente propiedad.
\end{ej}
\textbf{Propiedad:}\\\\
Todo vector de $\mathbb{R}^n$ se escribe como combinacion lineal de la base canonica. Explicitmente, si $(x_1,\dots,x_n) \in \mathbb{R}^n$ entonces \[
  (x_1,\dots,x_n)=x_1e_1+x_2e_2+\cdots+x_ne_n
\]
\begin{ej}
  \[
    \begin{aligned}
(1,2,3) &= (1,0,0)+(0,2,0)+(0,0,3) \\
        &= 1(1,0,0)+2(0,1,0)+3(0,0,1)\\
        &=1e_1+2e_2+3e_3
    \end{aligned}
  \]
\end{ej}\pagebreak
\textbf{Problema:}
  Sea $B=\{e_1,e_2,e_3\}$ la base canonica de $\mathbb{R}^3$, entonces \mbox{$(x_1,x_2,x_3)=x_1e_1+x_2e_2+x_3e_3$}.
\begin{demo} \;
$e_1=(1,0,0), \quad e_2=(0,1,0), \quad e_3=(0,0,1)$\\\\
\[
  \begin{aligned}
    x_1e_1+x_2e_2+x_3e_3 &=x_1\cdot(1,0,0)&&+x_2\cdot(0,1,0)&&+x3\cdot(0,0,1) \\
                         &= (x_1\cdot 1, x_1 \cdot 0, x_1 \cdot 0) &&+ (x_2 \cdot 0, x_2 \cdot 1 , x_3 \cdot 0 ) &&+ (x_3 \cdot 0, x_3 \cdot 0, x_3\cdot 1)\\
                         &= (x_1,0,0)&&+(0,x_2,0)&&+(0,0,x_3)\\
                         &= (x_1,x_2,x_3).
\end{aligned}\]\qed
\end{demo}
\begin{center}
\textbf{Producto escalar.}
\end{center}
En 2-espacios, daados dos vectores $v=(x_1,x_2)$ y $w=(y_1,y_2)$, definimos su producto escalar como \[
\langle v,w \rangle := x_1y_1+x_2y_2.
\]
Para el caso de 3-espacios, sean $v=(x_1,x_2,x_3)$ y $w=(y_1,y_2,y_3)$, entonces el producto escalar de $v$ y $w$ es \[
\langle v,w \rangle := x_1y_1 + x_2y_2 + x_3y_3.
\]
Finalmente, en los n-espacios, generalizamos la definicion de la manera obvia: sean $v=(x_1,\dots,x_n)$ y $w=(y_1,\dots,y_n$), definimos el producto escalar de $v$ y $w$ por \[
\langle v,w \rangle := x_1y_1 + x_2y_2 + \cdots + x_ny_n.
\]
Es importante notar que este ``producto'' es un numero real. \begin{ej}
  Si \[
    v=(1,3,-2) \quad \text{y} \quad w=(-1,4,-3),
  \]entonces
  \[
    \langle v,w \rangle = 1\cdot(-1)+3\cdot4+(-2)\cdot(-3)=-1+12+6=17.
  \]
\end{ej}
En algunos libros de texto se denota al producto escalar como $v \cdot w$. \\\\ Por el momento, no le damos una interpretacion geometrica al producto escalar y veremos esto cuando veamos la norma de un vector. \pagebreak
\begin{center}
\textbf{Propiedades basicas del producto escalar.}
\end{center}
Las siguientes propiedades son basicas y muy importantes.\\\\
Sean $v,w,u$ tres vectores, entonces \\\\
\textcolor{azulp2}{P1.} $\langle v,w \rangle = \langle w,v \rangle.$\\\\
\textcolor{azulp2}{P2.} $\langle v,w+u\rangle = \langle v,w \rangle + \langle v,u \rangle$ y \\
\phantom{P2.} $\langle v+w,u \rangle = \langle v,u \rangle + \langle w,u \rangle$.\\\\
\textcolor{azulp2}{P3.} Si $\lambda$ es un numero, entonces \[
  \langle \lambda v ,w \rangle = \lambda \langle v,w \rangle \quad \text{y} \quad \langle v, \lambda w \rangle = \lambda \langle v,w \rangle.
\]
\textcolor{azulp2}{P4.} Si $v=0$, entonces $\langle v,v \rangle = 0$, de lo contrario \[
\langle v,v \rangle > 0.
\]
\begin{demo} \textcolor{rojop2}{P1.}\\\\
  Sean $v=(x_1,x_2,\dots,x_n)$ y $w=(y_1,y_2,\dots ,y_n)$. Tenemos que \[
    \begin{aligned}
      \langle v,w \rangle = x_1y_1+x_2y_2+\cdots+x_ny_n.\\
      \langle w,v \rangle = y_1x_1+y_2x_2+\cdots+y_nx_n.\\
    \end{aligned}
  \]
\end{demo}
Como en $\mathbb{R}$ vale que para cualquier par de numeros $x,y$, se cumple que $xy=yx$, obviamente ambas expresiones son iguales. $\qed$
\begin{demo}
\textcolor{rojop2}{P2.} \\\\
Sea $u=(z_1,\dots,z_n).$ Entonces \[
w+u=(y_1,+z_1,\dots,y_n+z_n)
\]
y
\[
\begin{aligned}
  \langle v,w+u \rangle &= \big\langle (x_1,\dots,x_n),(y_1+z_1,\dots,y_n+z_n) \big\rangle \\
                        &= x_1(y_1+z_1)+\cdots + x_n(y_n+z_n) \\ 
                        &= x_1y_1+x_1z_1+\dots+x_ny_n+x_nz_n.
\end{aligned}
\]
Reordenando los teminos obtenemos \[
\langle v,w+u \rangle = x_1,y_1+ \cdots + x_ny_n + x_1z_1 + \cdots + x_n z_n,
\]
que no es otra cosa que $\langle v,w\rangle + \langle v,u \rangle$. \qed
\end{demo}
\begin{demo}
\textcolor{rojop2}{P3.} Como ejercicio.
\end{demo}
\begin{demo}
  \textcolor{rojop2}{P4.} \\\\
  Observemos que \[
    \langle v,v \rangle = x^2_1+x^2_2+\cdots + x^2_n. \quad \quad \quad \textcolor{rojop2}{(1)}
  \]
  Como $x^2_i \geq 0$ para todo $i$, entonces $\langle v,v \rangle \geq 0$. \\\\ Ademas, es claro que si $v$ tiene todas las coordenadas iguales a $0$, entonces $\langle v,v \rangle = 0$. \\\\ En el caso que $v \neq 0$, entonces, existe algun $i$ tal que $x_i \neq 0$, por lo tanto $x^2_i>0$ y por la ecuacion $\textcolor{rojop2}{(1)}$, tenemos que $\langle v,v \rangle > 0$. \qed
\end{demo}
\begin{center}
\textbf{La norma de un vector.}
\end{center}
\begin{defi}
  Si $v \in \mathbb{R}^n$, la norma de $v$ o longitud de $v$ es \[
    ||v||=\sqrt{\langle v,v \rangle}.
  \]
\end{defi}
Si $(x,y) \in \mathbb{R}^2$, entonces $||(x,y)||=\sqrt{x^2+y^2}$ (teorema de Pitagoras).
\begin{figure}[h]
\centering
\def\svgwidth{0.35\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v12.pdf_tex}
\end{figure}
\\ \pagebreak \\
Si $n=3$, por la aplicacion reiterada del teorema de Pitagoras obtenemos que la longitud de $v=(x,y,z)$ es $\sqrt{x^2+y^2+z^2}$.
\begin{figure}[h]
\centering
\def\svgwidth{0.35\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v13.pdf_tex}
\end{figure}
\\
En general, si $v=(x_1,x_2,\dots,x_n) \in \mathbb{R}^n \Rightarrow ||v|| = \sqrt{x^2_1+x^2_2+\dots+x^2_n}$.
\begin{prop}
  Sea $v \in \mathbb{R}^n$ y $\lambda \in \mathbb{R}$, entonces \[
||\lambda v || = |\lambda| \; ||v||.
  \]
\end{prop}
\begin{demo} \;\\\\
  $||\lambda v||^2=\langle \lambda v,\lambda v \rangle$ por \textcolor{azulp2}{P3},\[
\langle \lambda v,\lambda v \rangle =\lambda \langle v , \lambda v \rangle = \lambda^2 \langle v , v \rangle.
  \] Es decir $||\lambda v ||^2 =\lambda^2 ||v||^2$, por lo tanto, $||\lambda v || = |\lambda| \; ||v||. \qed$
\end{demo}
\begin{center}
\textbf{Distancia en $\mathbb{R}^n$.}
\end{center}
\begin{defi}
  Sea $v,w \in \mathbb{R}^n$, entonces las distancia entre $v$ y $w$ es $||v-w||$.
\end{defi}
\begin{obs}
  La norma del vector $v-w$ es la longitud del segmento que une $w$ con $v$.
\end{obs}
\begin{figure}[h]
\centering
\def\svgwidth{0.35\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v14.pdf_tex}
\end{figure}
 \pagebreak 
\begin{center}
  \textbf{Interpretacion geometrica del producto escalar.}
\end{center}
Sean $v_1=(x_1,y_1)$ y $v_2=(x_2,y_2)$ en $\mathbb{R}^2$; veremos a continuacion que \[
\langle v_1,v_2 \rangle = ||v_1|| \; ||v_2|| \; \cos(\theta),
\] donde $\theta$ es el angulo comprendido entre $v_1$ y $v_2$.
\\\\
Sea $\alpha_1$ el angulo comprendido entre $v_1$ y el eje horizontal y $a_2$ el angulo comprendido entre $v_2$ y el eje horizontal. Entonces \[
v_1=||v_1|| \; \big(cos(\alpha_1),sen(\alpha_1)\big), \quad v_2=||v_2||\; \big(cos(\alpha_2),sen(\alpha_2)\big),
\]
por lo tanto \[
\langle v_1,v_2 \rangle = ||v_1|| \; ||v_2|| \; \big(cos(\alpha_1)cos(\alpha_2)+sen(\alpha_1)sen(\alpha_2)\big).
\]
Por otro lado, por la propiedad de la suma de los cosenos tenemos que \[
cos(\alpha_1)cos(\alpha_2)+sen(\alpha_1)sen(\alpha_2)=cos(\alpha_1-\alpha_2).
\]
Es decir, \[
\langle v_1,v_2 \rangle = ||v_1|| \; ||v_2|| \; cos(\alpha_1-\alpha_2),
\]
y precisamente, $\theta=\alpha_1-\alpha_2$ es el angulo comprendido entre $v_1$ y $v_2$.\\\\ Esto se puede generalizar a $\mathbb{R}^n$: el angulo comprendido entre $v_1$ y $v_2$ es \[
  \theta = arcos\left(\frac{\langle v_1,v_2 \rangle}{||v_1|| \; ||v_2||}\right).
\]
\begin{center}
  \textbf{Vectores perpendiculares.}
\end{center}
El producto escalar $\langle v,w \rangle$ puede ser igual a $0$ para determinados vectores, incluso ambos distintos de $0$.\\\\
Por ejemplo, si $v=(1,2,3)$ y $w=(2,1,-\frac{4}{3})$, entonces \[
\langle v,w \rangle = 2+2-4=0.
\]
\begin{defi}
Decimos que dos vectores $v$ y $w$ en $\mathbb{R}^n$ sonm perpendiculares u ortogonales si $ \langle v,w \rangle =0$. Cuando $v$ y $w$ son orgogonales denotamos $v \perp w$.
\end{defi}\pagebreak
\begin{ej}
  En $\mathbb{R}^2$ consideremos los vectores \[
v=(3,1), \quad w=(-1,3),
 \]
\end{ej}
 representados en la siguiente figura: \\\\
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v15.pdf_tex}
\end{figure}\\\\
Luego, vemos que $\langle v,w \rangle = 0$, y por lo tanto $v$ es perpendicular a $w$, lo cual concuerda con nuestra intuicion.
\\\\
Es claro que esta definicion algebraica de perpendicularidad esta de acuerdo con la interpretacion geoemetrica del producto escalar: \[
  \begin{array}{c}
  v \text{ y } w \text{ perpendiculares (geometricamente) } \\ \Downarrow \\
\text{ el angulo comprendido entre } v \text{ y } w \text{ es } \theta=90^{\circ}
\\
\Downarrow 
\\
cos(\theta)=0 \\
\Downarrow 
\\
\langle v,w \rangle = ||v|| \; ||w|| \; cos(\theta) = 0.
\end{array}
\]
\pagebreak
\begin{center}
  \textbf{Rectas en $\mathbb{R}^2$.}
\end{center}
\begin{defi}
  Una recta esta formada por el conjunto de puntos $(x,y)$ en $\mathbb{R}^2$ que satisface la ecuacion \[
ax+by=c,
\] con $a,b,c \in \mathbb{R}$ y tal que $a,b$ no pueden ser simultaneamente $0$. Tambien suele decirse que esta ecuacion es la ecuacion implicita de la recta. \\\\ Mas formalmente, la recta es el conjunto \[
\big\{(x,y) \in \mathbb{R}^2 : ax+by=c \big\}.
\]
\end{defi}
\begin{obs} \; \\\\
  \bl Si $b \neq 0 $, entonces la recta es $ y = -\frac{a}{b}x+\frac{c}{b}=a'x+b'$, \\\\
  \bl Si $b=0$, entonces $a \neq 0$ y la recta es $x=\frac{c}{a}$.
\end{obs}
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v16.pdf_tex}
\end{figure}
Si consideramos el vector $(a,b)$ en $\mathbb{R}^2$, $c \in \mathbb{R}$ y $L$ la recta definida por los puntos $(x,y)$ tal que $ax+by=c$, entonces \[
  \begin{aligned}
    L &= \big\{(x,y) \in \mathbb{R}^2 : ax+by=c \big\} \\
      &= \big\{(x,y) \in \mathbb{R}^2 : \langle (x,y),(a,b) \rangle=c \big\}.
  \end{aligned}
\]
Ahora bien, consideremos $(x_0,y_0)$ un punto de la recta, entonces, obviamente tenemos que $\big\langle(x_0,y_0),(a,b)\big\rangle=c$.\\\\ Por lo tanto \[
  L =\big\{(x,y) \in \mathbb{R}^2 : \big\langle (x,y),(a,b)\big\rangle =\overbrace{\big\langle(x_0,y_0),(a,b) \big\rangle}^{c}\big\}.
\]
Por la propiedad \textbf{P2} del producto escalar, llegamos a la conclusion que \[
  L=\big\{(x,y) \in \mathbb{R}^2 : \langle (x,y) - (x_0,y_0),(a,b) \big\rangle =0\big\}.
\]
Sea $v_0=(x_0,y_0)$ y $v=(x,y)$, representemos graficamente la situacion: \\\\
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v17.pdf_tex}
\end{figure}\\

La recta $L$ es, entonces, la recta perpendicular a $(a,b)$ y que pasa por $(v_0)$. \\\\
\textbf{Conclusion.}\\\\
La ecuacion implicita de la recta $L$ perpendicular a $(a,b)$ y que pasa por $(x_0,y_0)$ es \[
  ax+by=\underbrace{\big\langle (x_0,y_0),(a,b) \big\rangle}_{c}.
\]
\begin{center}
\textbf{Definicion parametrica de la recta.}
\end{center}
\begin{defi}
  Sean $v,w \in \mathbb{R}^2$ tal que $w \neq 0$. Sea \[
    L=\{v+tw : t \in \mathbb{R} \}.
  \]
\end{defi}
Diremos entonces que $L$ es la recta que pasa por $v$ paralela a $w$.\\\\
Observemos que la recta $L$ esta dada por todos los puntos que se obtienen de la funcion \[
  X(t)=v+tw, \quad \text{ para } t \in \mathbb{R}.
\]
En el espacio $\mathbb{R}^2$, diremos que esta es la ecuacion parametrica o la representacion parametrica de la recta $L$ que pasa por el punto $v$ y es paralela a $w \neq 0$.\\
Podemos representar una recta dada en forma parametrica:
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v19.pdf_tex}
\end{figure}
Esta representacion parametrica tambien es util para describir el conjunto de los puntos que se encuentran en el segmento de linea entre dos puntos dados.\\\\
Sean $v,w$ dos puntos, entonces el segmento entre $v$ y $u$ consiste en todos los puntos \[
  S(t)=v+t(u-v) \quad \text{ con } 0 \leq t \leq 1.
\]
Observar que \\\\ \bl en tiempo $0$, $S(0)=v$, y \\\\ \bl en tiempo $1$, $S(1)=v+(u-v)=u$.\\\\
Como $t$ ``va'' de $0$ a $1$, el movil va de $v$ a $u$ en linea recta.\\\\

Extendiendo a ambos lados el segmento, podemos describir la recta que pasa por $v$ y $u$ por la ecuacion parametrica \[
  S(t)=v+t(u-v) \quad \text{ con } t \in \mathbb{R}.
\]
\begin{figure}[h]
\centering
\def\svgwidth{0.55\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v20.pdf_tex}
\end{figure}\\
\begin{ej}
  Encontrar una representacion parametrica para la recta que contiene los puntos $(1,-3,1)$ y $(-2,4,5)$.
\end{ej}
\textbf{Solucion}\\\\
Llamemos $v=(1,-3,1)$ y $u=(-2,4,5)$.
\\\\
Entonces $u-v=(-2,4,5)-(1,-3,1)=(-3,7,4)$ y la representacion parametrica de la recta que pasa por $u$ y $v$ es \[
X(t)=v+t(u-v)=(1,-3,1)+t(-3,7,4), \quad t \in \mathbb{R}.\]

\begin{center}
  \textbf{Ecuacion implicita y ecuacion parametrica de la recta.}
\end{center}
Ahora discutiremos la relacion entre una representacion parametrica y la ecuacion implicita de una recta en el plano.
\\\\ Sean $v,w \in \mathbb{R}^2$ con $w \neq 0$ y la recta descrita en forma parametrica: \[X(t)=v+tw.\]
Sea $v=(x_1,y_1), w=(x_2,y_2)$. Todo punto de la recta es de la forma \[
  (x,y)=(x_1,y_1)+t(x_2,y_2)=(x_1+tx_2,y_1+ty_2),
  \] equivalentemente \[
x=x_1+tx_2, \quad \quad y=y_1+ty_2,
\]para $t \in \mathbb{R}$
\\\\
Dado que $(x_2,y_2)\neq 0$, podemos despejar $t$ de alguna de las ecuaciones y usando la otra ecuacion eliminamos $t$.
\begin{ej} \;\\ Sea $L$ la recta conformada por el conjunto de puntos
  \[(x,y) : x=x_1+tx_2 \quad y=y_1+ty_2.\]
  Si $x_2 \neq 0$, $$t=\frac{x-x_1}{x_2} \quad \Rightarrow \quad y=y_1+\left(\frac{x-x_1}{x_2}\right)y_2.$$ Es decir, $$y=y_1+\frac{x}{x_2}y_2 - \frac{x_1}{x_2}y_2$$ 
  Multiplicando todo por $x_2$ queda $$x_2y=x_2y_1+xy_2-x_1y_2,$$
  que tiene la forma de una ecuacion implicita, es decir \[
    \underbrace{(-y_2)}_{a}x+\underbrace{x_2}_{b}y=\underbrace{x_2y_1-x_1y_2}_{c}.
  \]
\end{ej}
\pagebreak

\begin{ej}
  Sean $v=(2,1)$ y $w=(-1,5)$ y sea $X$ la recta que pasa por $v$ en la direccion $w$. Encontrar la ecuacion implicita de $X$. 
\end{ej}
La representacion parametrica de la recta que pasa por $v$ en la direccion de $w$ es \[
X(t)=(2,1)+t(-1,5)=(2-t,1+5t).
\]
Es decir, si miramos cada coordenada \[
x=2-t, \quad \quad y=1+5t.
\]
Despejando $t$ de la primera ecuacion obtenemos $t=2-x$.\\\\
Reemplazando este valor de $t$ en la segunda ecuacion obtenemos \[
y=1+5t=1+5(2-x)=y=11-5x,
\]
luego \[
5x+y=11,
\]
que es la ecuacion implicita de la recta.
\\\\
Reciprocamente, de la ecuacion implicita podemos obtener la representacion parametrica.
\begin{ej}
  Encontrar la representacion parametrica de la recta definida por \[
5x+2y=11.
  \]
\end{ej}
$$5x+2y=11 \quad \Rightarrow \quad 2y+11-5x \quad \Rightarrow \quad y = -\frac{5}{2}x+\frac{11}{2}.$$ \\\\
Reemplazando $x$ por $t$ (solo por notacion) obtenemos que \[
  Y(t)=\left(t,-\frac{5}{2}t+\frac{11}{2}\right)
\]
es la representacion parametrica de la recta. \pagebreak

\begin{center}
  \textbf{Planos en $\mathbb{R}^3$.}
\end{center}
Comenzaremos, debido a que es mas simple, con planos que pasan por el origen, como el de la siguiente figura.
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v21.pdf_tex}
\end{figure}
\\
En este caso, es claro que el plano esta determinado por un vector perpendicular al mismo.\\\\
Si $P$ es un plano que pasa por el origen y $u$ es un punto de $\mathbb{R}^3$, no nulo, tal que $u \perp P$, entonces \[
  P=\big\{v \in \mathbb{R}^3 : \langle v, u \rangle = 0\big\}.
\]
Sea ahora un plano $P$ que no pasa por el origen y $v_0 \in P$. \[
  P_0=\{v - v_0 : v \in P\}
\]
es un plano que pasa por el origen.
\\\\ Luego, si $u \perp P_0$, \[
  P_0=\big\{w : \langle w , u \rangle \big\}.
\]
De las ultimas dos ecuaciones deducimos que \[
v \in P \quad \Leftrightarrow \quad v - v_0 \in P_0 \quad \Leftrightarrow \quad \langle v - v_0, u \rangle = 0.
\]
Es decir \[
  P= \big\{v \in \mathbb{R}^3 : \langle v - v_0 , u \rangle = 0\big\}.
\]
Si $d= \langle v_0, u\rangle,$ \[
  P= \big\{v \in \mathbb{R}^3 : \langle vu \rangle = d\big\}.
\]
\begin{defi}
  Sean $a,b,c,d \in \mathbb{R}$ tal que $(a,b,c) \neq (0,0,0)$ y sea \[
    P=\big\{(x,y,z) : ax+by+cz=d\big\}.
  \]
  Entonces diremos que $P$ es un plano con ecuacion implicita $ax+by+cz=d$ y que $(a,b,c)$ es un vector normal al plano $P$.\\\\
  A esta forma de describir el plano tambien suele llamarsela la ecuacion normal del plano. \\\\
  Observar que la ecuacion $ax+by+cz=d$ no es mas que la ecuacion $\big\langle (x,y,z),(a,b,c)\big\rangle =d.$
\end{defi}
\begin{ej}
  El plano determinado por la ecuacion \[
2x-y+3z=5
  \] es perpendicular al vector $(2,-1,3)$.
  \\\\
  ¿Puntos del plano? Fijamos dos coordenadas y despejamos la tercera.\\\\ Por ejemplo, sea $x=1, y =1 $ y despejamos $z$: \[
3z=5-2+1=4,
  \]
  luego $z = \frac{4}{3}$ y entonces \[
    \left(1,1,\frac{4}{3}\right)
  \] es un punto en el plano.
\end{ej}
Se dice que dos planos son paralelos (en el 3-espacio) si sus vectores normales son paralelos, es decir son proporcionales.
\\\\
Se dice que son perpendiculares si sus vectores normales son perpendiculares.\\\\
El angulo entre dos planos se define como el angulo entre sus vectores normales.\pagebreak

\begin{center}
  \textbf{Ecuacion parametrica del plano en $\mathbb{R}^3$.}
\end{center}
\begin{defi}
  Sean $v,w_1,w_2 \in \mathbb{R}^3$ tal que $w_1,w_2$ no nulos y tal que $w_2$ no sea un multiplo de $w_1$. Sea \[
    P = \{v+sw_1+tw_2  : s,t \in \mathbb{R}\}.
  \]
  Diremos entonces que $P$ es el plano a traves de $v$ paralelo a los vectores $w_1$ y $w_2$.
\end{defi}
Si \[
  P=\{v+sw_1+tw_2 : s,t \in \mathbb{R}\},
\]
entonces el vector $v$ pertenece al plano y el plano \[
  P_0 = \{sw_1+tw_2 : s,t \in \mathbb{R} \}
\]
es el plano que pasa por el origen y paralelo a $P$.
\\\\
Si $P=\big\{(x,y,z) : ax+by+cz=d\big\}$ con $a \neq 0$. \\\\ 
Podemos poner $x$ en funcion de $y,z$ y los datos. Asi obtenemos una ecuacion paramtetrica de $P$.\\\\
Se puede hacer de forma analoga cuando $b \neq 0$ o $c \neq 0$.
\begin{ej}
  Sea el plano  \[
    P=\big\{(x,y,z) : ax+by+cz = d\big\} \quad \text{ y } a \neq 0.
  \]
  Entonces \[
    x=\frac{-by-cz+d}{a},
  \]
  luego \[
    \begin{aligned}
    &P=\left\{\left(\frac{-by-cz+d}{a},y,z\right): y,z \in \mathbb{R}
    \right\}  \\
    &P=\left\{\left(\frac{d}{a},0,0\right)+\left(-\frac{b}{a}y-\frac{c}{a}z,y,z\right) : y,z \in \mathbb{R} \right\} \\
    &P=\left\{\underbrace{\left(\frac{d}{a},0,0\right)}_{v}+\underbrace{\left(-\frac{b}{a},1,0\right)}_{w_1}y+\underbrace{\left(-\frac{c}{a},0,1\right)}_{w_2}z : y,z \in \mathbb{R}\right\}.
    \end{aligned}
  \]
\end{ej}\pagebreak
\begin{ej}
  Sea $P=\big\{(x,y,z) : x-2y+z=1\big\}.$ \\\\
  Como $x-2y+z=1 \quad \Rightarrow \quad x=2y-z+1$, tenemos que \[
    P=\big\{(2y-z+1,y,z) : y,z \in \mathbb{R}\big\},
    \] o, escrito de una forma mas estandar, \[
    P=\big\{(2s-t+1,s,t) : s,t \in \mathbb{R} \big\}.
    \]Tambien podemos escribir \[
    P = \{(1,0,0)+s(2,1,0)+t(-1,0,1) : s,t \in \mathbb{R} \}.
  \]
\end{ej}
¿Como obtenemos la ecuacion normal del plano? \\\\
Si encontramos $u \neq 0$ tal que $\langle u , w_1 \rangle =$ y $\langle u, w_2 \rangle = 0 $, entonces $\langle sw_1 + tw_2, u \rangle = 0$ para $s,t$ arbitrarios y \[
  P_0 = \big\{(x,y,z) : \big\langle(x,y,z),u \big\rangle = 0\big\}.
\]
Sea $d=\langle v,u \rangle$, entonces $\langle v+sw_1+tw_2,u \rangle = \langle v,u \rangle = d$, para $s,t$ arbitrarios. Es decir \[
  P=\big\{(x,y,z) : \big\langle(x,y,z),u \big\rangle=d\big\}.
\]
\begin{ej}
  Sea $P=\big\{(1,1,0)+s(-1,0,-1)+t(0,1,-2) : s,t \in \mathbb{R}\big\}.$ Encontrar la ecuacion implicita de $P$.
\end{ej}
Sea $u=(a,b,c)$, entonces \[
  \begin{aligned}
&\langle u,(-1,0,-1)\rangle = 0 && \Leftrightarrow &&-a-c=0 && \Leftrightarrow && a=-c, \\
&\langle u,(\phantom{-}0,1,-2)\rangle = 0 && \Leftrightarrow && \phantom{-}b-2c=0 && \Leftrightarrow && b=2c.
  \end{aligned}
\]
Luego $u=(-c,2c,c)$. Si, por ejemplo, $c=1, \Rightarrow u=(-1,2,1)$, luego: \[
  P_0=\big\{(x,y,z) : -x+2y + z =0\big\}.
\]
Como $\big\langle (1,1,0),(-1,2,1)\big\rangle=1$, obtenemos \[
  \begin{aligned}
    P&=\big\{(x,y,z) : \big\langle (x,y,z),(-1,2,1)\big\rangle=1\big\}\\
     &=\big\{(x,y,z):-x+2y+z=1\big\}.
  \end{aligned}
\]
\pagebreak 

\begin{center}
  \textbf{Definicion implicita de la recta en $\mathbb{R}^2$.}
\end{center}
\begin{defi}\;\\
  Una recta esta formada por el conjunto de puntos $(x,y) \in \mathbb{R}^2$ que satisfacen la ecuacion \[
ax+by=c,
  \]
  con $a,b,c \in \mathbb{R}$ y tal que $a,b$ no pueden ser simultaneamente $0$.
\\\\
Mas formalmente, la recta es el conjunto \[
  L=\big\{(x,y)\in \mathbb{R}^2 : ax+by + c\big\}
\]
\end{defi}
\begin{obs}
  \; \\\\
  \bl Si $b \neq 0$, entonces la recta es $y=-\frac{a}{b}x+\frac{c}{b}$,\\\\
  \bl Si $b=0$, entonces $a \neq 0$ y la recta es $x=\frac{c}{a}$.
\end{obs}
\begin{obs}
  La ecuacion implicita de la recta $L$ perpendicular a $(a,b)$ y que pasa por $p=(x_0,y_0)$ es \[
ax+by=\big\langle (x_0,y_0),(a,b)\big\rangle.
  \]
\end{obs}
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v22.pdf_tex}
\end{figure}
\pagebreak

\begin{center}
\textbf{Definicion parametrica de la recta.}
\end{center}
\begin{defi}
  Sean $v,w \in \mathbb{R}^2$ tal que $w\neq 0$. Sea \[
    L=\{v+tw : t \in \mathbb{R}\}.
  \]
Diremos entonces que $L$ es la recta que pasa por $v$ paralela a $w$.
\end{defi}
Observemos que la recta $L$ esta dada por todos los puntos que se obtienen de la funcion \[
  X(t)=v+tw, \quad \text{ para } t \in \mathbb{R}.
\]
En el espacio $\mathbb{R}^2$, diremos que esta es la ecuacion parametrica o la representacion parametrica de la recta $L$ que pasa por el punto $v$ y es paralela a $w \neq 0$.
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v19.pdf_tex}
\end{figure}

\begin{center}
\textbf{De ecuacion implicita a parametrica.}
\end{center}
\begin{ej}
  Encontrar la parametrica de la recta \[
5x+y=11.
  \]
\end{ej}
\textbf{Solucion.}
Observar que $y=-5x+11$, luego la recta es \[
  \begin{aligned}
    L&=\big\{(x,y) : y=-5x+11, x \in \mathbb{R}\big\} \\
     &= \big\{(x,-5x+11) : x \in \mathbb{R}\big\} \\
     &= \big\{(t,-5t+11) : t \in \mathbb{R}\big\} \\
     &= \big\{(0,11)+t(1,-5) : t \in \mathbb{R}\big\}.
  \end{aligned}
\]
\pagebreak

\begin{center}
\textbf{De ecuacion parametrica a ecuacion implicita.}
\end{center}
\begin{ej}
  Encontrar la representacion implicita de la recta \[
X(t)=(2,1)+t(-1,5)=(2-t,1+5t).
  \]
\end{ej}
\textbf{Solucion.} Coordenada a coordenada: \[
x=2-t, \quad \quad y=1+5t.
\]
Despejando $t$ de la primera ecuacion obtenemos $t=2-x$.
\\\\
Reemplazando el valor de $t$ en la segunda ecuacion obtenemos \[
y=1+5t=1+5(2-x)=11-5x,
\]
luego
\[
5x+y=11,
\]
que es la ecuacion implicita de la recta.
\begin{center}
\textbf{Recta que pasa por un punto perpendicular a un vector.}
\end{center}
\begin{ej}
  Sean $(a,b),(x_0,y_0) \in \mathbb{R}^2$.\\\\ 
  ¿Cual es la forma parametrica de la recta $L$ perpendicular a $(a,b)$ y que pasa por $p=(x_0,y_0)$?\\\\
\textbf{Solucion.} Para responder a la pregunta, comencemos por recordar que dos vectores en $\mathbb{R}^2$ son perpendiculares si el producto escalar entre ellos es nulo.\\\\
Elijamos $w\in\mathbb{R}^2$ no nulo tal que $\big\langle w,(a,b)\big\rangle=0$. (¿Existe?)\\\\
Entonces la recta que buscamos es la recta que pasa por $p$ con direccion $w$.
\end{ej}
\begin{figure}[h]
\centering
\def\svgwidth{0.55\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v23.pdf_tex}
\end{figure}

En este caso es sencillo buscar un vector ortogonal $w=(b,-a)$.\\\\
Observemos que por la ley del paralelogramo si $p$ en la recta, entonces $p+t(-b,a)$ tambien esta en la recta.
\begin{prop}
  Sean $(a,b),(x_0,y_0) \in \mathbb{R}^2$ con $(a,b) \neq 0$. La recta perpendicular a $(a,b)$ que pasa por $(x_0,y_0)$ es \[
    L=\big\{(x_0,y_0)+t(b,-a) : t \in \mathbb{R}\big\}
  \]
\end{prop}
\begin{ej}
  Encontrar una representacion parametrica para la recta que contiene los puntos $(2,2)$ y es perpendicular a $(2,1)$.
\end{ej}
\textbf{Solucion.} 
El vector ortogonal a $(2,1)$ es $(1,-2)$. Luego \[
  \begin{aligned}
    L &= \big\{(2,2)+t(1,-2) : t \in \mathbb{R}\big\}\\
     &=\big\{(2+t,2-2t) : t \in \mathbb{R}\big\}.
  \end{aligned}
\]
\begin{center}
\textbf{Recta entre dos puntos.}
\end{center}
Bien es sabido que solo hay una recta que pasa por dos puntos dados $p,q \in \mathbb{R}^2$. Graficamente esta es: 
\begin{figure}[h]
\centering
\def\svgwidth{0.75\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v24.pdf_tex}
\end{figure}

La manera mas facil de describir dicha recta es usando la forma parametrica. \\\\
 En efecto, del grafico vemos que $L$ es la recta que pasa por $p$ con direccion $w=p-q$.
\\\\
Lo anterior se resume en lo siguiente: 
\\\\
\textbf{Afirmacion.} Sean $p,q \in \mathbb{R}^2$ y $w=p-q$. Entonces la unica recta que pasa por ambos puntos es \[
  L=\{p + tw : t \in \mathbb{R}\}.
\]
\pagebreak

\begin{center}
\textbf{Segmento entre dos puntos.}
\end{center}
La representacion parametrica tambien es util para describir el conjunto de los puntos que se encuentran en el segmento de linea entre dos puntos dados.\\\\
Mas precisamente, el segmento entre $p$ y $q$ consiste en todos los puntos de la forma \[
  p+t(q-p) \quad \text{ con } 0\leq t \leq 1.
\]
Como $t$ ``va'' de $0$ a $1$, podemos pensar que recorremos el segmento desde \\\\
\bl $p$, esto es cuando $t=0$, a \\\\
\bl $q$, esto es cuando $t=1$.
\begin{center}
  \textbf{Planos en $\mathbb{R}^3$.}
\end{center}
Comenzaremos, con la ecuacion normal del plano 
\begin{figure}[h]
\centering
\def\svgwidth{0.65\textwidth}
\input{/home/jx/Escritorio/Cosas-Latex/Algebra/Figuras/v25.pdf_tex}
\end{figure}
\pagebreak

\begin{center}
  \textbf{Planos en $\mathbb{R}^3$. Definicion implicita.} 
\end{center}
\begin{defi}
  Sean $a,b,c,d \in \mathbb{R}$ tal que $(a,b,c) \neq (0,0,0)$ y sea \[
    P=\big\{(x,y,z) : ax+by+cz=d\big\}.
  \]
Entonces diremos que $P$ es un plano con ecuacion implcita $ax+by+cz=d$ y que $(a,b,c)$ es un vector normal al plano $P$.\\\\
A esta forma de describir el plano tambien suele llamarsela la ecuacion normal del plano.\\\\
\end{defi}

Observar que la ecuacion $ax+by+cz=d$ no es mas que la ecuacion \mbox{$\big\langle(x,y,z),(a,b,c)\big\rangle=d.$}
\begin{ej}
  El plano determinado por la ecuacion \[
2x-y+3z=5
  \]
  es perpendicular al vector $(2,-1,3)$.
\end{ej}
¿Puntos del plano? Fijamos dos coordenadas y despejamos la tercera.\\\\
\\\\
Por ejemplo, sea $x=1$, $y=1$ y despejamos $z$: \[
3x=5-2+1=4,
\]
luego $z=\frac{4}{3}$ y entonces \[
  \left(1,1,\frac{4}{3}\right)
\]es un punto en el plano.
\\\\
Se dice que dos planos son paralelos (en el 3-espacio) si sus vectores normales son paralelos, es decir son proporcionales.\\\\
Se dice que son perpendiculares si sus vectores normales son perpendiculares.\\\\
El angulo entre dos plano se define como el angulo entre sus vectores normales.
\pagebreak

\begin{center}
  \textbf{Ecuacion parametrica del plano en $\mathbb{R}^3$.}
\end{center}
\begin{defi}
  Sean $v,w_1,w_2 \in \mathbb{R}^3$ tal que $w_1,w_2$ no nulos y tal que $w_2$ no sea un multiplo de $w_1$. Sea \[
    P=\{v+sw_1+tw_2 : s,t \in \mathbb{R}\}.
  \]
  Diremos entonces que $P$ es el plano a traves de $v$ paralelo a los vectores $w_1$ y $w_2$
  \end{defi}
  Si \[
    P=\{v+sw_1+tw_2 : s,t \in \mathbb{R}\},
  \]
  entonces el vector $v$ pertenece al plano y el plano \[
    P_0=\{sw_1+tw_2 : s,t \in \mathbb{R}\}
  \]
es el plano que pasa por el origen y paralelo a $P$.
\\\\
Si $P=\big\{(x,y,z) : ax+by+cz=d\big\}$ con $a \neq 0$.
\\\\
Podemos poner $x$ en funcion de $y$, $z$ y los datos. Asi obtenemos una ecuacion parametrica de $P$. \\\\
Se puede hacer de forma analoga cuando $b \neq 0$ o $c \neq 0$. 
\begin{ej}
  Sea $P=\big\{(x,y,z) : x-2y+z=1\big\}$.\\\\
  Como $x-2y+z=1$ $\Leftrightarrow$ $x=2y-z+1$, tenemos que\[
    P=\big\{(2y-z+1,y,z) : y,z \in \mathbb{R} \big\},
  \] 
  o, escrito de forma mas estandar, \[
    P=\big\{(2s-t+1,s,t) : s,t \in \mathbb{R}\big\}.
  \]
  Tambien podemos escribir \[
    P=\big\{(1,0,0)+s(2,1,0)+t(-1,0,1):s,t \in \mathbb{R}\big\}.
  \]
\end{ej}
¿Como obtenemos la ecuacion normal del plano?
\\\\
Si encontramos $u \neq 0$ tal que $\langle u,w_1 \rangle =0$ y $\langle u,w_2 \rangle=0$, entonces \mbox{$\langle sw_1+tw_2,u \rangle =0$} para $s,t$ arbitrarios y \[
  P_0=\big\{(x,y,z) : \big\langle(x,y,z),u\big\rangle=0\big\}.
\]
Sea $d=\langle v,u \rangle$, entonces $\langle v+sw_1+tw_2,u\rangle = \langle v,u \rangle = d $, para $s,t$ arbitrarios. Es decir \[
  P=\big\{(x,y,z) : \big\langle (x,y,z), u \big\rangle=d\big\}.
\]
\begin{obs}
  
\end{obs}
\begin{ej}
  Sea $P=\big\{(1,0,0)+s(-1,0,-1)+t(0,1,-2)\big\}.$ Encontrar la ecuacion implicita de $P$.
\end{ej}
\textbf{Solucion.}
Sea $u=(a,b,c)$, entonces \[
  \begin{aligned}
&\langle u,(-1,0,-1)\rangle = 0 && \Leftrightarrow &&-a-c=0 && \Leftrightarrow && a=-c, \\
&\langle u,(\phantom{-}0,1,-2)\rangle = 0 && \Leftrightarrow && \phantom{-}b-2c=0 && \Leftrightarrow && b=2c.
  \end{aligned}
\]
Luego $u=(-c,2c,c)$. Si, por ejemplo, $c=1, \Rightarrow u=(-1,2,1)$, luego: \[
  P_0=\big\{(x,y,z) : -x+2y + z =0\big\}.
\]
Como $\big\langle (1,1,0),(-1,2,1)\big\rangle=1$, obtenemos \[
  \begin{aligned}
    P&=\big\{(x,y,z) : \big\langle (x,y,z),(-1,2,1)\big\rangle=1\big\}\\
     &=\big\{(x,y,z):-x+2y+z=1\big\}.
  \end{aligned}
\]
\begin{center}
\section{Sistemas de ecuaciones lineales.}
\end{center}
\begin{center}
\textbf{El problema general.}
\end{center}
El problema a resolver sera el siguiente: buscamos numeros $x_1,\dots,x_n$ en el cuerpo $\mathbb{K}\; (= \mathbb{R} \text{ o } \mathbb{C})$ que satisfagan las siguientes condiciones
\[
\begin{matrix}
  a_{11}x_{1} & + & a_{12}x_{2} & + & \cdots & + & a_{1n}x_n & = & y_1 \\ 
  \vdots & & \vdots  & & & & \vdots & & \\
  a_{m1}x_{1} & + & a_{m2}x_2 & + & \cdots & + & a_{mn}x_{n} & = & y_{m}
\end{matrix}\quad \quad (\star)
\]
donde $y_1, \dots, y_m$ y $a_{ij}$ $(1 \leq i \leq m, 1 \leq j \leq n)$ son numeros en $\mathbb{K}$.
\\\\
Se dice que las ecuaciones $(\star)$ forman un sistema de ecuaciones lineales de $m$ ecuaciones con $n$ incognitas.\\\\
\bl El sistema es homogeneo si $y_i=0$ para todo $i$.\\\\
\bl El sistema es no homogeneo si $y_i \neq 0$ para algun $i$.
\\\\
Los siguientes son sistemas de $(2)$ ecuaciones lineales con $2$ incognitas: \[
  (1) \quad \quad \begin{matrix}
    2x_1 & + & x_2 & = & 0 \\
    2x_1 & + & x_2 & = & 1
  \end{matrix}
\]
\[
  (2) \quad \quad \begin{matrix}
    2x_1 & + & x_2 & = & 0 \\
    2x_1 & - & x_2 & = & 1
  \end{matrix}
\]
\[
  (3) \quad \quad \begin{matrix}
    2x & + & y & = & 1 \\
    4x & + & 2y & = & 2
  \end{matrix}
\]
\pagebreak

\textbf{Problema 1.}\\\\
Encontrar las soluciones $(x,y,z)$ del sistema de ecuaciones:
\[
  \begin{matrix*}[r]
    x & & & + & 2z & = & 1\\
    x & - & y & + & 3z & = & 2 \\
    2x & - & y & + & 5z & = & 3
  \end{matrix*}
\]
Es decir, queremos encontrar los numeros reales $x,y$ y $z$ que satisfagan las ecuaciones anteriores.
\\\\
\textbf{Solucion.}\\\\
Veremos que la unica solucion es $(x,y,z)=(-1,0,1)$.
\\\\
Supongamos que $(x,y,z)$ es una solucion de nuestro sistema. Entonces tambien vale que\[
  \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& x & - & 3y & + & 3z\phantom{)} & = & 2 \\
    (-1) & \cdot &(x & & & + & 2z ) & = & (-1) \cdot 1 \\
    \hline 
         & & & - & 3y & + & z\phantom{)} & = & 1
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
  \begin{matrix}[rrrrrrrrrrrr]
    x & & & +  & 2z & = & 1 \\
      & - & 3y & + & z & = & 1 \\
    2x & - & y & + & 5z & = & 3
  \end{matrix}
\]
Entonces tambien vale que: \[
  \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& 2x & - & y & + & 5z\phantom{)} & = & 3 \\
    (-2) & \cdot &(x & & & + & 2z ) & = & (-2) \cdot 1 \\
    \hline 
         & & & - & y & + & z\phantom{)} & = & 1
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
  \begin{matrix}[rrrrrrrr]
    x & & & + & 2z & = & 1 \\
      & - & 3y & + & z & = & 1 \\
      & - & y & + & z & = & 1
    \end{matrix} \quad \quad \text{ equivalentemente } \quad \quad \begin{matrix}[rrrrrrrrrrrrr]
    x & & & +  & 2z & = & 1 \\
      & - & y & + & z &= & 1 \\
      & - & 3y & + & z & = & 1 
  \end{matrix}
\]
Entonces vale que: \[
  \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&&  & - & 3y & + & z\phantom{)} & = & 1 \\
    (-3) & \cdot & & (-& y& + & z ) & = & (-3) \cdot 1 \\
    \hline 
         & & &  &  & - & 2z\phantom{)} & = & -2
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema
\[
  \begin{matrix}[rrrrrrrr]
    x & & & + & 2z & = & 1 \\
      & - & y & + & z & = & 1 \\
      &  &  & - & 2z & = & -2
    \end{matrix} \quad \quad \text{ equivalentemente } \quad \quad \begin{matrix}[rrrrrrrrrrrrr]
    x & & & + & 2z & = & 1 \\
      & - & y & + & z & = & 1 \\
      &  &  &  & z & = & 1
  \end{matrix}
\]
Entonces tambien vale que: \[
  \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& x &  &  & + & 2z\phantom{)} & = & 1 \\
    (-2) & \cdot & (& & &  & z ) & = & (-2) \cdot 1 \\
    \hline 
         & & x &  &  &  & \phantom{)} & = & -1
  \end{matrix} \quad \quad \text{ y } \quad \quad
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&&  & - & y & + & z\phantom{)} & = & 1 \\
    (-1) & \cdot & & (& &  & z ) & = & (-21) \cdot 1 \\
    \hline 
         & &  & - & y &  & \phantom{)} & = & 0
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema
\[
  \begin{matrix}[rrrrrrrr]
    x & & &  &  & = & -1 \\
      & - & y &  &  & = & 0 \\
      &  &  &  & z & = & 1
    \end{matrix} \quad \quad \text{ equivalentemente } \quad \quad \begin{matrix}[rrrrrrrrrrrrr]
      & & &  &   x & = & -1 \\
      &  & &  & y & = & 0 \\
      &  &  &  & z & = & 1
  \end{matrix}
\]
En resumen, supusimos que $(x,y,z)$ es una solucion del sistema \[
 \begin{matrix}[rrrrrrrrrrrr]
    x & & & + & 2z & = & 1 \\
    x  & - & 3y & + & 3z & = & 2 \\
    2x & - & y & + & 3z & = & 1
  \end{matrix}
\]
y probamos que \[
x=-1, \quad y=0, \quad z=1.
\]
\textbf{Comprobemos.} Si reemplazamos en el sistema $x,y$ y $z$ por estos valores 
\[
 \begin{matrix}[rrrrrrrrrrrr]
    -1 & & & + & 2\cdot(1) & = & 1 \\
    -1  & - & 3\cdot0 & + & 3\cdot(1) & = & 2 \\
    2\cdot(-1) & - & 0 & + & 3\cdot(1) & = & 1
  \end{matrix}
\]
vemos que se verifican las igualdades del sistema.
\\\\
Podria usceder que el sistema no tenga solucion como en el siguiente caso.\\\\
\textbf{Problema 2.}\\\\
Encontrar las soluciones $(x,y,z)$ del sistema de ecuaciones:
\[
 \begin{matrix}[rrrrrrrrrrrr]
     x &  & & + & 2z& = & 1  \\
     x & - & 3y & + & 3z & = & 2 \\
     2x & - & 3y & + & 5z & = &  4 
  \end{matrix}
\]
Supongamos que $(x,y,z)$ es una solucion de nuestro sistema \[
 \begin{matrix}[rrrrrrrrrrrr]
     x &  & & + & 2z& = & 1  \\
     x & - & 3y & + & 3z & = & 2 \\
     2x & - & 3y & + & 5z & = &  4 
  \end{matrix}
\]
Entonces tambien vale que: \[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& x & - & 3y & + & 3z\phantom{)} & = & 2 \\
    (-1) & \cdot & (x& & & + & 2z ) & = & (-1) \cdot 1 \\
    \hline 
         & &  & - & 3y & + & z \phantom{)} & = & 1
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
 \begin{matrix}[rrrrrrrrrrrr]
     x &  & & + & 2z& = & 1  \\
      & - & 3y & + & z & = & 1 \\
     2x & - & 3y & + & 5z & = &  4 
  \end{matrix}
\]
Entonces tambien vale que: \[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& 2x & - & 3y & + & 5z\phantom{)} & = & 4 \\
    (-2) & \cdot & (x& & & + & 2z ) & = & (-2) \cdot 1 \\
    \hline 
         & &  & - & 3y & + & z \phantom{)} & = & 2
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
 \begin{matrix}[rrrrrrrrrrrr]
     x &  & & + & 2z& = & 1  \\
      & - & 3y & + & z & = & 1 \\
      & - & 3y & + & z & = &  2 
  \end{matrix}
\]
Entonces tambien vale que: \[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&&  & - & 3y & + & z\phantom{)} & = & 2 \\
    (-1) & \cdot & &(- & 3y& + & z ) & = & (-1) \cdot 1 \\
    \hline 
         & &  &  &  &  & 0 \phantom{)} & = & 1
  \end{matrix}
\]
Absurdo, es decir el sistema no posee solucion.
\\\\
\textbf{Poblema 3.}\\\\
Encontrar las soluciones $(x,y,z)$ del sistema de ecuaciones:
\[\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
     x  & - & 3y & + & 3z & = & 2 \\
     2x & - & 3y & + & 5z & = & 3
  \end{matrix}
\]\textbf{Solucion.} Veremos que el conjunto de soluciones del sistema es \[
  \left\{\left(-2z+1,\frac{z-1}{3},z\right) : z \in \mathbb{R}\right\}.
\]
Es decir, todas las soluciones son de la forma \[
  x=-2z+1 \text{ e } y=\frac{z-1}{3} \text{ donde } z \in \mathbb{R}.
\]
Supongamos que $(x,y,z)$ es una solucion de nuestro sistema
\[\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
     x  & - & 3y & + & 3z & = & 2 \\
     2x & - & 3y & + & 5z & = & 3
  \end{matrix}
\]

Entonces tambien vale que: 
\[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& x & - & 3y & + & 3z\phantom{)} & = & 2 \\
    (-1) & \cdot &(x & &  & + & 2z ) & = & (-1) \cdot 1 \\
    \hline 
         & &  & - &  3y& + &  z\phantom{)} & = & 1
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
     x  & - & 3y & + & z & = & 1 \\
     2x & - & 3y & + & 5z & = & 3
  \end{matrix}
\]
Entonces tambien vale que: \[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& 2x & - & 3y & + & 5z\phantom{)} & = & 3 \\
    (-2) & \cdot &(x & &  & + & 2z ) & = & (-2) \cdot 1 \\
    \hline 
         & &  & - &  3y& + &  z\phantom{)} & = & 1
  \end{matrix}
\]
Por lo tanto $(x,y,z)$ tambien es solucion del sistema \[
\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
       & - & 3y & + & z & = & 1 \\
    x & - & 3y & + & z & = & 1
  \end{matrix} \quad \text{ equivalentemente } \quad \begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
       & - & 3y & + & z & = & 1 \\
     \end{matrix} 
\]
Dado que $(x,y,z)$ es solucion del sistema \[\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
       & - & 3y & + & z & = & 1 \\
     \end{matrix} 
\]
podemos despejar $x$ e $y$ en funcion de $z$. Esto es, \[
  \begin{aligned}
    x &=-2z+1 \\
    y&=\frac{z-1}{3}
  \end{aligned}
\]
y no tenemos ninguna condicion sobre $z$.
\\\\
En resumen, supusimos que $(x,y,z)$ es una solucion del sistema \[
\begin{matrix}[rrrrrrrrrrrr]
     x  &   &    & + & 2z & = & 1  \\
     x  & - & 3y & + & 3z & = & 2 \\
     2x & - & 3y & + & 5z & = & 3
  \end{matrix}
\]
y probamos que \[
  x=-2z+1 \text{ e } y=\frac{z-1}{3}.
\]
\textbf{Comprobemos.} Si reemplazamos $x,y$ y $z$ por estos valores \[
\begin{matrix}[rrrrrrrrrrrr]
  (-2z+1)  &   &    & + & 2z & = & 1  \\
  (-2z+1)  & - & 3\cdot(\frac{z-1}{3}) & + & 3z & = & 2 \\
  2\cdot(-2z+1) & - & 3\cdot(\frac{z-1}{3}) & + & 5z & = & 3
  \end{matrix}
\]
vemos que verifican las igualdades del sistema.\\\\
\begin{center}
\textbf{Justificacion del metodo de eliminacion de incognitas.}
\end{center}

\begin{prop}\;\\\\
  Sean $c_1,\dots, c_m$ en $\mathbb{K}$. Si $(x_1,\dots,x_n) \in \mathbb{K}^n$ es solucion del sistema de ecuaciones \[
    \begin{matrix}
      a_{11}x_{1} & + & a_{12}x_{2}& + & \cdots & + & a_{1n}x_{n} & = & y_1 \\
      \vdots & & \vdots && &&\vdots \\
      a_{m1}x_{1} & + & a_{m2}x_{2} & + & \cdots & + & a_{mn}x_{n} & = & y_{m}.
    \end{matrix}
  \]
  entonces $(x_1,\dots,x_n)$ tambien es solucion de la ecuacion \[
    \sum_{i=1}^{m}{c_i}(a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n)=\sum_{i}c_iy_i.
  \]
  (se usara mas adelante, llamemosla proposicion $(\triangle)$
\end{prop}
\begin{demo}\;\\\\
  Por hipotesis \[
      \begin{matrix}
      a_{11}x_{1} & + & a_{12}x_{2}& + & \cdots & + & a_{1n}x_{n} & = & y_1 \\
         \end{matrix}
  \]
  Luego,\[
 \sum_{i=1}^{m}{c_i}(a_{i1}x_1+a_{i2}x_2+\cdots+a_{in}x_n)=\sum_{i}c_iy_i.
\]
  \qed
\end{demo}
\begin{obs}
La ecuacion anterior se puede reescribir: \[
  \left(\sum_{i=1}^{m}{c_ia_{i1}}\right)x_1+ \cdots +\left(\sum_{i=1}^{m}{c_ia_{in}}\right)x_n=\sum_{i=1}^{m}{c_iy_i}.
\]Es decir es una nueva ecuacion lineal con $n$ incognitas.
\end{obs}
La idea de hacer combinaciones lineales de ecuaciones es fundamental en el proceso de eliminacion de incognitas.
\begin{defi}
  Decimos que dos sistemas de ecuacones lineales son equivalentes si cada ecuacion de un sistema es combinacion lineal del otro.
\end{defi}\begin{teo}
  Dos sistemas de ecuaciones lineales equivalentes tienen las mismas soluciones.
\end{teo}
\begin{demo}\;\\\\
  Sea \[
    \begin{matrix}
      a_{11}x_{1} & + & a_{12}x_{2}& + & \cdots & + & a_{1n}x_{n} & = & y_1 \\
    \vdots & & \vdots && &&\vdots && & \phantom{\star} \quad(\star) \\
      a_{m1}x_{1} & + & a_{m2}x_{2} & + & \cdots & + & a_{mn}x_{n} & = & y_{m}.
    \end{matrix}
  \]
  equivalente a \[
\begin{matrix}
      b_{11}x_{1} & + & b_{12}x_{2}& + & \cdots & + & b_{1n}x_{n} & = & z_1 \\
      \vdots & & \vdots && &&\vdots && &\quad (\star \star) \\
      b_{k1}x_{1} & + & b_{k2}x_{2} & + & \cdots & + & b_{kn}x_{n} & = & z_{k}.
    \end{matrix}
  \]
  Esto quiere decir que \\\\
  \textcolor{azulp2}{1.} las ecuaciones de $(\star\star)$ se obtienen a partir de combinaciones lineales de las ecuaciones del sistema $(\star)$, y \\\\
  \textcolor{azulp2}{2.} las ecuaciones de $(\star)$ se obtienen a partir de combinaciones lineales de las ecuaciones del sistema $(\star\star)$.\\\\
  Luego, por la primera proposicion (proposicion $\triangle$): \\\\
  \textcolor{azulp2}{1.} si $(x_1,\dots,x_n)$ es solucion de $(\star)$, tambien sera solucion de cada una de las ecuaciones de $(\star\star)$ y por lo tanto solucion del sistema $(\star\star)$, y \\\\
  \textcolor{azulp2}{2.} si $(x_1,\dots,x_n)$ es solucion de $(\star\star)$, tambien sera solucion de cada una de las ecuaciones de $(\star)$ y por lo tanto solucion del sistema $(\star)$.
  \qed
\end{demo}
\begin{obs}
  Las combinaciones lineales que hemos utilizado en los tres sistemas que hemos trabajado son
\begin{list}{$\circ$}{}  
\item sumar a una ecuacion una constante por otra,  
\item multiplicar una ecuacion por una constante no nula, y 
\item permutar ecuaciones. 
\end{list}
\end{obs}
Veremos que estas operaciones son ``reversibles'', es decir, asi como haciendo estas operaciones en la ecuacion llegamos de \[
  \begin{matrix}
    x & & & + & 2z & = & 1 \\
    x & - & 3y & + & 3z & = &2 \\
    2x & - & y & + & 5z & = & 3
  \end{matrix}\quad \quad \text { a } \quad \quad 
  \begin{matrix}[l]
x=-1\\
y=0\\
z=1.
  \end{matrix}
\]
Haciendo las ``operaciones inversas'' (que son del mismo tipo) podemos llegar de \[
 \begin{matrix}[l]
x=-1\\
y=0\\
z=1.
  \end{matrix}
\quad \quad \text { a } \quad \quad 
  \begin{matrix}
    x & & & + & 2z & = & 1 \\
    x & - & 3y & + & 3z & = &2 \\
    2x & - & y & + & 5z & = & 3
  \end{matrix} 
\]
Luego, ambos sistemas son equivalentes y, por lo tanto, tiene las mismas soluciones.
\\\\
\textbf{Conclusiones.}
\begin{list}{$\circ$}{}  
\item  Un sistema de ecuaciones puede tener una, ninguna o infinitas soluciones.
\item  Hemos cambiado nuestro sistema inicial haciendo combinaciones lineales de las ecuaciones.
\item El nuevo sistema es mas sencillo en el sentido que: 
  \begin{list}{$\bullet$}{}
  \item Cada ecuacion tiene menos incognitas. 
  \item Las soluciones quedan descriptas explicitamente.
  \end{list}
\item Las soluciones del nuevo sistema son las soluciones de nuestro sistema original.
\end{list}
\begin{ej}[Resolver el sistema de ecuaciones]
  \[
    \begin{matrix}
      x & - & 2y & - & 2z & + & t & = & 4 \\
      x & + & y & + & z & - & t & = & 5 \\
      x & - & y & - & z & + & t & = & 6 \\
      6x & - & 3y & - & 3z & + & 2t & = & 32.
  \end{matrix} 
\]
\end{ej}
(1°) Este sistema es no homogeneo pues esta igualado a constantes no nulas. \\\\
(2°) Resolvamos el sistema por eliminacion de variables.
\\\\
Reemplazo la segunda ecuacion por ella misma menos la primera ecuacion.\[
 \begin{matrix}[rrrrrrrrrrrrrrrrrr]
&& x & - & y & + & z & - & t\phantom{)} & = & 5 \\
   (-1) & \cdot &(x &- & 2y & - & 2z  & + & t)& = & (-1) \cdot 4 \\
    \hline 
        &  &   &  & 3y & + & 3z & -  & 2t \phantom{)}&= & 1
  \end{matrix}
\]
Obtenemos el sistema \[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1) \quad &  x & - & 2y & - & 2z & + & t & = & 4 \\
    (2') \quad &  & & 3y & + & 3z & - & 2t & = & 1 \\
    (3) \quad & x & - & y & - & z & + & t & = & 6 \\
    (4) \quad & 6x & - & 3y & - & 3z & + & 2t & = & 32\\
\end{matrix}.
\]\pagebreak

Analogamente, eliminamos $x$ en otras ecuaciones haciendo $(3)-(1), (4)-6\cdot(1)$ y obtenemos
\[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1) \quad &  x & - & 2y & - & 2z & + & t & = & 4 \\
    (2') \quad &  & & 3y & + & 3z & - & 2t & = & 1 \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
    (4') \quad &  &  & 9y & + & 9z & - & 4t & = & 8\\
\end{matrix}.
\]
La $x$ fue eliminada de las ecuaciones $2$°, $3$° y $4$°.\\\\
Ahora eliminamos la y de las ecuaciones $1$°,$2$° y $4$°. Hacemos $(1)+2\cdot (3')$, $(2')-3\cdot(3'),\;(4')-9\cdot(3')$ y obtenemos
\[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1') \quad &  x &  &  &  &  & + & t & = & 8 \\
    (2'') \quad &  & &  &  &  & - & 2t & = & -5 \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
    (4'') \quad &  &  &  &  &  & - & 4t & = & -10\\
\end{matrix}.
\]
Hacemos $\frac{(2'')}{-2}$ y tenemos
\[
  \begin{matrix}[lrrrrrrrrrrrrr]
  (1') \quad &  x &  &  &  &  & + & t & = & 8 \\
    (2''') \quad &  & &  &  &  &  & t & = & \sfrac{5}{2} \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
    (4'') \quad &  &  &  &  &  & - & 4t & = & -10\\
\end{matrix}.
\]
Haciendo $(4'')+4\cdot(2''')$ obtenemos
\[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1') \quad &  x &  &  & - & 4z & + & t & = & 8 \\
    (2''') \quad &  & &  &  &  &  & t & = & \sfrac{5}{2} \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
    (4''') \quad &  &  &  &  &  &  &  0 & = & 0 \\
\end{matrix}.
\]
Ahora hacemos $(1')-(2''')$ y obtenemos \[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1'') \quad &  x &  &  &  &  &  &  & = & \sfrac{11}{2} \\
    (2''') \quad &  & &  &  &  &  & t & = & \sfrac{5}{2} \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
    (4''') \quad &  &  &  &  &  &  &  0 & = & 0 \\
\end{matrix}.
\]
Que es equivalente al sistema
\[
  \begin{matrix}[lrrrrrrrrrrrrr]
    (1'') \quad &  x &  &  &  &  &  &  & = & \sfrac{11}{2} \\
    (2''') \quad &  & &  &  &  &  & t & = & \sfrac{5}{2} \\
    (3') \quad &  &  & y & + & z &  &  & = & 2 \\
   \end{matrix}.
\]
Ahora es muy facil despejar
\[
  \begin{matrix}[l]
    x=\frac{11}{2}\\
    t=\frac{5}{2}\\
    y=-z+2
  \end{matrix}
\]
Las soluciones de este sistema son \[
  \left\{\left(\frac{11}{2},-z+2,z,\frac{5}{2} : z \in \mathbb{R}\right)\right\}.
\]
Recordemos que si de un 1° sistema pasamos a un 2° sistema por combinaciones lineales las solucion del 1° sistema son iguales a las soluciones del 2° sistema.\\\\
\textbf{Ejercicio.} Pasar del segundo sistema al primero.\\\\
Luego, las soluciones del 2° son iguales a las soluciones del 1°. \\\\
Como soluciones del 1° $\Rightarrow$ soluciones del 2° y soluciones del 2° $\Rightarrow$ soluciones del 1° tenemos que \[
  \text{soluciones del 1°} \quad \Leftrightarrow \quad \text{soluciones del 2°.}
\]
A continuacon se presentaran las nociones de: \\\\
\quad \bl Matriz.\\\\
\quad \bl Matriz ampliada. \\\\
\quad \bl Operaciones elementales por fila.\\\\
\quad \bl Matriz escalon reducida por fila (MERF).\\\\
Se relacionaran todos estos conceptos con los sistemas de ecuaciones lineales.\\\\
\quad \bl Matriz: representara un sistema de ecuaciones homogeneo.\\\\
\quad \bl Matriz ampliada: representara un sistema de ecuaciones lineales no homogeneo.\\\\
\quad \bl Operaciones elementales por fila: representaran ciertas operaciones entre las diferentes ecuaciones del sistema.\\\\
\quad \bl Matriz escalon reducida por fila (MERF): representaran sistemas de ecuaciones lineales que se resuelven en forma trivial.
\pagebreak

\begin{defi}
  Una matriz $m \times n$ es un arreglo de numeros reales de $m$ filas y $n$ columnas. \\\\
  $\mathbb{R}^{m \times n}$ y $M_{m \times n} (\mathbb{R})$ denotan el conjunto de matrices $m\times n$.
\end{defi}
\begin{ej}
  \[
    \bbordermatrix{~ & C_1 & C_2 & C_3 \cr
  F_{1} & 2 & 1 & 2 \cr
F_{2} & 3 & 0 & \pi \cr} 
\quad \quad \quad
\bbordermatrix{~ & C_1 & C_2 & C_3 \cr
F_{1} & \sqrt{2} & \frac{1}{2} & 9 \cr} 
\quad \quad \quad
\bbordermatrix{~ & C_1 \cr
F_{1} & 10 \cr
F_{2} & -1 \cr 
F_{3} & \phantom{-}1 \cr} 
  \]
  \end{ej}
  \textbf{Convenciones.} La notacion $A=[a_{ij}] \in \mathbb{R}^{m \times n}$ quiere decir que $A$ es una matriz $m \times n$ de la siguiente forma \[
\begin{bmatrix}
  a_{11} &   a_{12}   & \cdots & a_{1j} & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}   & \cdots & a_{2j} & \cdots & a_{2n}   \\ 
\vdots & \vdots &  & \vdots & & \vdots\\
a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots & a_{in} \\
\vdots & \vdots & & \vdots &  & \vdots \\
  a_{m1} &   a_{m2}   & \cdots & a_{mj} & \cdots & a_{mn}   \\ 
\end{bmatrix}
  \]
\begin{list}{$\circ$}{}  
\item Sea $A = [a_{ij}] \in \mathbb{R}^{m \times n}$ una matriz. Escribiremos $[A]_{ij}$ para denotar la entrada $a_{ij}$ de $A$.

\item Dos matrices del mismo tamaño $A=[a_{ij}] \in \mathbb{R}^{m \times n}$ y $B=[b_{ij}] \in \mathbb{R}^{m \times n}$ son iguales si cada una de sus entradas lo son: \[
    A=B \quad \quad \Longleftrightarrow \quad \quad a_{ij} = b_{ij}\quad \forall i,j.
  \]
\end{list}
\textbf{Notacion.} Usaremos matrices para representar los sistemas de ecuaciones. \begin{defi}
  Si $A=[a_{ij}] \in \mathbb{R}^{m \times n}$ e $Y=[y_{j}] \in \mathbb{R}^m$ entonces \[
AX=Y
  \]
  representa al sistema de ecuaciones\[
\begin{matrix}
  a_{11}x_{1} & + & a_{12}x_{2} & + & \cdots & + & a_{1n}x_n & = & y_1 \\ 
  a_{21}x_{1} & + & a_{22}x_{2} & + & \cdots & + & a_{2n}x_n & = & y_1 \\
  \vdots & & \vdots  & & & & \vdots & & \\
  a_{m1}x_{1} & + & a_{m2}x_2 & + & \cdots & + & a_{mn}x_{n} & = & y_{m}.
\end{matrix}
  \]
\end{defi}
Tambien lo podemos denotar \[
\begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}   & \cdots & a_{2n}   \\ 
\vdots & \vdots & \ddots & \vdots  \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
 \vdots \\
 x_n
\end{bmatrix}
=
\begin{bmatrix}
  y_1\\
  y_2\\
  \vdots \\
  y_m
\end{bmatrix}
\]\begin{ej}
El sistema de ecuaciones \[
 \begin{matrix}[rrrrrrrrrrrr]
     x_1 &  & & + & 2x_3 & = & 1 \\
     x_1 & - &3x_2 & + & 3x_3 & = & 2 \\
     2x_1& - & 3x_2 & + & 5x_3 & = & 3
  \end{matrix}
\] es representado de la forma $AX=Y$:
\[
\begin{bmatrix}[rrrrrr]
  1 & 0 & 2 \\
  1 & -3 & 3 \\
  2 & -3 & 5 \\
\end{bmatrix}
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
=
\begin{bmatrix}
  1\\
  2\\
  3
\end{bmatrix}
\]
\end{ej}
\begin{obs}
  
\end{obs}
\begin{list}{$\circ$}{}  
\item Si una incognita no aparece en una ecuacion, el correspondiente coeficiente de la matriz es $0$. 
\item La cantidad de incognitas queda determinada por la cantidad de columnas de la matriz $A$.
\end{list}
\begin{center}
\textbf{Operaciones elementales por fila: motivacion.}
\end{center}
Las operaciones elementales por fila son:
\begin{list}{$\circ$}{}  
\item transformaciones con las cuelaes podemos modificar una matriz de manera tal que los correspondientes sistemas de ecuaciones tengna las mismas soluciones. 
\item la version "matricial" de las combinaciones lineales de ecuaaciones que hicimos en la clase anterior para encontrar las soluciones de los sistemas.
\end{list}\pagebreak

\begin{obs}
  Hay tres tipos de operaciones las cuales definiremos a continuacion.
\end{obs}
En una matriz $A$ de $m \times n$, cada fila puede ser considerada un vector en $\mathbb{R}^n$. \\\\
Si la fila $i$ de $A$ es \[
  \begin{bmatrix}
    a_{i1} & a_{i2} & \cdots & a_{in},
  \end{bmatrix}
\]
y la denotamos $F_i (A)$ o simplemente $F_i$ si $A$. Si $c \in \mathbb{K}$, entonces 
\begin{list}{$\circ$}{}  
\item $cF_{i} = \begin{bmatrix} 
    ca_{i1} & ca_{i2} & \cdots & ca_{in}
  \end{bmatrix}.$
\item $F_r + F_s = \begin{bmatrix}
    a_{r1}+a_{s1} & a_{r2} + a_{s2} & \cdots & a_{rn}+a_{sn}
  \end{bmatrix}$.
\item $F_i=\begin{bmatrix} 
    0 & 0 & \cdots & 0
  \end{bmatrix}$, la fila nula.
\end{list}
\begin{center}
\textbf{Operaciones elementales por fila: definicion.}
\end{center}
\begin{defi}
  Sea $A= [a_{ij}]$ una matriz $ m \times n$, diremos que $e$ es una operacion elemental por fila si aplicada a la matriz $A$ se obtiene $e(A)$ de la siguiente manera:\\\\
  \textcolor{azulp2}{E1.} multiplicando la fila $r$ por una constante $c \neq 0$, o \\\\
  \textcolor{azulp2}{E2.} cambiando la fila $F_r$ por $F_r+tF_s$ con $r \neq s$, para algun $t \in \mathbb{K}$, o\\\\
  \textcolor{azulp2}{E3.} permutando la fila $r$ por la fila $s$.\\\\
  E1, E2, y E3 son tres tipos de operaciones elementales.
\end{defi}
\textbf{E1:} multiplicar la fila $i$ por un numero real $c \neq 0$. 
\[
  \begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{a_{i1}} & \textcolor{rojop2}{a_{i2}} & \cdots  &  \textcolor{rojop2}{a_{in}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
  \end{bmatrix}  \quad \quad \xrightarrow{cF_i} \quad \quad \begin{bmatrix}
      a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{c  a_{i1}} & \textcolor{rojop2}{c  a_{i2}} & \cdots  &  \textcolor{rojop2}{c  a_{in}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
  \end{bmatrix}
\]
\begin{ej}
  Multiplicar la primera fila por $-2$: \[
    \begin{bmatrix}[rrrrrr]
      1 & 2 \\
      3 & 4 \\
      5 & 6
    \end{bmatrix} 
    \quad \quad \xrightarrow{(-2)\cdot F_{1}} \quad \quad \begin{bmatrix}[rrrrrr]
      -2 & -4 \\
      3 & 4 \\
      5 & 6
    \end{bmatrix}
  \]
\end{ej}
\pagebreak

\textbf{E2:} sumar a la fila $r$ un multiplo de la fila $s$.\[
  \begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{a_{s1}} & \textcolor{rojop2}{a_{s2}} & \cdots  &  \textcolor{rojop2}{a_{sn}} \\
\vdots & \vdots & & \vdots \\
\textcolor{rojop2}{a_{r1}} & \textcolor{rojop2}{a_{r2}} & \cdots  &  \textcolor{rojop2}{a_{rn}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
  \end{bmatrix} \quad \quad \xrightarrow{F_r + tF_{s}} \quad \quad  \begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{a_{s1}} & \textcolor{rojop2}{a_{s2}} & \cdots  &  \textcolor{rojop2}{a_{sn}} \\
\vdots & \vdots & & \vdots \\
\textcolor{rojop2}{a_{r1}+ta_{s1}} & \textcolor{rojop2}{a_{r2}+ta_{s2}} & \cdots  &  \textcolor{rojop2}{a_{rn}+ta_{sn}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
  \end{bmatrix}
\]
\begin{ej}
  Sumar a la segunda fila la primer fila multiplicada por $3$: \[
    \begin{bmatrix}
      1 & 2 \\
      3 & 4 \\
      5 & 6 
      \end{bmatrix} \quad \quad \xrightarrow{F_2+3F_1} \quad \quad  \begin{bmatrix}
      1 & 2 \\
      3+3\cdot1 & 4+3\cdot 2 \\
      5 & 6 
    \end{bmatrix}
    =
    \begin{bmatrix}
      1 & 2 \\
      6 & 10 \\
      5 & 6
    \end{bmatrix}
  \]
\end{ej}
\textbf{E3:} intercambiar las filas $r$ y $s$. \[
\begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{a_{s1}} & \textcolor{rojop2}{a_{s2}} & \cdots  &  \textcolor{rojop2}{a_{sn}} \\
\vdots & \vdots & & \vdots \\
\textcolor{rojop2}{a_{r1}} & \textcolor{rojop2}{a_{r2}} & \cdots  &  \textcolor{rojop2}{a_{rn}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
\end{bmatrix} \quad \quad \xrightarrow{F_r \leftrightarrow F_s} \quad \quad  \begin{bmatrix}
  a_{11} &   a_{12}    & \cdots & a_{1n}   \\ 
  a_{21} &   a_{22}    & \cdots & a_{2n}   \\ 
\vdots & \vdots & & \vdots & \\
\textcolor{rojop2}{a_{r1}} & \textcolor{rojop2}{a_{r2}} & \cdots  &  \textcolor{rojop2}{a_{rn}} \\
\vdots & \vdots & & \vdots \\
\textcolor{rojop2}{a_{s1}} & \textcolor{rojop2}{a_{s2}} & \cdots  &  \textcolor{rojop2}{a_{sn}} \\
\vdots & \vdots & & \vdots & \\
  a_{m1} &   a_{m2}    & \cdots & a_{mn}   \\ 
\end{bmatrix} 
\]
\begin{ej}
  Intercambiar la segunda y tercera fila: 
  \[
    \begin{bmatrix} 
      1 & 2 \\
      3 & 4 \\
      5 & 6
      \end{bmatrix} \quad \quad \xrightarrow{F_2 \leftrightarrow F_3}\quad\quad \begin{bmatrix}
      1 & 2 \\
      5 & 6 \\
      3 & 4
    \end{bmatrix}
  \]
\end{ej}
\begin{center}\pagebreak 
\textbf{Convenciones.}
\end{center}
\begin{list}{$\circ$}{}  
\item Si $A$ es una matriz, $e(A)$ denotara la matriz que obtenemos despues de modificar a $A$ por cierta operacion elemental $e$.
\end{list}
\begin{ej}\;\\
  Si $e$ es la operacion elemental intercambiar la segunda y tercer fila y \mbox{$A = \begin{bmatrix}
    1 & 2 \\ 
  3 & 4 \\
5 & 6
\end{bmatrix}$}, entonces $e(A)=\begin{bmatrix} 1 & 2 \\ 5 & 6 \\ 3 & 4 \end{bmatrix}$
\end{ej}
\begin{itemize}
  \item Como hicimos en los ejemplos, cuando le apliquemos una operacion elemental a una matriz especificaremos arriba de una flecha que operacion aplicamos: \[
      A \quad  \overset{e}{\longrightarrow}  \quad e(A).
    \]
    Esta notacion es obligatoria para la correcion de examenes.
\end{itemize}
\begin{teo}
  A cada operacion elemental por fila $e$ le corresponde otra operacion elemental $e'$ (del mismo tipo que $e$) tal que $e'(e(A))=A$ y $e(e'(A)) = A$. En otras palabras, la operacion inversa de una operacion elemental, es otra operacion elemental del mismo tipo.
\end{teo}
\begin{demo}
  \;\\\\
\textcolor{azulp2}{E1.} La operacion inversa de multiplicar la fila $r$ por $c \neq 0$ es multiplicar la misma fila por $\frac{1}{c}$.\\\\
\textcolor{azulp2}{E2.} La operacion inversa de multiplicar la fila $s$ por $t \in \mathbb{K}$ y sumarla a la fila $r$ es multiplicar la fila $s$ por $-t \in \mathbb{K}$ y sumarla a la fila $r$.\\\\
\textcolor{azulp2}{E3.} La operacion inversa de permutar la fila $r$ por la fila $s$ es la misma operacion. \\\\\qed
\end{demo}\pagebreak

\begin{defi}
  Consideremos el siguiente sistema de ecuaciones
  \[\begin{matrix}
  a_{11}x_{1} & + & a_{12}x_{2} & + & \cdots & + & a_{1n}x_n & = & y_1 \\ 
  a_{21}x_{1} & + & a_{22}x_{2} & + & \cdots & + & a_{2n}x_n & = & y_1 \\
  \vdots & & \vdots  & & & & \vdots & & \\
  a_{m1}x_{1} & + & a_{m2}x_2 & + & \cdots & + & a_{mn}x_{n} & = & y_{m}
\end{matrix}
\] y sea $A$ la matriz correspondiente al sistema. La matriz ampliada del sistema es \[
A'=\begin{bmatrix}[ccc|c]
  a_{11} & \cdots & a_{1n} & y_{1} \\
  a_{21} & \cdots & a_{2n} & y_{2} \\
  \vdots & & \vdots & \vdots \\
  a_{m1} & \cdots & a_{mn} & y_m
\end{bmatrix}
\] que tambien podemos denotar \[
A'=\begin{bmatrix}[c|c] A & Y \end{bmatrix}.
\]
\end{defi}
\begin{obs}
  Hay una correspondencia biunivoca entre \[
    \text{sistemas de ecuaciones lineales} \quad \longleftrightarrow \quad \text{matrices ampliadas.}
  \]
\end{obs}
\begin{ej}
  Dado el sistema \[
    \begin{bmatrix}[rrrr]
      1 & 0 & 2 \\
      1 & -3 & 3 \\
      2 & -3 & 5 
    \end{bmatrix}
    \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
    \end{bmatrix}
=
\begin{bmatrix}
1 \\ 2 \\ 3
\end{bmatrix},
  \]
  la matriz ampliada es \[
    \begin{bmatrix}[rrr|c]
      1 & 0 & 2 & 1 \\ 1 & -3 & 3 & 2 \\ 2 & -3 & 5 & 3
    \end{bmatrix}
  \]
\end{ej}
\begin{center} \pagebreak

\textbf{Operaciones elementales $\longrightarrow$ operaciones entre ecuaciones.}
\end{center}
Sea $AX=Y$ un sistema de ecuaciones lineales y $\begin{bmatrix}[c|c] A & Y \end{bmatrix}$ su matriz ampliada. 
\begin{equation} \tag{E1}
\begin{gathered}
  \text{Multiplicar fila }r \text{ por }c \neq 0 \\
\longleftrightarrow\\
\text{multiplicar ecuacion }r\text{-esima por }c \neq 0.\\
\end{gathered}
\end{equation}
\\
\begin{equation} \tag{E2}
\begin{gathered}
  \text{Cambiar fila } F_r \text{ por } F_{r}+tF_{s} \text{ con } r \neq s\text{, para algun }t \in \mathbb{K} \\
\longleftrightarrow\\
\text{sumar a la ecuacion }r\text{-esima }t \text{ veces la ecuacion }s\text{-esima}.\\
\end{gathered}
\end{equation}
\\
\begin{equation} \tag{E3}
\begin{gathered}
\text{Permutar fila }r\text{ por fila } s\\
\longleftrightarrow\\
\text{permutar la ecuacion } r\text{-esima por la ecuacion } s\text{-esima.}
\end{gathered}
\end{equation}
\begin{teo}
  Sea $[A|Y]$ la matriz ampliada de un sistema de ecuaciones lineales y sea $[B|Z]$ una matriz que se obtiene a partir de $[A|Y]$ por medio de operaciones elementales. Entonces, los sistemas correspondientes a $[A|Y]$ y $[B|Y]$ tienen las mismas soluciones.
\end{teo}
\begin{demo}\;
\begin{list}{$\circ$}{}  
\item  $[A|Y] \leadsto [B|Z] \quad \Rightarrow \quad$ filas$[B|Z] =$ combinaciones linealaes de filas$[A|Y]$. 
\item Luego, Soluciones$[A|Y] \Rightarrow $ Soluciones$[B|Z]$.
\end{list}
Como toda operacion elemental tiene inversa $\Rightarrow$
\begin{list}{$\circ$}{}  
\item  $[B|Z] \leadsto [A|Y] \quad [A|Y] =$ combinaciones lineales de filas $[B|Z]$.
\item Luego, Soluciones$[B|Z] \Rightarrow$ Soluciones$[A|Y]$.
\end{list}
Por lo tanto Soluciones$[A|Y]=$ Soluciones$[B|Z]$.\\ \qed
\end{demo}
\begin{ej}
  Resolvamos el siguiente sistema: \[
 \begin{matrix}[rrrrrrrrrrrr]
     2x_1& - & x_2 & + &x_3 & = & 2 \\
        x_1  & - & 4x_2 &  & & = & 1 \\
     2x_1 & + & 6x_2 & - & &x_3 = & 0,
  \end{matrix}
  \]
  para $x_i \in \mathbb{R} (1 \leq i \leq 4).$
\end{ej}\pagebreak 

Encontraremos una matriz que nos dara un sistema de ecuaciones equivalente, pero con soluciones mucho mas evidentes: \[
  \begin{bmatrix}[rrr|r]
   2 & -1 & 1 & 2 \\
   1 & -4 & 0 & 1 \\
   2 & 6 & -1 & 0 
 \end{bmatrix}
\xrightarrow{F_1 \leftrightarrow F_2} 
\begin{bmatrix}[rrr|r]
   1 & -4 & 0 & 1 \\
   2 & -1 & 1 & 2 \\
   2 & 6 & -1 & 0 
 \end{bmatrix}
\xrightarrow{F_2 - 2F_1} 
\begin{bmatrix}[rrr|r]
   1 & -4 & 0 & 1 \\
   0 &  7 & 1 & 0 \\
   2 & 6 & -1 & 0 
\end{bmatrix}
\]
\[
\begin{bmatrix}[rrr|r]
   1 & -4 & 0 & 1 \\
   0 &  7 & 1 & 0 \\
   2 & 6 & -1 & 0 
\end{bmatrix}
\xrightarrow{F_3-2F_1} 
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 1 & 0 \\
  0 & 14 & -1 & -2 
\end{bmatrix}
\xrightarrow{F_3-2F_2} 
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 1 & 0 \\
  0 &  0 & -3 & -2 
\end{bmatrix}
\]
\[
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 1 & 0 \\
  0 &  0 & -3 & -2 
\end{bmatrix}
\xrightarrow{F_3/(-3)}
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 1 & 0 \\
  0 &  0 & 1 & \sfrac{2}{3} 
\end{bmatrix}
\xrightarrow{F_2-F_3}
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 0 & -\sfrac{2}{3} \\
  0 &  0 & 1 & \sfrac{2}{3} 
\end{bmatrix}
\]
\[
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 7 & 0 & -\sfrac{2}{3} \\
  0 &  0 & 1 & \sfrac{2}{3} 
\end{bmatrix}
\xrightarrow{F_2/7} 
\begin{bmatrix}[rrr|r]
  1 & -4 & 0 & 1 \\
  0 & 1 & 0 & -\sfrac{2}{21} \\
  0 &  0 & 1 & \sfrac{2}{3} 
\end{bmatrix}
\xrightarrow{F_1+4F_2}
\begin{bmatrix}[rrr|r]
  1 & 0 & 0 & \sfrac{13}{21} \\
  0 & 1 & 0 & -\sfrac{2}{21} \\
  0 &  0 & 1 & \sfrac{2}{3} 
\end{bmatrix}.
\]
Volvamos a las ecuaciones: el nuevo sistema de ecuaciones, equivalente al original, es \[
  \begin{matrix}
    x_1=\frac{13}{21}\\
    x_2=-\frac{2}{21}\\
    x_3=\frac{2}{3},
  \end{matrix}
\]
por lo tanto, el sistema tiene una sola solucion: \[
  \left(\frac{13}{21},-\frac{2}{21},\frac{2}{3}\right).
\]
\begin{center}
\textbf{Sistemas homogeneos.}
\end{center}
Si el sistema de ecuaciones lineales es homogeneo, es decir del tipo $AX=0$, entonces la matriz ampliada es \[[A|0].\] Haciendo operaciones elementales sucesivas llegamos a otra matriz \[
[B|0].
\]
Luego, en este caso (sistema homogeneo) la convencion es no escribir la matriz ampliada para resolver el sistema, sino trabajar directamente sobre la matriz $A$.
\pagebreak

\begin{defi}
Una matriz $A$ de $m \times n$ se llama reducida por filas o $MRF$ si \begin{enumerate}[label=\textcolor{azulp2}{(\alph*)}]
    \item  la primera entrada no nula de una fila de $A$ es $1$. Este $1$ es llamado $1$ principal.
    \item  Cada columna de $A$ que contiene un $1$ principal tiene todos los otros elementos iguales a $0$.
 \end{enumerate}
\end{defi}
\begin{ej}
  Las siguientes matrices son $MRF$: \[
    \begin{bmatrix}[rrr] 1 & 0 & -1 \\ 0 & 1 & 3 \\ 0 & 0 & 0 \end{bmatrix} \quad \quad \begin{bmatrix}[rrr] 0 & 1 & 3 \\ 1 & 0 & -1 \\ 0 & 0 & 0 \end{bmatrix} ;
  \]
\end{ej}
\begin{ej}
  Las siguientes matrices, no son $MRF$: \[
    \begin{bmatrix} 1 & 0 & 1 \\ 0 & 2 & 3 \\ 0 & 0 & 0 \end{bmatrix} \text{no cumple (a)}, \quad \quad \begin{bmatrix}[rrr] 1 & 0 & -1 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{bmatrix} \text{no cumple (b).}
  \]
\end{ej}
\begin{defi}
  Una matriz $A$ de $m \times n$ es escalon reducida por fila o $MERF$ si, es $MRF$ y 
\begin{enumerate}[label = \textcolor{azulp2}{(\alph*)}, start=3]    \item todas las filas cuyas entradas son todas iguales a cero estan al final de la matriz, y 
  \item en dos filas consecutivas no nulas el $1$ principal de la fila inferior esta mas a la derecha que el $1$ principal de la fila superior.
 \end{enumerate}
\end{defi}
\begin{obs}
  Es muy facil obtener la solucion de un sistema $AX=Y$ donde $A$ es una $MERF$. 
\end{obs}
\begin{ej}
  La solucion del sistema $AX=Y$ con \[
    A=\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \quad  \text{e} \quad Y=\begin{bmatrix}[r] -1 \\ 0 \\ 1 \end{bmatrix}
  \]
  es $(x_1,x_2,x_3)=(-1,0,1)$.
\end{ej}
En efecto, si escribimos explicitamente el sistema la solucion queda determinada automaticamente: \[
  \left\{ \begin{array}{llr}x_1 & = & -1 \\ x_2 & = & 0 \\ x_3 & = & 1 \end{array}\right. .
\] \pagebreak

\begin{ej}
  El conjunto de soluciones del sistema $AX=Y$ con \[
    A=\begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & -\sfrac{1}{3} \end{bmatrix} \quad \text{e} \quad Y=\begin{bmatrix} 1 \\ -\sfrac{1}{3}\end{bmatrix}
  \]
  es 
  \[ 
    \left\{\left(-2x_3+1, \frac{1}{3}x_3-\frac{1}{3},x_3\right) : x_3 \in \mathbb{R}\right\}.
  \]
  En efecto, si escribimos explicitamente el sistema: \[
    \left\{\begin{array}{llr} x_1 + 2x_3 & = & 1 \\ x_2 - \frac{1}{3} & = & -\frac{1}{3}\end{array}\right. \quad \quad \Longrightarrow \quad \quad \begin{array}{lll} x_1 & = & -2x_3 + 1 \\x_2 & = & \frac{1}{3} x_3 - \frac{1}{3} \end{array}.
  \]
\end{ej}
\begin{teo}
  Toda matrix $m \times n$ sobre $\mathbb{K}$ es equivalente por fila a una matriz escalon reducida por fila.
\end{teo}
\textbf{Idea de la demostracion.} 
\begin{enumerate}[label=\textcolor{azulp2}{P\arabic*.}]
  \item Nos ubicamos en la primera fila.
  \item Si la fila es $0$ y no es la ultima, pasar a la fila siguiente y de nuevo \textcolor{azulp2}{P2}.
  \item Si la fila no es $0$, 
    \begin{enumerate}[label=\textcolor{azulp2}{P3.\arabic*}]
  \item si la primera entrada no nula esta en la columna $k$ y su valor es $c$, dividir la fila por $c$ (ahora la primera entrada no nula vale $1$),
  \item con operaciones elementales del tipo $F_r+tF_s$ hacer $0$ todas las entradas en la columna $k$ (menos la de la columna actual).
\end{enumerate}
\item Si la fila no es la ultima, pasar a la fila siguiente e ir a \textcolor{azulp2}{P2}.
\item Intercambiando las filas, ponemos los $1$ principal de forma escalonada y las filas nulas al final.
\end{enumerate}
\qed
\begin{obs}
  La demostracion del teorema anterior nos da un algoritmo para encontrar $MERF$.
\end{obs}
\pagebreak

\begin{center}
\textbf{Metodo de Gauss.}
\end{center}
Sea $AX=Y$ el sistema de ecuaciones lineales. El metodo de Gauss consiste en llevar a cabo los siguientes pasos:
\begin{list}{$\circ$}{}  
\item Escribir $[A|Y]$ la matriz ampliada del sistema. 
\item Reducimos la matriz $[A|Y] \leadsto [R|Z]$ de tal forma que $R$ sea $MERF$ (para ello utilizamos el algoritmo P1$\cdots$P5).
\item El sistema $RX=Z$ tiene soluciones faciles de encontrar.
\end{list}
Claramente la parte mas importante del metodo de Gauss es utilizar el algoritmo P1$\cdots$P5, que llamaremos el algoritmo de Gauss-Jordan o eliminacion de Gauss-Jordan.
\begin{center}
\textbf{Algoritmo de Gauss-Jordan.}
\end{center}
Sea $A$ una matriz $m \times n$. Repitamos el algoritmo: 
\begin{enumerate}[label=\textcolor{azulp2}{P\arabic*.}]
    \item Nos ubicamos en la primera fila. 
    \item Si la fila es $0$ y no es la ultima, pasar a la fila siguiente y de nuevo \textcolor{azulp2}{P2}.
    \item Si la fila no es $0$, \begin{enumerate}[label=\textcolor{azulp2}{P3.\arabic*}] 
      \item si la primera entrada no nula esta en la columna $k$ ysu valor es $c$, dividir la fila por $c$ (ahora la primera entrada no nula vale $1$),
      \item con operaciones elementales del tipo $F_r+tF_s$ hacer $0$ todas las entradas en la columna $k$ (menos la de la columna actual).
\end{enumerate}
\item Si la fila no es la ultima, pasar a la fila siguiente e ir a \textcolor{azulp2}{P2}. 
\item Intercambiando las filas, ponemos los $1$ principal de forma escalonada y las filas nulas al final.
 \end{enumerate}
La matriz que se obtiene es una $MERF$.
\begin{ej}
  Aplicaremos el algoritmo de Gauss-Jordan a la siguiente matriz: \[
    A=\begin{bmatrix}[rrrrrr] 
      1 & 0 & 2 & 1 \\
      1 & -3 & 3 & 2 \\
      2 & -3 & 5 & 3
    \end{bmatrix}
  \]
  Como la primera fila es no nula, pasamos a P3:
  \[
    A \quad \xrightarrow{F_2-F_1} \quad
    \begin{bmatrix}[rrrrr]
      1 & 0 & 2 & 1 \\
      0 & -3 & 1 & 1 \\
      2 & -3 & 5 & 3
      \end{bmatrix} \quad \xrightarrow{F_3-2F_1} \begin{bmatrix}[rrrrrr]
      1 & 0 & 2 & 1 \\
      0 & -3 & 1 & 1 \\
      0 & -3 & 1 & 1 
    \end{bmatrix}
\]
Por P4 pasamos a trabajar con la fila $2$ y pasamos a $P2$. Como al fija no es nula hacemos $P3.1$ \[
  \xrightarrow{F_2-\frac{F_3}{3}} \quad \begin{bmatrix}[rrrr]1 & 0 & 2 & 1 \\ 0 & 1 & -\sfrac{1}{3} & -\sfrac{1}{3} \\ 0 & -3 & 1 & 1 \end{bmatrix}
\]
Ahora P3.2: \[
  \xrightarrow{F_3+3F_2} \begin{bmatrix}[rrrrrr] 1 & 0 & 2 & 1 \\ 0 & 1 & -\sfrac{1}{3} & -\sfrac{1}{3} \\ 0 & 0 & 0 & 0 \end{bmatrix}
\]
Esta ultima matriz es una $MERF$. En este caso, no hizo falta usar P5, es decir permutar filas.
\end{ej} 
\begin{ej}[Metodo de Gauss]
  A continuacion explicaremos en $3$ pasos el metodo de Gauss para resolver el sistema de ecuacioens \[
AX=Y.
  \]
\end{ej}
Ejemplificaremos los pasos con el sistema \begin{equation}\tag{E}
  \begin{bmatrix}[rrr]1 & 0 & 2 \\ 1 & -3 & 3 \\ 2 & -3 & 5 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\end{equation}

\textbf{Primer paso: Matriz ampliada}
\begin{list}{$\circ$}{}  
\item  Armar la matriz ampliada: \[
    A'=[A|Y],
  \]
  es decir, le agregamos a $A$ una columna igual a $Y$.
\end{list}
En nuestro caso, la matriz ampliada del sistema $(E)$ es \[
  \begin{bmatrix}[rrr|r] 
    1 & 0 & 2 & 1 \\
    1 & -3 & 3 & 2 \\
    2 & -3 & 5 & 3 
  \end{bmatrix}
Observar que esta matriz es igual a la del ejemplo anterior.
\]
\textbf{Segundo paso: reducir la matriz ampliada (Gauss-Jordan)}
\begin{list}{$\circ$}{}  
\item  Usar operaciones elementales por filas para transformar la matriz ampliada $A'$ en una matriz $B'$ de la forma \[
    B'=[B|Z]
  \]
  donde $B$ es una $MERF$ y $Z$ es una columna.
\end{list}
En el ejemplo: \[
  A'= \begin{bmatrix}[rrr|r] 1 & 0 & 2 & 1 \\ 1 & -3 & 3 & 2 \\ 2 & -3 & 5 & 3 \end{bmatrix}\quad \xrightarrow{F_2-F_1} \quad
    \begin{bmatrix}[rrr|r]
      1 & 0 & 2 & 1 \\
      0 & -3 & 1 & 1 \\
      2 & -3 & 5 & 3
      \end{bmatrix} \quad \xrightarrow{F_3-2F_1}\quad \begin{bmatrix}[rrr|rrr]
      1 & 0 & 2 & 1 \\
      0 & -3 & 1 & 1 \\
      0 & -3 & 1 & 1 
    \end{bmatrix}\]
    \[
      \xrightarrow{F_2-\frac{F_3}{3}} \quad \begin{bmatrix}[rrr|r]1 & 0 & 2 & 1 \\ 0 & 1 & -\sfrac{1}{3} & -\sfrac{1}{3} \\ 0 & -3 & 1 & 1 \end{bmatrix} \quad \xrightarrow{F_3+3F_2} \quad \begin{bmatrix}[rrr|r] 1 & 0 & 2 & 1 \\ 0 & 1 & -\sfrac{1}{3} & -\sfrac{1}{3} \\ 0 & 0 & 0 & 0 \end{bmatrix} = B'
\]
\pagebreak 

\textbf{Tercer paso: despejar y describir el conjunto de soluciones}
\begin{list}{$\circ$}{}  
\item Escribir explicitamente el sistema $BX=Z$. 
\item Despejar en cada ecuacion la incognita correspondiente al $1$ principal.
\item Describir el conjunto de soluciones. Hay tres opciones: tener solo una solucion; infinitas, parametrizadas por las incognitas que no corresponden a $1$'s principales; no tener solucion.
\end{list}
En el ejemplo: \[
  BX=Z \leadsto \left\{\begin{array}{lll} x_1 + 2x_3 = 1 \\ x_2 - \frac{1}{3}x_3 = -\frac{1}{3}x_3 \end{array}\right. \leadsto \begin{array}{lll} x_1=-2x_3+1 \\
x_2=\frac{1}{3}x_3-\frac{1}{3}x_3\end{array}
\]
\begin{ej}
  Resolver el sistema de ecuaciones: \[
    (E)\left\{\begin{array}{ll}x_2 -x_3 + x_4 & =1 \\ 2x_3 + x_4 & = 3 \\ x_1 + x_2 - x_4 & = 1 \\ x_1 + 2x_2 - _3 & = 2\end{array} \right.
  \]
\textbf{Solucion.} El conjunto de soluciones es \[
Sol(E)=\left\{\left[\frac{5}{2}x_4-\frac{3}{2},-\frac{3}{2}x_4+\frac{5}{2},-\frac{1}{2}x_4+\frac{3}{2},x_4\right] : x_4 \in \mathbb{R}\right\}.
\]
Reducimos la matriz ampliada siguiendo al pie de la letra el algoritmo: \[
  A'=\begin{bmatrix}[rrrr|r]
    0 & 1 & -1 & 1 & 1 \\
    0 & 0 & 2 & 1 & 3 \\
    1 & 1 & 0 & -1 & 1 \\
    1 & 2 & -1 & 0 & 2 \\
  \end{bmatrix} \underset{F_4-2F_1}{\xrightarrow{F_3-F_1}} \begin{bmatrix}[rrrr|r] 
    0 & 1 & -1 & 1 & 1 \\
    0 & 0 & 2 & 1 & 3 \\
    1 & 0 & 1 & -2 & 0 \\
    1 & 0 & 1 & -2 & 0 
  \end{bmatrix}
\]
\[
  \xrightarrow{\frac{1}{2}F_2} \begin{bmatrix}[rrrr|r] 
    0 & 1 & -1 & 1 & 1 \\
    0 & 0 & 1 & \sfrac{1}{2} & \sfrac{3}{2} \\
    1 & 0 & 1 & -2 & 0 \\
    1 & 0 & 1 & -2 & 0 
  \end{bmatrix}
  \underset{\begin{subarray}{l} F_3-F_2 \\ F_4 - F_2 \end{subarray}}{\xrightarrow{F_1+F_2}} \begin{bmatrix}[rrrr | r] 0 & 1 & 0 & \sfrac{3}{2} & \sfrac{5}{2} \\ 
0 & 0 & 1 & \sfrac{1}{2} & \sfrac{3}{2} \\
1 & 0 & 0 & -\sfrac{5}{2} & -\sfrac{3}{2} \\
1 & 0 & -\sfrac{5}{2} & -\sfrac{3}{2} 
\end{bmatrix} 
\]
\[
  \xrightarrow{F_4-F_3} \begin{bmatrix}[rrrr|r] 0 & 1 & 0 & \sfrac{3}{2} & \sfrac{5}{2} \\
    0 & 0 & 1 & \sfrac{1}{2} & \sfrac{3}{2} \\
    1 & 0 & 0 & -\sfrac{5}{2} 
  \end{bmatrix} = B' \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad \quad \quad \quad 
\]\pagebreak 

Si reescribimos las ecuaciones a partir de $B'$, obtenemos \[
  (E')\begin{dcases} x_2 + \frac{3}{2}x_4 = \frac{5}{2} \\x_3 + \frac{1}{2}x_4 = \frac{3}{2} \\ x_1 - \frac{5}{2}x_4 = -\frac{3}{2}  \end{dcases}\]
  Si despejamos respecto a $x_4$, obtenemos \[
    x_1=\frac{5}{2}x_4-\frac{3}{2}, \quad \quad x_2=-\frac{3}{2}x_4+\frac{5}{2}, \quad \quad x_3 = -\frac{1}{2}x_4 + \frac{3}{2}, 
  \] que es la solucion del sistema.
\end{ej}
\begin{center}
\textbf{Soluciones de los sistemas de ecuaciones lineales.}
\end{center}
¿Como saber si el sistema tiene o no tiene solucion? ¿Una o infinitas? \\\\
Las respuestas a estas preguntas dependen de la forma de $B$ y $Z$. \\\\ Asumamos que $B \in \mathbb{R}^{m \times n}$ y $Z \in \mathbb{R}^{m \times 1}$ donde \[
  B=\begin{bmatrix} 0 & \cdots & 1 & * & 0 & * & * & 0 & * & * \\ 0 & \cdots & 0 & \cdots & 1 & * & * & 0 & * & * \\ 
    \vdots & & \vdots & & \vdots & & & \vdots & & \vdots \\
    0 & \cdots & 0 & \cdots & 0 & \cdots & \cdots & 1 & * & * \\ 
    0 & \cdots & 0 & \cdots & 0 & \cdots & \cdots & 0 & \cdots & 0 \\
    \vdots & & \vdots & & \vdots & & & \vdots & & \vdots \\
    0 & \cdots & 0 & \cdots & 0 & \cdots  & \cdots & 0 & \cdots & 0 \end{bmatrix} 
    \quad \text{ y } \quad Z = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_r \\ z_{r+1} \\ \vdots \\ z_m \end{bmatrix} 
\]
Si $k_1, \dots, k_r$ son las columnas que contienen $1$ principales, el sistema $BX=Z$ tiene la siguiente forma \[
  \left\{\begin{array}{ccccc} 
      x_{k_{1}} & + & \sum_{j\neq k_1, \dots, k_r} b_{1j}x_{j} & = & z_1 \\
      x_{k_{2}}& + & \sum_{j\neq k_1, \dots, k_r} b_{2j}x_{j} & = & z_2 \\
      \vdots & && \vdots  \\
      x_{k_{r}} & + & \sum_{j \neq k_1, \dots, k_r}b_{rj}x_{j} & = & z_r \\
              & & 0 & = & z_{r+1} \\
              &&\vdots &&\vdots \\
              &&0 & = & z_m
  \end{array}\right.
\]
\begin{teo}
  El sistema $BX=Z$ tiene solucion si y solo si $$Z_{r+1}=z_{r+2}=\cdots =z_{m}=0.$$
\end{teo}
\begin{demo}[$\Rightarrow$]\;\\
  El sistema $BX=Z$ tiene solucion $\Rightarrow$ $Z_{r+1}=z_{r+2}=\cdots =z_{m}=0.$\\\\
  Si el sistema tiene solucion entonces $Z_{r+1}=z_{r+2}=\cdots =z_{m}=0$, pues si alguno de estos $z$'s fuera no nulo tendriamos un absurdo.
\[
  \left\{\begin{array}{ccccc} 
      x_{k_{1}} & + & \sum_{j\neq k_1, \dots, k_r} b_{1j}x_{j} & = & z_1 \\
      x_{k_{2}}& + & \sum_{j\neq k_1, \dots, k_r} b_{2j}x_{j} & = & z_2 \\
      \vdots & && \vdots  \\
      x_{k_{r}} & + & \sum_{j \neq k_1, \dots, k_r}b_{rj}x_{j} & = & z_r \\
              & & \vdots & & \vdots \\
              &&0 &&z_i \neq 0 \\
              &&\vdots &  & \vdots
  \end{array}\right.
\]
\end{demo}
\begin{demo}[$\Leftarrow$] \;\\
  Si $z_{r+1}=z_{r+2}= \cdots = z_{m} = 0 \Rightarrow$ el sistema $BX=Z$ tiene solucion.\\\\
  Si $z_{r+1}=z_{r+2}= \cdots = z_{m} = 0$, obtenemos \[
 \left\{\begin{array}{ccccc} 
      x_{k_{1}} & = & -\sum_{j\neq k_1, \dots, k_r} b_{1j}x_{j} & + & z_1 \\
      x_{k_{2}}& = & -\sum_{j\neq k_1, \dots, k_r} b_{2j}x_{j} & + & z_2 \\
      \vdots & & \vdots  \\
 x_{k_{r}} & = & -\sum_{j\neq k_1, \dots, k_r} b_{rj}x_{j} & + & z_r \\
  \end{array}\right. .
  \]
  Entonces $x_{k_{1}}=z_{1}, x_{k_{2}}=z_2,\dots, x_{k_{r}}=z_r$ y todos los otros $x_j=0$ es una solucion al sistema.\\
\qed
\end{demo}
En la demostracion de $(\Leftarrow)$ encontramos una solucon haciendo cero todas las incognitas que no corresponden con $1$ principales. \\\\ Pero le podriamos haber dado cualquier otro valor y asi encontrar otras soluciones, siempre y cuando hubiera incognitas que no se corresponden con $1$ principales.

\begin{teo}
  Supongamos que el sistema tiene solucion y la cantidad de $1$ principales es igual a la cantidad de incognitas. Entonces el sistema $BX=Z$ tiene una unica solucion, la cual es $X=Z$. 
\end{teo}
\begin{demo}
  En este caso, el sistema $BX=Z$ tiene la siguiente forma \[
    \left\{
      \begin{array}{ccc} 
        x_1 & = & z_1 \\
        x_2 & = & z_2 \\
        \vdots & & \vdots \\
        x_n & = & z_n \\
        0 & = & z_{n+1} \\
        \vdots & & \vdots \\
        0 & = & z_{m}
      \end{array}
    \right.
  \]
  y la solucion queda determinada explicitamente. \\ 
  \qed 
\end{demo}\;
\begin{teo}
  Supongamos que el sistema tiene solucion y hay mas incognitas que $1$ principales. Entonces el sistema $BX=Z$ tiene infinitas soluciones de la forma 
 \[
 \begin{array}{ccccc} 
      x_{k_{1}} & = &z_1 -\sum_{j\neq k_1, \dots, k_r} b_{1j}x_{j} \\
      x_{k_{2}}& = &z_2 -\sum_{j\neq k_1, \dots, k_r} b_{2j}x_{j} &  \\
      \vdots & & \vdots  \\
 x_{k_{r}} & = &z_r -\sum_{j\neq k_1, \dots, k_r} b_{rj}x_{j} & \\
  \end{array}  \]
  y los $x_j$ con $j \neq k_1, \dots, k_r$, pueden tomar cualquier valor real.
\end{teo}
\begin{demo}
  Como existe al menos un $j \neq k_1, \dots, k_r$ variando los $x_j$ donde $j \neq k_1, \dots, k_r$ obtenemos infinitas soluciones. \qed
\end{demo}\;\\
\textbf{Conclusion.} Para saber si el sistema $AX=Y$ tiene o no soucion, una o infinitas, lo transformamos a uno de la forma $BX=Z$ con $B$ $MERF$ usando operaciones elementales por filas y aplicamos los teoremas anteriores. \\\\
\textbf{Caso particular.} Un sistema $AX=Y$ se dice homogeneo si $Y=0$. \\\\
 Estos sistemas siempre tienen solucion pues $X=0$ es una solucion.
\pagebreak 
\section{Algebra de matrices.}
\begin{center}
\textbf{Matrices.}
\end{center}
Recordemos: sea $\mathbb{K}$ cuerpo. 
\begin{list}{$\circ$}{}  
\item  Una matriz $A$ es un elemento de $(\mathbb{K}^n)^m$ que se escribe con $m$-filas y $n$-columnas \[
    A= \underbrace{\left.\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\
      \vdots & \vdots & & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix}\right\}}_{n\text{-columnas.}} m\text{-filas.}   \]
\item A cada elemento de la matriz la llamamos entrada o coeficiente. 
  \item Si $A$ es una matriz $ m \times n$, denotamos $[A]_{ij}$ la entrada que se ubica en la fila $i$ y la columna $j$. En la matriz de arriba $[A]_{ij}=a_{ij}$.
  \item El conjunto de matrices de orden $m \times n$ con entradas en $\mathbb{K}$ lo denotamos $\mathbb{K}^{m \times n}$, o $M_{m \times n}(\mathbb{K})$, o simplemente $M_{m \times n}$.
\end{list}
\begin{center}
\textbf{Matriz cuadrada.}
\end{center}
Una matriz $A \in \mathbb{K}^{n \times n}$ se dice cuadrada de orden $n$ porque tiene igual cantidad de filas que de columnas.
\[
  A=\begin{bmatrix} 
    a_{11} & a_{12} & \cdots & a_{1n} \\
  a_{21} & a_{22} & \cdots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{n1} & a_{n2} & \cdots & a_{nn}
  \end{bmatrix}
\]
Los elementos de la diagonal principal son $a_{ii}$ con $1 \leq i \leq n$.\\\\
\textbf{Matriz cuadrada: ejemplos.}
\[\begin{aligned}
  A&=\begin{bmatrix}[rrrr] 5 & -1 & 3 & 4 \\ 0 & 0 & 1 & 0 \\ 1 & -2 & 7 & 5 \\ -1 & 2 & 128 & -20 \end{bmatrix}\\
  B&= \begin{bmatrix}[rrr] 5 & 3 & 4 \\ 0 & 0 & 0 \\ 1 & 7 & 5 \end{bmatrix} \\
  C&= \begin{bmatrix} 1 \end{bmatrix}
\end{aligned}
\]
\pagebreak

\begin{center}
\textbf{Matriz diagonal.}
\end{center}
Una matriz cuadrada $D \in \mathbb{K}^{n \times n}$ se dice diagonal de orden $n$ si todas las entradas fuera de la diagonal son nulas. \[
  D=\begin{bmatrix} d_1 & 0 & 0 & \cdots & 0 \\
    0 & d_2 & 0 & \cdots & 0 \\
    0 & 0 & d_3 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & d_n
  \end{bmatrix}
\]
Las entradas de $D$ se pueden describir como sigue \[
  \begin{bmatrix}D\end{bmatrix}_{ij} = \begin{dcases}
  d_i & \text{si }  i = j  \\
0 & \text{si } i \neq j \end{dcases}
\]
\textbf{Matriz diagonal: ejemplos} \[\begin{aligned}
  A&=\begin{bmatrix}5 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & -5 & 0 \\
  0 & 0 & 0 & -20 \end{bmatrix} \\
    B&=\begin{bmatrix}5 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 7 \end{bmatrix} \\
    C&=\begin{bmatrix}1\end{bmatrix}
  \end{aligned}
\]
\begin{center}
\textbf{Matriz escalar.}
\end{center}
Una matriz cuadrada $E \in \mathbb{K}^{n \times n}$ se dice escalar de orden $n$ si es diagonal y todos los elementos de la diagonal son iguales, por ejemplo, en el caso $4 \times 4$ las matrices escalares son \[
  E=\begin{bmatrix} c & 0 & 0 & 0 \\ 0 & c & 0 & 0 \\ 0 & 0 & c & 0 \\ 0 & 0 & 0 & c \end{bmatrix} 
\]con $ c \in \mathbb{K}$.\\\\
Las entradas de $E$ se pueden describir como sigue \[
  \begin{bmatrix}E\end{bmatrix}_{ij} = \begin{dcases}
   c & \text{si }  i = j  \\
0 & \text{si } i \neq j \end{dcases}
\]
\textbf{Matriz escalar: ejemplos} \[
  \begin{aligned}
    A&=\begin{bmatrix} 2 & 0 & 0 & 0 \\ 0 & 2 & 0 & 0 \\ 0 & 0 & 2 & 0 \\ 0 & 0 & 0 & 2 \end{bmatrix} \\
    B&= \begin{bmatrix} 5 & 0 & 0 \\ 0 & 5 & 0 \\ 0 & 0 & 5 \end{bmatrix} \\
    C&= \begin{bmatrix} 1 \end{bmatrix}
  \end{aligned}
\]
\begin{center}
\textbf{Matriz identidad.}
\end{center}
La matriz diagonal de orden $n$ con todos unos en la diagonal se llama matriz identidad de orden $n$ y se denota $Id_n$.\[
  Id_n=\begin{bmatrix} 1 & 0 & 0 & \cdots & 0 \\
    0 & 1 & 0 & \cdots & 0 \\
    0 & 0 & 1 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & 1
  \end{bmatrix}
    \]
    Las entradas de $Id_n$ se pueden describir como sigue \[
    \begin{bmatrix}Id_n\end{bmatrix}_{ij} = \begin{cases} 1 & \text{si } i = j \\
    0 & \text{si } i \neq j 
  \end{cases}
  \]
  A veces escribiremos simplemente $Id$, omitiendo el subindice $n$.
\begin{center}
\textbf{Matriz nula.}
\end{center}
La matriz nula de orden $m \times n$ es la matriz cuyas entradas son todas ceros. Se la denota $0$.
\[
  0:=\begin{bmatrix} 0 & 0 & 0 & \cdots & 0 \\
    0 & 0 & 0 & \cdots & 0 \\
    0 & 0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & 0
  \end{bmatrix}
\]
Las entradas de $0$ se pueden describir como sigue \[
  \begin{bmatrix}0\end{bmatrix}_{ij}= 0 \quad \forall i,j 
\]
\pagebreak
\begin{center}
\textbf{Matriz triangular superior.}
\end{center}
Una matriz cuadrada cuyas entradas por debajo de la diagonal principal son cero se llama matriz triangular superior.\[
A=\begin{bmatrix} a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
  0 & a_{22} & a_{23} & \cdots & a_{2n} \\
  0 & 0 & a_{33} & \cdots & a_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & a_{nn}
  \end{bmatrix}
\]
En formula, $A$ es triangular superior si $a_{ij}=0$ para todo $i < j$.\\\\
\textbf{Matriz triangular superior: ejemplos.}
\[
  \begin{aligned}
    A&=\begin{bmatrix}2 & 1 & 0 & 7 \\ 0 & -1 & -4 &5 \\ 0 & 0 & 5 & 3 \\ 0 & 0 & 0 & 3 \end{bmatrix} \\
    B&=\begin{bmatrix}5 & 3 & 2 \\ 0 & 1 & 1 \\ 0 & 0 & -1 \end{bmatrix} \\
    C&=\begin{bmatrix}1 \end{bmatrix}
  \end{aligned}
\]
\begin{center}
\textbf{Matriz triangular inferior.}
\end{center}
Una matriz cuadrada cuyas entradas por encima de la diagonal principal son cero se llama matriz triangular inferior. \[
A=\begin{bmatrix} a_{11} & 0 & 0 & \cdots & 0 \\
  a_{21} & a_{22} & 0 & \cdots & 0 \\
  a_{31} & a_{32} & a_{33} & \cdots & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n2} & a_{n3} & \cdots & a_{nn}
  \end{bmatrix}
\]
En formula, $A$ es triangular inferior si $a_{ij}=0$ para todo $i > j$. \pagebreak

\begin{center}
\textbf{Suma de matrices.}
\end{center}
\begin{defi}
  Sean $A,B \in \mathbb{K}^{m \times n}$, la suma $A+B$ es la matriz que resulta de sumar ``coordenada a coordenada'' las matrices $A$ y $B$.\\\\
  En simbolos, \[
    A+B\in\mathbb{K}^{m \times n} \text{ con } \begin{bmatrix}A+B\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ij}+\begin{bmatrix}B\end{bmatrix}_{ij}.
  \]
\end{defi}
\begin{ej}
  Si $A=\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$ y $B=\begin{bmatrix}10 & 20 & 30 \\ 40 & 50 & 60 \end{bmatrix}$ entonces \[
  A+B=\begin{bmatrix}1 + 10 & 2 + 20 & 3 + 30 \\ 4 + 40 & 5 + 50 & 6 + 60 \end{bmatrix} = \begin{bmatrix}11 & 22 & 33  \\ 44 & 55 & 66 \end{bmatrix}
\]
\end{ej}
Podemos visualizar la suma de matrices asi: \[
  \begin{aligned} 
    \phantom{=}&\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\
      a_{21} & a_{22} & \cdots & a_{2n}\\
      \vdots & \vdots & & \vdots \\
      a_{m1} & a_{m2} & \cdots & a_{mn}
\end{bmatrix} + \begin{bmatrix} b_{11} & b_{12} & \cdots & b_{1n} \\
   b_{21} & b_{22} & \cdots & b_{2n}\\
      \vdots & \vdots & & \vdots \\
      b_{m1} & b_{m2} & \cdots & b_{mn}
\end{bmatrix} \\
               = &\begin{bmatrix} a_{11}+b_{11} & a_{12}+b_{12} & \cdots & a_{1n}+b_{1n} \\
      a_{21}+b_{21} & a_{22}+b_{22} & \cdots & a_{2n}+b_{2n}\\
      \vdots & \vdots & & \vdots \\
      a_{m1}+b_{m1} & a_{m2}+b_{m2} & \cdots & a_{mn}+b_{mn}
\end{bmatrix} 
  \end{aligned}
\]
\begin{center}
\textbf{Propiedades de la suma de matrices.}
\end{center}
La suma de matrices satisface las mismas propiedades que la suma de numeros reales y complejos (y enteros).\\\\
\textbf{Proposicion.} \\\\
Si $A,B,C$ son matrices $m \times n$, entonces \\\\ $\begin{array}{llll} \textcolor{azulp2}{\text{S1.}} & A+B=B+A & \text{(conmutatividad de la suma)} \\ 
  \textcolor{azulp2}{\text{S2.}} & A+(B+C)=(A+B)+C & \text{(asociatividad de la suma)} \\ 
  \textcolor{azulp2}{\text{S3.}} &A+0=A & \text{(elemento neutro)} \\ 
 \textcolor{azulp2}{\text{S4.}} & A+(-A)=0 & \text{(existencia de opuesto)} \\ 
\end{array}$ \\\\
donde \[
-A=\begin{bmatrix} -a_{11} & \cdots & -a_{1n} \\
      \vdots & \ddots  & \vdots \\
      -a_{m1} & \cdots & -a_{mn}
\end{bmatrix}
\]
\pagebreak

\textbf{Notacion.}
\begin{list}{$\circ$}{}  
\item  Debido a la propiedad asociativa podemos eliminar los parentesis en una suma, es decir, denotaremos \[
  A+B+C := A+(B+C)=(A+B)+C.\]
\item Usualmente denotaremos \[ \begin{aligned}
    &A-B :=A+(-B), \\
    -&A+B:=(-A)+B.
  \end{aligned}
  \]
\end{list}
La demostracion de las propiedades anteriores se deduce de que las mismas propiedades valen coordenada a coordenada en $\mathbb{K}$.\\\\
Demostraremos la asociatividad y las otras propiedades se dejan a cargo del lector.
\\\\
\textbf{Demostracion de la asociatividad.} \\\\
Queremos ver que las matrices $A+(B+C)$ y $(A+B)+C$ son iguales. O sea, que cada una de sus coordenadas son iguales. Esto es cierto por lo siguiente: \[
  \phantom{=}\begin{bmatrix}A+(B+C)\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ij}+\begin{bmatrix}B+C\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ij}+\left(\begin{bmatrix}B\end{bmatrix}_{ij}+ \begin{bmatrix}C\end{bmatrix}_{ij} \right) 
\]
\[
  = \left( \begin{bmatrix} A \end{bmatrix}_{ij} + \begin{bmatrix}B\end{bmatrix}_{ij} \right) + \begin{bmatrix}C \end{bmatrix}_{ij}=\begin{bmatrix}A+B\end{bmatrix}_{ij}+\begin{bmatrix}C\end{bmatrix}_{ij}=\begin{bmatrix}(A+B)+C)\end{bmatrix}_{ij}
\]
\qed
\begin{center}
\textbf{Producto de matrices.}
\end{center}
\begin{defi}
  Sean $A \in \mathbb{K}^{m \times n}$ y $B\in \mathbb{K}^{n \times p}$. \\\\
  El producto $A \cdot B$ es una matriz de orden $m \times p$ cuyas entradas son definidas por la siguiente formula \[
  \begin{bmatrix} A \cdot B \end{bmatrix}_{ij}=\sum_{k = 1 } ^ {n} {\begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix} B \end{bmatrix}_{kj}}.
  \]
\end{defi}
 Podemos visualizar la multiplicacion asi: \[
  \begin{bmatrix} \vdots & \vdots & & \vdots \\
    \textcolor{rojop2}{a_{i1}} & \textcolor{rojop2}{a_{i2}} & \textcolor{rojop2}{\cdots} & \textcolor{rojop2}{a_{in}} \\
    \vdots & \vdots && \vdots \end{bmatrix} \cdot \begin{bmatrix}\cdots & \textcolor{azulp2}{b_{ij}} & \cdots \\
    \cdots & \textcolor{azulp2}{b_{2j}} & \cdots \\
           & \textcolor{azulp2}{\vdots} \\
    \cdots & \textcolor{azulp2}{b_{nj}} & \cdots \end{bmatrix} = \begin{bmatrix} & \vdots & \\
  \cdots & \sum_{k=1} ^ n \textcolor{rojop2}{a_{ik}}\cdot \textcolor{azulp2}{b_{kj}}& \cdots \\
         & \vdots \end{bmatrix}
\]
Es muy importante recalcar que por la definicion, se puede multiplicar una matriz $ m \times n$ por una matriz $ r \times p$, solo si $ n= r $ y en ese caso, la multiplicacion resulta ser una matriz $ m \times p$.
\begin{obs}
  Para multiplicar $(\textcolor{azulp2}{m} \times \textcolor{rojop2}{n})\cdot (\textcolor{rojop2}{n} \times \textcolor{azulp2}{p})$ el numero de columnas de la primera matriz debe coincidir con el numero de filas de la segunda matriz (las $\textcolor{rojop2}{n}$ internas deben coincidir) y queda como resultado una matriz $ \textcolor{azulp2}{m} \times \textcolor{azulp2}{p}$ (la parte externa).\end{obs}
  \begin{ej}
  Si \[
    A=\begin{bmatrix}1 & 0 \\ -3 & 1 \end{bmatrix}, \quad \quad B=\begin{bmatrix}5 & -1 & 2 \\ 15 & 4 & 8 \end{bmatrix},
  \]
  como $A$ es $2 \times 2$ y $B$ es $2 \times 3$, la matriz $AB$ sera $2 \times 3$. Obtenemos: \[
    A\cdot B = \begin{bmatrix} 1 \cdot 5 + 0 \cdot 15 & 1 \cdot (-1) + 0 \cdot 4 & 1 \cdot 2 + 0 \cdot 8 \\
    -3 \cdot 5 + 1 \cdot 15 & -3 \cdot (-1) + 1 \cdot 4 & -3 \cdot 2 + 1 \cdot 8 \end{bmatrix} 
    = 
      \begin{bmatrix} 5 & -1 & 2 \\ 0 & 7 & 2 \end{bmatrix}.
  \]
  \end{ej}
  Observemos que, debido a nuestra definicion, no es posible multiplicar $B$ por $A$, pues no esta definido multiplicar una matriz $2 \times 3$ por una $2 \times 2$.
  \begin{ej}
  Si \[
    A=\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \text{ y } B=\begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix},
  \]
  como $A$ es $2 \times 3$ y $B$ es $3 \times 2$, la matriz $AB$ sera $2 \times 2$. Obtenemos: \[
    A\cdot B = \begin{bmatrix} 
      1 \cdot 1 + 2 \cdot 3 + 3 \cdot 5 & 1 \cdot 2 + 2 \cdot 4 + 3 \cdot 6 \\
      4 \cdot 1 + 5 \cdot 3 + 6 \cdot 5 & 4 \cdot 2 + 5 \cdot 4 + 6 \cdot 6 
      \end{bmatrix} = \begin{bmatrix} 22 & 28 \\ 49 & 64 \end{bmatrix}
  \]
  En este caso: \[
    \begin{aligned}
      \begin{bmatrix}A\cdot B \end{bmatrix}_{12} & = \sum_{k=1}^{n}{\begin{bmatrix}A\end{bmatrix}_{1k}\cdot \begin{bmatrix}B\end{bmatrix}_{k2}} \\
                                                 & = \begin{bmatrix}A\end{bmatrix}_{11} \cdot \begin{bmatrix}B\end{bmatrix}_{12} + \begin{bmatrix} A \end{bmatrix}_{12} \cdot \begin{bmatrix} B \end{bmatrix}_{22} + \begin{bmatrix}A\end{bmatrix}_{13} \cdot \begin{bmatrix} B \end{bmatrix}_{32} \\
                                                 & = 1 \cdot 2 + 2 \cdot 4 + 3 \cdot 6 \\
                                                 & = 28
    \end{aligned}
  \]
\end{ej}
\begin{obs}
  Sean $A=\begin{bmatrix}a_{ij}\end{bmatrix}$ matriz $ m \times n$ y $B = \begin{bmatrix} b_{ij} \end{bmatrix}$ matriz $ n \times p$, entonces si multiplicamos la matriz que se forma con la fila $i$ de $A$ por la matriz que determina la columna $j$ de $B$, obtenemos el coeficiente $ij$ de $AB$. Esquematicamente \[
  \begin{bmatrix} a_{i1} & a_{i2} & \cdots & a_{in} \end{bmatrix} \cdot \begin{bmatrix} b_{1j} \\ b_{2j} \\ \vdots \\ b_{nj} \end{bmatrix} = \sum_{k= 1 } ^ {n} {a_{ik}b_{kj}}=c_{ij}.
\]
\end{obs}
Por lo tanto diremos a veces, que el coeficiente $ij$ de la matriz $AB$ es la fila $i$ de $A$ por la columna $j$ de $B$. O, con abuso de notacion, \[
F_i(A) \cdot C_j(B) = \langle F_i(A), C_j(B)\rangle
\]
(producto escalar).
\begin{center}
\textbf{Propiedades del producto de matrices.}
\end{center}
Las propiedades mas basicas del producto de matrices son las siguientes.\\\\
\textbf{Proposicion.}\begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
  \item  Si $A \in \mathbb{K}^{m \times n}$, entonces \begin{equation}
      \tag{elemento neutro}
      A\cdot Id_n=Id_m \cdot A = A.
  \end{equation}
\item Si $A \in \mathbb{K}^{m \times n}, B\in \mathbb{K}^{n \times p}$ y $C \in \mathbb{K}^{p \times q}$, entonces \begin{equation} 
    \tag{asociativa}
    A\cdot(B\cdot C) = (A\cdot B)\cdot C.
  \end{equation}
\item Si $A \in \mathbb{K}^{m \times n}$ y $B,C \in \mathbb{K}^{n \times p}$ entonces \begin{equation} \tag{distributiva} A\cdot (B+C) =A\cdot B + A \cdot C. \end{equation}
\item Si $A,B \in \mathbb{K}^{m \times n}$ y $C \in \mathbb{K}^{n \times p}$, entonces \begin{equation} \tag{distributiva} (A+B)\cdot C = A\cdot C + B \cdot C.
  \end{equation}
\end{enumerate}
Las demostraciones de estas propiedades son rutinarias. Probaremos la ley asociativa y la propiedad de existencia del elemento neutro. Las leyes distributivas las dejamos a cargo del lector.\\\\
\textbf{Demostracion: 1 - $Id$ es el elemento neutro del producto.}\\\\
Notemos que las entradas de la matriz $Id_k$ son determinadas de la siguiente manera \[
  \begin{bmatrix}Id_k\end{bmatrix}_{ij} = \begin{cases} 1 & \text{si } i =j \\
0 & \text{si } i \neq j \end{cases}
\]
porque son todas ceros salvo en la diagonal (donde el numero de fila y columna coinciden). Entonces \[
  \begin{alignedat}{6}
    &\begin{bmatrix}A \cdot Id_n \end{bmatrix}_{ij} & = \sum_{k=1}^n {\begin{bmatrix}A \end{bmatrix}_{ik}\cdot\begin{bmatrix}Id_n\end{bmatrix}_{kj}} &= \begin{bmatrix}A\end{bmatrix}_{ij} \cdot 1 &= \begin{bmatrix}A\end{bmatrix}_{ij} \\
    &\begin{bmatrix}Id_m \cdot A \end{bmatrix}_{ij} &= \sum_{k=1}^m \begin{bmatrix}Id_m \end{bmatrix}_{ik} \cdot \begin{bmatrix}A\end{bmatrix}_{kj} & = 1 \cdot \begin{bmatrix}A\end{bmatrix}_{ij} & = \begin{bmatrix}A\end{bmatrix}_{ij}
  \end{alignedat}
\]\qed
\pagebreak 

\textbf{Demostracion: 2 - ley asociativa.} \\\\
Para probar la asociatividad tenemos que verificar que todas las entradas de $A\cdot(B\cdot C)$ y $(A \cdot B) \cdot C$ son iguales.\[
  \begin{aligned}
    \begin{bmatrix} A \cdot (B\cdot C) \end{bmatrix}_{ij}  
    &= \sum_{k=1}^{n}\begin{bmatrix}A\end{bmatrix}_{ik} \cdot \begin{bmatrix}B \cdot C \end{bmatrix}_{kj} \\ 
    &= \sum_{k=1}^{n}\begin{bmatrix}A\end{bmatrix}_{ik} \cdot \left(\sum_{l=1}^{p} \begin{bmatrix} B \end{bmatrix}_{kl} \cdot \begin{bmatrix}C\end{bmatrix}_{lj}\right)\\
    &= \sum_{k=1}^n \sum_{l=1}^{p} \begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix} B \end{bmatrix}_{kl} \cdot \begin{bmatrix} C \end{bmatrix}_{lj}\\
    &= \sum_{l=1}^{p} \sum_{k=1}^{n} \begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix} B \end{bmatrix}_{kl} \cdot \begin{bmatrix}C\end{bmatrix}_{lj} \\
    &= \sum_{l=1}^{p} \left(\sum_{k=1}^{n}\begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix} B \end{bmatrix}_{kl}\right) \cdot \begin{bmatrix} C \end{bmatrix}_{lj} \\
    &= \sum_{l=1}^{n} \begin{bmatrix}A \cdot B \end{bmatrix}_{il} \cdot \begin{bmatrix} C \end{bmatrix}_{lj} = \begin{bmatrix} (A \cdot B ) \cdot C \end{bmatrix}_{ij}\\
  \end{aligned}
\]
\qed
\begin{center}
\textbf{Propiedades que no valen en general.}
\end{center}
Veamos ahora algunas propiedades que no valen en general cuando multiplicamos matrices. Es decir, daremos contraejemplos.\\\\
\textbf{El producto no es conmutativo.}
\[
  \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \cdot \begin{bmatrix}5 & 6 \\ 7 & 8 \end{bmatrix} \neq \begin{bmatrix}5 & 6 \\ 7 & 8 \end{bmatrix} \cdot \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}
\]
\textbf{Multiplicar matrices no nulas puede dar cero.}
\[
  \begin{bmatrix}1 & 1 \\ 1 & 1 \end{bmatrix} \cdot \begin{bmatrix}-1 & -1 \\ 1 & 1 \end{bmatrix} = \begin{bmatrix}0 & 0 \\ 0 & 0 \end{bmatrix}
\]
\begin{obs}
Cuando las matrices son cuadradas podemos multiplicarlas por si mismas y definimos, de forma analoga a lo que ocurre en los productos de numeros, la potencia de una matriz: sea $A$ matriz $m \times m$, y sea $k \in \mathbb{N}$ entonces \[
  A^0=Id_m, \quad \quad A^k = A^{k-1} A,
\]
es decir $A^k$ es multiplicar $A$ consigo mismo $k-veces$.\\\\
\textbf{Elevar al cuadrado u otra potencia puede dar cero.}\[
  \begin{bmatrix} 0 & 1 \\ 0 & 0  \end{bmatrix}^2 =  \begin{bmatrix} 0 & 1 \\ 0 & 0  \end{bmatrix} \cdot  \begin{bmatrix} 0 & 1 \\ 0 & 0  \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}
\]
\end{obs}
\textbf{No se cumple la propiedad cancelativa.}\\\\
Si $A \neq 0$ y $AB=AC$ no necesariamente se cumple que $B=C$. Por ejemplo, 
\[
  \begin{bmatrix} 2 & 0 \\ 4 & 0 \end{bmatrix} = \begin{bmatrix}1 & 0 \\2 & 0 \end{bmatrix} \cdot \begin{bmatrix}2 & 0 \\ 8 & 1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 2 & 0 \end{bmatrix} \cdot \begin{bmatrix}2 & 0 \\ 5 & 3 \end{bmatrix}
\]
\textbf{No se cumple la formula del binomio.}\\\\
Sean $A,B$ matrices $ m \times m$, entonces \[
  \begin{aligned} 
    (A+B)^2 & = (A+B)(A+B) \\
            &= A(A+B) + B(A+B) \\
            &= AA + AB + BA + BB \\
            &= A^2 + AB + BA + B^2 
  \end{aligned}
\]
y esta ultima expresion puede no ser igual a $A^2+2AB+B^2$ ya que el producto de matrices no es conmutativo (en general).
\begin{center}
\textbf{Multiplicacion por la matriz 0.}
\end{center}
La multiplicacion por la matriz nula resulta en otra matriz nula. \\\\
Es decir, $0 \cdot A=A\cdot 0=0$ para toda matriz $A$:\[
  \begin{alignedat}{3}
    \begin{bmatrix} 0 \cdot A \end{bmatrix}_{ij} & = \sum_{k}\begin{bmatrix}0\end{bmatrix}_{ik} \cdot \begin{bmatrix} A \end{bmatrix}_{kj} & = \sum_{k}0 \cdot \begin{bmatrix}A\end{bmatrix}_{kj}& = 0 \\
    \begin{bmatrix} A \cdot 0 \end{bmatrix}_{ij} & = \sum_{k}\begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix}0 \end{bmatrix}_{kj} &= \sum_{k} \begin{bmatrix}A \end{bmatrix}_{ik}\cdot 0 & = 0
  \end{alignedat}
\]
\begin{center}
\textbf{Multiplicacion por matrices diagonales.}
\end{center}
Sea $D=\begin{bmatrix}d_1 & 0 & \cdots & 0  \\ 0 & d_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_m \end{bmatrix}$ y $A=\begin{bmatrix}a_{ij}\end{bmatrix}\in \mathbb{K}^{m \times n}$ \[
\begin{bmatrix} DA \end{bmatrix}_{ij} = \sum_{k=1}^{m} \begin{bmatrix}D \end{bmatrix}_{ik} \cdot \begin{bmatrix}A \end{bmatrix}_{kj}=\begin{bmatrix} D \end{bmatrix}_{ii} \begin{bmatrix} A \end{bmatrix}_{ij} = d_i a_{ij}
\]
y por lo tanto $DA=\begin{bmatrix}d_ia_{ij} \end{bmatrix}_{ij} \in \mathbb{K}^{m \times n}, \Rightarrow$  \[
DA=\begin{bmatrix} d_1F_1 \\ d_2 F_2 \\ \vdots \\d_mF_m \end{bmatrix}
\]\pagebreak 

La anterior es la multiplicacion a izquierda por una matriz diagonal.\\\\
La multiplicacion a derecha viene dada por: si $A=\begin{bmatrix}a_{ij} \end{bmatrix} \in \mathbb{K}^{n \times m}$, entonces \[
\begin{bmatrix} AD \end{bmatrix}_{ij} = \sum_{k=1}^{m} \begin{bmatrix}A \end{bmatrix}_{ik} \cdot \begin{bmatrix}D \end{bmatrix}_{kj} = \begin{bmatrix} A \end{bmatrix}_{ij} \cdot \begin{bmatrix} D \end{bmatrix}_{jj}=d_j a_{ij}
\]y por lo tanto $AD = \begin{bmatrix}d_ja_{ij} \end{bmatrix} \in \mathbb{K}^{n \times m}, \Rightarrow$ \[
AD=\begin{bmatrix}d_1C_1 & d_2 C_2 & \cdots & d_mC_m\end{bmatrix}
\]
\begin{center}
\textbf{Multiplicacion de una matriz por un escalar.}
\end{center}
Sea $A \in \mathbb{K}^{m \times n}$ y $c \in \mathbb{K}$. La matriz $cA$ es la matriz que se obtiene multiplicando toda las entradas de $A$ por $c$. En simbolos, \[
  cA \in \mathbb{K}^{m \times n} \quad \text{ con } \quad \begin{bmatrix}cA\end{bmatrix}_{ij}=c\begin{bmatrix}A\end{bmatrix}_{ij}
\]
\begin{ej}
Si $A = \begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$ y $c=10$ entoncs \[
10A=\begin{bmatrix}10 & 20 & 30 \\ 40 & 50 & 60 \end{bmatrix}\]
En este caso: $\begin{bmatrix}cA\end{bmatrix}_{12} = 10 \begin{bmatrix}A \end{bmatrix}_{12}=20$.
\end{ej}
\begin{obs}
  Observar que multiplicar por $c$ una matriz $m \times n$, es lo mismo que multiplicar por la matriz escalar $m \times m$ con los coeficientes de la diagonal iguales a $c$.
\end{obs}
\begin{obs}
  Debido a la observacion anterior y a las propiedades del producto d matrices, se cumple: si $A,B \in \mathbb{K}^{m \times n}$, $C \in \mathbb{K}^{n \times p}$ y $c,d \in \mathbb{K}$, entonces \begin{enumerate}[label=\textcolor{azulp2}{P\arabic*.}]
    \item $1\cdot A=A$. 
    \item $(cd)A=c(dA)$, 
 \end{enumerate}
\begin{enumerate}[label=\textcolor{azulp2}{D\arabic*.}]
  \item $c(A+B)=cA+cB$,\quad (propiedad distributiva)
\item $(c+d)A\phantom{,}=cA+dA$.\quad (propiedad distributiva) \end{enumerate}
\end{obs}\pagebreak 

\begin{list}{$\circ$}{}  
\item El hecho de que $\mathbb{K}^{m \times n}$ satisfaga las propiedades $\textcolor{azulp2}{S1}$, $\textcolor{azulp2}{S2}$, $\textcolor{azulp2}{S3}$, $\textcolor{azulp2}{S4}$, $\textcolor{azulp2}{P1}$, $\textcolor{azulp2}{P2}$, $\textcolor{azulp2}{D1}$ y $\textcolor{azulp2}{D2}$, nos dice que $\mathbb{K}^{m \times n}$ es un espacio vectorial. Observar que no difiere mucho de $\mathbb{K}^{n}$ (que cumplia las mismas propiedades).
\item Observar que $K^{n \times n}$ (las matrices cuadradas $n \times n$) satisface las siguientes propiedades: si $A,B,C \in \mathbb{K}^{n \times n}$ y $c \in \mathbb{K}$, entonces \\\\
  $\begin{array}{lll} 
    \textcolor{azulp2}{P3.} & Id\cdot A=A=A\cdot Id, & \text{(elemento neutro)}\\
    \textcolor{azulp2}{P4.} & c(AB)=(cA)B=A(cB), & \text{(conmutatividad $\times$ escalares)} \\
    \textcolor{azulp2}{P5.} & A(BC)=(AB)C, & \text{(asociatividad)} \\
    \textcolor{azulp2}{D3.} & C(A+B)=CA+CB, & \text{(distributividad)}\\
    \textcolor{azulp2}{D4.} & (A+B)C=AB+AC. & \text{(distributividad)}
  \end{array}$\\\\
  Estas propiedades nos dicen que $\mathbb{K}^{n \times n}$ es una $\mathbb{K}$-algebra asociativa con unidad.\\\\
  \textbf{[¡Solo para matrices cuadradas!]}
\end{list}
\begin{center}
\textbf{Sistemas de ecuaciones y multiplicacion de matrices.}
\end{center}
Sistema de $m$ ecuaciones lineales con $n$ incognitas: \[
  \begin{matrix}
  a_{11}x_{1} & + & a_{12}x_{2} & + & \cdots & + & a_{1n}x_n & = & y_1 \\ 
  \vdots & & \vdots  & & & & \vdots & & \\
  a_{m1}x_{1} & + & a_{m2}x_2 & + & \cdots & + & a_{mn}x_{n} & = & y_{m}
\end{matrix}\quad \quad (\star)
\]
donde $y_1, \dots,y_m$ y $a_{ij}$ $(1 \leq i \leq m, 1 \leq j \leq n)$ son numeros en $\mathbb{K}$.\\\\
Si denotamos \[
  A=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ 
    \vdots & \vdots && \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn} \\
    \end{bmatrix}, 
    \quad \quad 
    X=\begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}, \quad \quad Y=\begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}
,\]
y hacemos el producto de matrices \[
  A \cdot X = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ 
    \vdots & \vdots & & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix} \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}
    \] obtenemos \[
   \begin{bmatrix} a_{11}x_1 & a_{12}x_2 & \cdots & a_{1n}x_n \\ 
    \vdots & \vdots & & \vdots \\
    a_{m1}x_1 & a_{m2}x_2 & \cdots & a_{mn}x_n \end{bmatrix} = \begin{bmatrix} y_1 \\ \vdots \\ y_m \end{bmatrix}.
  \]
  Como dos matrices son iguales si y solo si sus coeficientes son iguales, la igualdad anterior signific que recuperamos el sistema de ecuaciones original.\\\\
  Esto nos dice que la notacion matricial antes utilizada para expresar un sistema de ecuaciones \[
AX=Y
  \]
  es consistente con el, ahora definido, producto de matrices \[
A\cdot X = Y.
  \]
  \textbf{Pregunta} \begin{list}{$\circ$}{}  
\item  ¿Cual es la relacion entre las operaciones elementales que nos llevan $A$ a una $MERF$ y las matrices?
\end{list}
\textbf{Respuesta} \begin{list}{$\circ$}{}  
\item Veremos que hacer una operacion elemental en la matriz $A$ es lo mismo que multiplicar $A$ por una matriz elemental (a definir mas abajo), y que 
\item reducir por filas a $A$ se obtiene multiplicando repetidas veces a $A$ por matrices elementales.
\end{list}
\begin{obs} \;
\begin{list}{$\circ$}{}  
\item En el lenguaje matricial, al vector $v=(t_1,t_2, \dots,t_n) \in \mathbb{R}^n$, lo representamos como el vector columna \[
    \begin{bmatrix} t_1 \\ t_2 \\ \vdots \\ t_n \end{bmatrix}.
  \]
\item Para saber si $v$ es solucion del sistema $AX=Y$ debemos verificar que vale la igualdad $Av=Y$. 
\item \quad \quad  $v \in \mathbb{R}^n$ es solucion de $AX=Y \Longleftrightarrow Av =Y$.
\item \quad \quad Sol$(\star)=\left\{v \in \mathbb{R}^n | Av=Y\right\}.$
\end{list}
\end{obs}
Esta observacion es util como herramienta para demostrar ciertas propiedades de los sistemas. 
\begin{prop}
  Sean $v$ y $w$ soluciones del sistema homogeneo $AX=0$. Entonces $v+tw$ tambien es solucion del sistema $AX=0$ para todo $t \in \mathbb{R}$.
\end{prop}
\begin{demo}% \[\begin{aligned}
% x &= 1 \tag{tag}\\
% \end{aligned}\]
\begin{align*}
  A(v+tw)&=Av+A(tw) &&\text{(distributividad)}\\
         &=0+(At)w &&\text{($AV=0$ $+$ asociatividad)} \\
         &= (tA)w && \text{(conmutatividad $\times$ escalares)} \\
         &= t(Aw) &&\text{(conmutatividad $\times$ escalares)} \\
         &= t0 &&\text{($Aw=0$)} \\
         &= 0.
  \end{align*} \qed
\end{demo}\pagebreak 

\begin{center}
\textbf{Matrices elementales.}
\end{center}
\begin{defi}
  Una matriz $m \times m$ se dice elemental si fue obtenida por medio de una unica operacion elemental a partir de la matriz identidada $Id_m$.
\end{defi}\begin{ej}
Las siguientes matrices son elementales:\[\begin{aligned}
  &\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \xrightarrow{F_2+2F_1} \begin{bmatrix}1 & 0 & 0 \\2 & 1 & 0 \\ 0& 0 & 1 \end{bmatrix}, \quad  \begin{bmatrix}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\0 & 0 & 0 & 1 \end{bmatrix} \xrightarrow{F_1 \leftrightarrow F_4} \begin{bmatrix}0 & 0 & 0 & 1 \\ 0 & 1 & 0 & 0 \\ 0& 0 & 1 & 0 \\ 1 & 0 & 0 & 0 \end{bmatrix},\\
  &\begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} \xrightarrow{3F_2} \begin{bmatrix}1 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
\end{aligned}
 \]\end{ej}
\begin{center}
\textbf{Matrices elementales $2 \times 2$.}
\end{center}
\begin{enumerate}[label=\textcolor{azulp2}{E\arabic*.}]
  \item Si $ c \neq 0$, multiplicar por $c$ la primera fila y multiplicar $c$ por la segunda fila son, respectivamente, \[
      \begin{bmatrix} c & 0 \\ 0 & 1 \end{bmatrix} \text{ y } \begin{bmatrix}1 & 0 \\ 0 & c \end{bmatrix},
    \]
  \item si $c \in \mathbb{K}$, sumar a la fila $2$ la fila $1$ multiplicada por $c$ o sumar a la fila $1$ la fila $2$ multiplicada por $c$ son, respectivamente, \[
      \begin{bmatrix} 1 & 0 \\ c & 1 \end{bmatrix} \text{ y } \begin{bmatrix}1 & c \\ 0 & 1 \end{bmatrix}.
    \]
  \item Finalmente, intercambiando la fila $1$ por la fila $2$ obtenemos la matriz \[
      \begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}. 
    \]
 \end{enumerate}
 \pagebreak

\begin{center}
\textbf{Matrices elementales $m \times m$.}
\end{center}
\begin{enumerate}[label=\textcolor{azulp2}{E\arabic*.}]
\item Si $ c\neq 0$, multiplicar por $c$ la fila $k$ de la matriz identidad, resulta en la matriz elemental que tiene todos $1$'s en la diagonal, excepto en la posicion $k$, donde vale $c$, es decir si $e(Id_m)=\begin{bmatrix}a_{ij}\end{bmatrix}$, entonces \[
  a_{ij}=\begin{dcases}1 & \text{si }i=j \text{ e } i \neq k, \\
    c & \text{si }i = j = k, \\
  0 & \text{si }i \neq j. \end{dcases}
  \]
  Graficamente, \begin{equation}\tag{E1}
    \bbordermatrix{
      ~ &  &  & \overset{k}{\downarrow} \cr
        & 1 & \cdots &  & \cdots & 0 \cr
        & \vdots & \ddots &  &  & \vdots \cr
       \xrightarrow{k} & 0 & \cdots & c & \cdots & 0 \cr
        & \vdots & & & \ddots & \vdots \cr 
        & 0 & \cdots & & \cdots & 1 \cr
    } \end{equation}
  \item si $c \in \mathbb{K}$, sumar a la fila $r$ la fila $s$ multiplicada por $c$. \[
      \begin{dcases}
        1 & \text{si } i =j, \\
        c & \text{si }i = r, j =s, \\
      0 & \text{otro caso.}\end{dcases}
    \]
    Graficamente \begin{equation}\tag{E2} 
      \bbordermatrix{ 
        ~& && \overset{r}{\downarrow} & & \overset{s}{\downarrow} \cr 
         & 1 & \cdots & & \cdots & & \cdots & 0 \cr
         & \vdots & \ddots & & & && \vdots \cr 
        \xrightarrow{r} & 0 & \cdots & 1 & \cdots & c & \cdots & 0 \cr 
                        & \vdots & & & \ddots & & & \vdots \cr
                      \xrightarrow{s}  & 0 & \cdots & 0 & \cdots & 1 & \cdots & 0 \cr
        & \vdots & & & && \ddots  & \vdots\cr
                        & 0 & \cdots &  & \cdots &  & \cdots &   1 \cr
      }
    \end{equation}\pagebreak

  \item Finalmente, intercambiar la fila $r$ por la fila $s$ resulta ser \[
      a_{ij}=\begin{dcases} 1 & \text{si } (i=j,i\neq r,i\neq s)\text{ o } (i=r,j=s) \text{ o } (i=s,j=r) \\
      0 & \text{ otro caso. } \end{dcases}
    \]
    Graficamente, \begin{equation}\tag{E3} 
      \bbordermatrix{
        ~& && \overset{r}{\downarrow} & & \overset{s}{\downarrow} \cr 
         & 1 & \cdots & & \cdots & & \cdots & 0 \cr
         & \vdots & \ddots & & & && \vdots \cr 
        \xrightarrow{r} & 0 & \cdots & 0 & \cdots & 1 & \cdots & 0 \cr 
                        & \vdots & & & \ddots & & & \vdots \cr
                      \xrightarrow{s}  & 0 & \cdots & 1 & \cdots & 0 & \cdots & 0 \cr
        & \vdots & & & && \ddots  & \vdots\cr
                        & 0 & \cdots &  & \cdots &  & \cdots &   1 \cr
                    }\end{equation}
 \end{enumerate}\begin{teo}
   Sea $A$ matriz $n \times n$, sea $e$ una operacion elemental por fila y sea $E$ la matriz elemental $E=e(Id_n)$. Entonces $e(A)=e(Id_n)A=EA$.
 \end{teo}
Hagamos la demostracion de la prueba para matrices $2 \times 2$.
\\\\La prueba en general es similar.\\ \begin{demo}\;

  \begin{enumerate}[label=\textcolor{azulp2}{E\arabic*.}]
  \item  Si $c\neq 0$, y sea $e$ la operacion elemental de multiplicar por $c$ la primera fila. Entonces, $E=e(Id_2)$ resulta en la matriz elemental \[
    E=\begin{bmatrix}c & 0 \\ 0 & 1 \end{bmatrix}.
  \]Ahora bien,\[
  \begin{aligned}
    EA&=\begin{bmatrix}c & 0 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \\ 
      &= \begin{bmatrix} c \cdot a_{11} + 0 \cdot a_{21} & c \cdot a_{12} + 0 \cdot a_{22} \\ 0 \cdot a_{11}+ 1 \cdot a_{21} & 0 \cdot a_{12} + 1 \cdot a_{22} \end{bmatrix}
      = \begin{bmatrix} c\cdot a_{11} & c \cdot a_{12} \\ a_{21} & a_{22} \end{bmatrix} = e(A).
  \end{aligned}
  \]
  De forma analoga se demuestra en el caso que la operacion elemental sea multiplicar la segunda fila por $c$. \pagebreak

\item Sea $c \in \mathbb{K}$, y sea $e$ la operacion elemental de a la fila $2$ sumarle la fila $1$ multiplicada por $c$. Entonces $E=e(Id_2)$ resulta en la matriz elemental: \[
    E=\begin{bmatrix} 1 & 0 \\ c & 1 \end{bmatrix}.
    \]Ahora bien, \[
    EA=\begin{bmatrix} 1 & 0 \\ c & 1 \end{bmatrix} \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} \\ c \cdot a_{11}+a_{21} & c \cdot a_{12} + a_{22} \end{bmatrix}=e(A).
  \]
  La demostracion es analoga si la operacion elemental es sumar a la fila $1$ la fila $2$ multiplicada por $c$. 
\item Finalmente, sea $e$ intercambiar la fila $1$ por la fila $2$. Entonces, $E=e(Id_2)$ es la matriz elemental \[
    E=\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}.
    \]Ahora bien, \[
    EA=\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} = \begin{bmatrix} a_{21} & a_{22} \\ a_{11} & a_{12} \end{bmatrix} = e(A).
  \]\qed
 \end{enumerate}
 \end{demo}
 \begin{corol} Sean $A$ y $B$ matrices $m \times n$. Entonces
\begin{list}{$\circ$}{}  
\item $B$ equivalente por filas a $A$ si y solo si $B=PA$ donde $P$ es producto de matrices elementales. 
\end{list}
Mas aun, 
\begin{list}{$\circ$}{}  
\item si $e_1,e_2, \dots,e_k$ son operaciones elementales de fila y $E_i=e_i(Id)$, entonces \[
    B=e_k(e_{k-1}(\cdots(e_1(A))\cdots)) \Rightarrow B=E_kE_{k-1}\cdots E_1 A.
  \]
\end{list}
 \end{corol}
\pagebreak 

 \begin{demo}[$\Rightarrow$]
   Si $B$ equivalente por filas a $A$ existen operaciones elementales $e_1,\dots,e_k$ tal que \[
     B=e_{k}(e_{k-1}(\cdots (e_1(A))\cdots)).
   \]
   Mas formalmente, si 
\begin{list}{$\circ$}{}  
\item $A_1=e_1(A),$ 
\item $A_i=e_i(A_{i-1})$ para $i=2, \dots,k$, y 
\item $e_k(A_{k-1})=B$.
\end{list}
Entonces, si $E_i=e_i(Id_m)$, por el teorema anterior\begin{list}{$\circ$}{}  
\item  $A_1=E_1A$. 
\item $A_i=E_iA_{i-1}$ para $i=2,\dots,k$, y 
\item $E_kA_{k-1}=B$.
\end{list}
En otras palabras \[
  B=PA \quad \text{ con } \quad P=E_{k}E_{k-1}\cdots E_{1}.
\]
 \end{demo}
\begin{demo}[$\Leftarrow$]
  Si $B=PA$, con $P=E_{k}E_{k-1}\cdots E_{1}$ donde $E_{i}=e_{i}(Id_m)$ es una matriz elemental, entonces (razonamiento similar al anterior) \[
    B=PA=E_{k}E_{k-1}\cdots E_{1}A.
  \] 
  Mas formalmente, si 
\begin{list}{$\circ$}{}  
\item $A_1=E_1A$, 
\item $A_{i}=E_iA_{i-1}$ para $i=2,\dots,k$, y 
\item $E_kA_{k-1}=B$.
\end{list}
Luego, por el teorema anterior, 
\begin{list}{$\circ$}{}  
\item $A_1=e_1(A)$, 
\item $A_i = e_i(A_{i-1})$ para $i=2,\dots,k$, y
\item $e_{k}(A_{k-1})=B$.
\end{list}
Por lo tanto, $B$ es equivalente por filas a $A$.
\end{demo}
\qed
\pagebreak

\begin{center}
\textbf{Matrices invertibles.}
\end{center}
\begin{defi}
  Sea $A$ una matriz $n \times n$ con coeficientes en $\mathbb{K}$. \\\\
   Una matriz $B \in M_{n \times n}(\mathbb{K})$ es inversa de $A$ Si $BA=AB=Id_n$. \\\\
   En ese caso, diremos que $A$ es invertible.
\end{defi}
\begin{ej}
  La matriz $\begin{bmatrix}2 & -1 \\ 0 & 1 \end{bmatrix}$ tiene inversa $\begin{bmatrix} \tfrac{1}{2} & \tfrac{1}{2} \\ 0 & 1 \end{bmatrix}$ pues es facil comprobar que \[
  \begin{bmatrix}2 & -1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} \tfrac{1}{2} & \tfrac{1}{2} \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \quad \text{ y } \quad \begin{bmatrix} \tfrac{1}{2} & \tfrac{1}{2} \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 2 & -1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}.
\]
\end{ej}
\begin{center}
\textbf{Preguntas (y respuestas).}
\end{center}
\begin{list}{$\circ$}{}  
\item ¿Toda matriz no nula tiene inversa? \\ Respuesta: \textbf{no}, por ejemplo \[
    \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}
  \]
\item Si la matriz tiene una inversa ¿es unica?\\ Respuesta: \textbf{si}. Se vera a continuacion.
\item Si $BA=Id_n$ ¿es cierto que $AB=Id_n$? Es decir, ¿si $A$ tiene inversa a izquierda, entonces tiene inversa a derecha y es la misma matriz? \\ Respuesta: \textbf{si}. Se vera mas adelante.
\end{list}
\begin{prop}
  Sea $A \in M_{n \times n} (\mathbb{K})$, \begin{enumerate}[label=\textcolor{azulp2}{(\arabic*)}]
    \item sean $B,C \in M_{n \times n}(\mathbb{K})$ tales que $BA=Id_n$ y $AC=Id_n$, entonces $B=C$;
    \item si $A$ invertible la inversa es unica.
 \end{enumerate}
\end{prop}
\begin{demo}\;
  \begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
    \item \[
        B=BId_n=B(AC)=(BA))C=Id_nC=C.
      \]
    \item Sean $B$ y $C$ inversas de $A$, es decir $BA=AB=Id_n$ y $CA=AC=Id_n$. En particular, $BA=Id_n$ y $AC=Id_n$, luego, por \textcolor{azulp2}{1.}, $B=C$. 
 \end{enumerate}\qed
\end{demo}
\pagebreak

\begin{defi}
  Sea $A \in M_{n \times n}(\mathbb{K})$ invertible. A la unica matriz inversa de $A$ la llamamos la matriz inversa de $A$ y la denotamos $A^{-1}$.
\end{defi}
\begin{ej}
  Sea $A$ la matriz \[    \begin{bmatrix}2 & 1 & -2 \\ 1 & 1 & -2 \\ -1 & 0 & 1 \end{bmatrix} .
  \] Entonces, $A$ es invertible y su inversa es \[
    A^{-1}=\begin{bmatrix}1 & -1 & 0 \\ 1 & 0 & 2 \\ 1 & -1 & 1 \end{bmatrix}.
  \]Esto se resuelve comprobando que $AA^{-1}=Id_3$ y $A^{-1}A=Id_3$.
\end{ej}
\begin{teo}
  Sean $A$ y $B$ matrices $n \times n$ con coeficientes en $\mathbb{K}$. Entonces \begin{list}{$\circ$}{}  
  \item si $A$ invertible, entonces $A^{-1}$ es invertible y su inversa es $A$, es decir $(A^{-1})^{-1}=A$;
\end{list}
\end{teo}
\begin{demo}
  \[\begin{aligned}
    AA^{-1}&=Id_n \quad \Rightarrow \quad \text{la inversa a izquierda de }A^{-1} \text{ es }A,\\
    A^{-1}A&=Id_n \quad \Rightarrow \quad \text{la inversa a derecha de }A^{-1} \text{ es } A,
  \end{aligned}
  \]
Concluyendo: $A$ es la inversa de $A^{-1}$.\\
\qed
\end{demo}
\begin{teo}
  Sean $A$ y $B$ matrices $n \times n$ con coeficientes en $\mathbb{K}$. Entonces \begin{list}{$\circ$}{}  
  \item si $A$ y $B$ son invertibles, entonces $AB$ es invertible y\\ $(AB)^{-1}=B^{-1}A^{-1}$.
\end{list}
\end{teo}
\begin{demo}
  Simplemente debemos comprobar que $B^{-1}A^{-1}$ es inversa a izquieda y derecha de $AB$: \[
    (B^{-1}A^{-1})AB=B^{-1}(A^{-1}A)B=B^{-1}Id_nB=B^{-1}B=Id_n.
  \]
  Analogamente, \[
    AB(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AId_nA^{-1}=AA^{-1}=Id_n.
  \]
  \qed
\end{demo}
\begin{obs}
  Si $A_{1}, \dots, A_{k}$ son invertibles, entonces $A_{1} \dots A_{k}$ es invertible y su inversa es \[
    (A_1 \dots A_k)^{-1}=A_{k}^{-1}\dots A_{1}^{-1}.
  \]
  El resultado es una generalizacion del teorema anterior y su demostracion se hace por induccion en $k$ (usando el teorema anterior). Se deja como ejercicio al lector.
\end{obs}
\begin{obs}
La suma de matrices invertibles no necesariamente es invertible, por ejemplo $A+(-A)=0$ que no es invertible.
\end{obs}
\begin{teo}
  Una matriz elemental es invertible.
\end{teo}
\begin{demo}\;\\\\
  Sea $E$ la matriz elemental que se obtiene a patir de $Id_n$ por la operacion elemental $e$. Sea $e'$ la operacion elemental inversa y $E'=e'(Id_n)$. Entonces \[\begin{aligned}
    EE'&=e(e'(Id_n))=Id_n, \\
    E'E&=e'(e(Id_n))=Id_n.
\end{aligned}
  \]
  Luego $E'=E^{-1}$. \\
  \qed
\end{demo}
Es facil encontrar explicitamente la matriz inversa de una matriz elemental, por ejemplo, en el caso $2 \times 2$ tenemos: \begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
  \item Si $ c \neq 0$, \[
      \begin{bmatrix}c & 0 \\ 0 & 1 \end{bmatrix}^{-1}=\begin{bmatrix}\sfrac{1}{c} & 0 \\ 0 & 1 \end{bmatrix} \text{ y } \begin{bmatrix}1 & 0 \\ 0 & c \end{bmatrix}^{-1}=\begin{bmatrix}1 & 0 \\ 0 & \sfrac{1}{c}\end{bmatrix},
    \] 
  \item si $c \in \mathbb{K}$, \[
      \begin{bmatrix} 1 & 0 \\ c & 1 \end{bmatrix}^{-1}=\begin{bmatrix}1 & 0 \\ -c & 1 \end{bmatrix} \text{ y } \begin{bmatrix}1 & c \\ 0 & 1 \end{bmatrix}^{-1}=\begin{bmatrix}1 & -c \\ 0 & 1 \end{bmatrix}.
    \]
  \item  Finalmente, \[
      \begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}^{-1}=\begin{bmatrix}0 & 1 \\ 1 & 0 \end{bmatrix}.
    \]
 \end{enumerate}
\begin{lema}
  Sea $R \in \mathbb{K}^{n \times n}$. Entonces \[
    R \text{ es } MERF \land R \text{ es invertible} \; \Rightarrow \; R = Id_n.
  \]
\end{lema}
\begin{demo}
  Supongamos que la $r_{11}=0$. Como $R$ es $MERF$ $\Rightarrow C_1 = 0$. \\ Sea $t \neq 0$ en $\mathbb{K}$. Como $C_1=0$,\[
    R\begin{bmatrix}t \\ 0 \\ \vdots \\ 0 \end{bmatrix} = \begin{bmatrix} 0 & * & \cdots & * \\ 
    0 & * & \cdots & * \\ 
    \vdots \\
    0 & * & \cdots & * \end{bmatrix} \begin{bmatrix} t \\ 0 \\ \vdots \\ 0 \end{bmatrix} = \begin{bmatrix}0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}.
  \]
  Luego, como $R$ tiene inversa: 
  \[
    \begin{bmatrix} t \\ 0 \\ \vdots \\ 0 \end{bmatrix} = Id\begin{bmatrix} t \\ 0 \\ \vdots \\ 0 \end{bmatrix} =R^{-1}R\begin{bmatrix}t \\ 0 \\ \vdots \\ 0 \end{bmatrix} = R^{-1} \begin{bmatrix}0 \\ 0 \\ \vdots \\0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}. 
  \]
  Esto nos dice que $t=0$, lo cual es absurdo pues habiamos partido de $t \neq 0$. Por lo tanto \[
R= \begin{bmatrix} 1 & * & \cdots & * \\ 
    0 & * & \cdots & * \\ 
    \vdots \\
    0 & * & \cdots & * \end{bmatrix}.  \]
    Por induccion podemos probar que $R=Id_n$. \\ \qed
\end{demo}
\pagebreak

\begin{teo}
  Sea $A$ matriz $n \times n$ con coeficiences en $\mathbb{K}$. Las siguientes afirmaciones son equivalentes \begin{enumerate}[label=\textcolor{azulp2}{\roman*}]
    \item $A$ es invertible, 
    \item $A$ es quivalente por filas a $Id_n$, 
    \item $A$ es producto de matrices elementales.
 \end{enumerate}
\end{teo}
\begin{demo}
  \; \begin{enumerate}[label=\textcolor{azulp2}{\roman*)}] 
    \item $\Rightarrow$ \textcolor{azulp2}{\text{ii)}} Sea $R$ una $MERF$ que se obtiene de $A$.
\begin{list}{$\circ$}{}  
\item  Existen $E_1,\dots,E_k$ matrices elementales tal que $E_1\cdots E_kA=R$.
\item Como $E_1,\dots,E_k$, $A$ invertibles $\Rightarrow$ $E_1\cdots E_kA$ invertible. 
\item $R$ es $MERF$ e invertible $\Rightarrow R= Id_n$.
\item $A$ es equivalente por filas a $R=Id_n$.
\end{list}
\item $\Rightarrow$ \textcolor{azulp2}{iii)} \begin{list}{$\circ$}{}  
\item  $A$ es equivalente por filas a $Id_n \Rightarrow $ existen $E_1,\cdots,E_k$ matrices elementales tal que $E_1 \cdots E_kA=Id_n.$ 
\item Sean $F_1,\dots,F_k$ las inversas de $E_1,\dots ,E_k$, respectivamente $\Rightarrow$ \[
F_k \cdots F_1 E_1 \cdots E_k A = A.
  \]
\item Como $E_1 \cdots E_kA=Id_n \Rightarrow F_k \cdots F_1 Id_n = A$.
\item Como multiplicar por $Id_n$ deja igual la matriz, \[
F_k \cdots F_1 = A.
  \]
\end{list}
\item $\Rightarrow$ \textcolor{azulp2}{i)} Sea $A=E_1E_2\cdots E_k$ donde $E_i$ es una matriz elemental $(i = 1, \dots , k)$. Como cada $E_i$ es invertible, el producto de ellos es invertible, por lo tanto $A$ es invertible. \\ \qed
  \end{enumerate}
\end{demo}
\pagebreak

\begin{corol}
  Sean $A$ y $B$ matrices $m \times n$. Entonces, $B$ es equivalente por filas a $A$ si y solo si existe matriz invertible $P$ de orden $ m \times m$ tal que $B=PA$.
\end{corol}
\begin{demo}
  \;\\($\Rightarrow$) \begin{list}{$\circ$}{}  
\item  $B$ es equivalente por filas a $A$ $\Rightarrow$ $\exists E_1, \dots, E_k$ matrices elementales tal que $B=E_1 \cdots E_k A$. 
\item Sea $P=E_1 \cdots E_k$, luego $B=PA$. 
\item Cada $E_i$ es invertible $\Rightarrow P=E_1 \cdots E_k$ es invertible. 
\end{list}
($\Leftarrow$)\begin{list}{$\circ$}{}  
\item Sea $P$ matriz invertible tal que $B=PA$. 
\item Como $P$ es invertible $\Rightarrow P=E_1 \cdots E_k$, producto de matrices elementales. 
\item Por lo tanto, $B=PA=E_1\cdots E_kA$ es equivalente por filas a $A$. 
\end{list}\qed
\end{demo}
\begin{corol}
  Sea $A$ matriz $n \times n$. Sean $e_1, \dots, e_k$ operaciones elementales por filas tal que \begin{equation}\tag{*}
    e_1(e_2(\cdots(e_k(A))\cdots))=Id_n.
  \end{equation}
  Entonces, $A$ invertible y \begin{equation}\tag{**}
    e_1(e_2(\cdots(e_k(Id_n))\cdots))=A^{-1}.
  \end{equation}
\end{corol}\begin{demo}\;
  \begin{list}{$\circ$}{}  
\item  (*) $\Rightarrow A$ es equivalente por filas a $Id_n \Rightarrow A$ es invertible.
\item Sean las matrices elementales $E_i=e_i(Id_n)$ para $i=1,\dots,k$, entonces $E_1E_2 \cdots E_kA=Id_n$, por lo tanto, \[
    \begin{aligned}
&E_1E_2 \cdots E_k  AA^{-1} &&=Id_nA^{-1} \\
\ArrowBetweenLines
&E_1E_2 \cdots E_k  Id_n &&= A^{-1} \\
\ArrowBetweenLines 
&e_1(e_2(\cdots(e_k(Id_n))\cdots)) &&=A^{-1}.
    \end{aligned}
  \]\end{list}
  \qed
\end{demo}
\pagebreak

Este ultimo corolario nos provee un metodo sencillo para calcular la inversa de una matriz cuadrada $A$ invertible. \begin{enumerate}[label=\textcolor{azulp2}{\arabic*}]
    \item Aplicando operaciones elementales $e_1,\dots,e_k$ encontraremos $R = Id_n$ la $MERF$ de $A$. 
    \item Aplicando las operaciones elementales $e_1, \dots ,e_k$ a $Id_n$, obtenemos $A^{-1}$ la inversa de $A$.
 \end{enumerate}
 Para facilitar el calculo es conveniente comenzar con $A$ e $Id_n$ e ir aplicando paralelamente las operaciones elementales por fila.
 \begin{ej}
   Calculemos la inversa (si tiene) de \[
     A=\begin{bmatrix} 2 & -1 \\ 1 & 3 \end{bmatrix}.
   \]
 \end{ej}
 \textbf{Solucion.} Trataremos de reducir por filas a $A$ y todas las operaciones elementales las haremos en paralelo partiendo de la matriz identidad: \[\begin{aligned}
   \begin{bmatrix}[l|l]A & Id \end{bmatrix}= &\begin{bmatrix}[cc|cc] 2 & -1 & 1 & 0 \\ 1 & 3 & 0 & 1 \end{bmatrix} &&\xrightarrow{F_1 \leftrightarrow F_2} &&\begin{bmatrix}[cc|cc] 1 & 3 & 0 & 1 \\ 2 & -1 & 1 & 0 \end{bmatrix} \\
   \xrightarrow{F_2-2F_1} &\begin{bmatrix}[cc|cc]1 & 3 & 0 & 1 \\ 0 & -7 & 1 & -2 \end{bmatrix} &&\xrightarrow{F_2/(-7)} &&\begin{bmatrix}[cc|cc] 1 & 3 & 0 & 1 \\ 0 & 1 & -\tfrac{1}{7} & \tfrac{2}{7}] \end{bmatrix} \\
   \xrightarrow{F_1-3F_2} &\begin{bmatrix}[cc|cc]1 & 0 & \tfrac{3}{7} & \tfrac{1}{7} \\ 0 & 1 & -\tfrac{1}{7} & \tfrac{2}{7} \end{bmatrix}.
 \end{aligned}
 \]
 Luego, $A$ es invertible y su inversa es \[
   A^{-1}=\begin{bmatrix}\tfrac{3}{7} & \tfrac{1}{7} \\ -\tfrac{1}{7} & \tfrac{2}{7} \end{bmatrix}.
 \]
 \qed
\begin{list}{$\circ$}{}  
\item  El lector desconfiado podra comprobar, haciendo el producto de matrices, que $AA^{-1}=A^{-1}A=Id_2$.
\end{list}
\pagebreak

\begin{teo}
  Sea $A$ matriz $n \times n$ con coeficientes en $\mathbb{K}$. Entonces, las siguientes afirmaciones son equivalentes. \begin{enumerate}[label=\textcolor{azulp2}{\roman*)}]
    \item $A$ es invertible. 
    \item El sistema $AX=Y$ tiene una unica solucion para toda matriz $Y$ de orden $ n \times 1$. 
    \item El sistema homogeneo $AX=0$ tiene una unica solucion trivial.
 \end{enumerate}
\end{teo}
\textcolor{azulp2}{i)} $\Rightarrow$ \textcolor{azulp2}{ii)} Sea $X_0$ solucion del sistema $AX=Y$, luego \[
  AX_0=Y \quad \Rightarrow \quad A^{-1}AX_0=A^{-1}Y\quad \Rightarrow \quad X_0=A^{-1}Y.
\]
Es decir, $X_0$ es unico (siempre igual a $A^{-1}Y$).\\\\
\textcolor{azulp2}{ii)} $\Rightarrow$ \textcolor{azulp2}{iii)} Es trivial, tomando $Y=0$.\\\\
\textcolor{azulp2}{iii)} $\Rightarrow$ \textcolor{azulp2}{i)}
\begin{list}{$\circ$}{}  
\item  Sea $R$ la $MERF$ equivalente a $A$.
\item Si $R$ tiene una fila nula, entonces el sistema $AX=0$ tiene mas de una solucion, contradiciendo la hipotesis.
\item Por lo tanto, $R$ no tiene filas nulas.
\item Como $R$ es una matriz cuadrada y es $MERF$, tenemos que $R=Id_n$.
\end{list}
\begin{corol}
  Sea $A$ una matriz $n \times n$ con coeficientes en $\mathbb{K}$. \begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
    \item Si existis $B$ matriz $n \times n$ tal que $BA=Id_n$, entonces $AB=Id_n$. \\($A$ tiene inversa a izquierda $\Rightarrow$ es invertible). 
    \item Si existe $C$ matriz $n \times n$ tal que $AC=Id_n$, entonces $CA=Id_n$. \\($A$ tiene inversa a derecha $\Rightarrow$ es invertible).
 \end{enumerate}
\end{corol}
\begin{demo}\;
  \begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
    \item  Sea $B$ tal que $BA=Id_n$. El sistema $AX=0$ tiene una unica solucion, pues \[
AX_0=0 \quad \Rightarrow \quad BAX_0=B0=0 \quad \Rightarrow \quad Id_nX_0=0 \quad \Rightarrow \quad X_0 = 0.
      \]
      Luego, $A$ es invertible (y su inversa es $B$).
    \item Sea $C$ tal que $AC=Id_n$. Luego $A$ es la inversa a izquierda de $C$. Por lo que demostramos mas arriba, $C$ es invertible y su inversa es $A$, es decir $AC=Id_n$ y $CA=Id_n$, luego $C$ es invertible. \\
      \qed
 \end{enumerate}
\end{demo}
El siguiente teorema reune algunos resultados ya demostrados.
\begin{teo}
 Sea $A \in \mathbb{K}^{n \times n}$. Las siguientes afirmaciones son equivalentes \begin{enumerate}[label=\textcolor{azulp2}{\arabic*.}]
   \item $A$ es invertible, 
   \item $A$ es equivalente por fila a $Id_n$, 
   \item el sistema $AX=0$ tiene solucion unica (la trivial), 
   \item el sistema $AX=Y$ tiene slucion unica para todo $Y \in \mathbb{K}^{n}$ (la solucion es $A^{-1}Y$), 
   \item $A$ es el producto de matrices elementales,
   \item existe $B$ matriz $n \times n$ tal que $BA=Id$,
\item existe $C$ matriz $n \times n$ tal que $AC=Id$,
\item $det(A)\neq 0$ (esto lo veremos mas adelante).
 \end{enumerate}
\end{teo}
\begin{center}
\textbf{Matrices invertibles $2 \times 2$.}
\end{center}
Dados $a,b,c,d, \in \mathbb{R}$, determinaremos cuando la matriz $A=\begin{bmatrix}a & b \\ c & d \end{bmatrix}$ es invertible y en ese caso, cual es su inversa.
\\\\
\textbf{Solucion.}\\\\
Para poder aplicar el metodo de Gauss-Jordan debemos analizar dos casos: $a \neq 0$ y $a = 0$. \begin{enumerate}[label=\textbf{Caso \arabic*}]
    \item  $a=\neq 0$. \\
      Como $a \neq 0$, entonces \[
        \begin{bmatrix}a & b \\ c & d \end{bmatrix} \xrightarrow{F_1 / a} \begin{bmatrix} 1 & \frac{b}{a} \\ c & d \end{bmatrix} \xrightarrow{F_2-cF_1} \begin{bmatrix}1 & \frac{b}{a} \\ 0 & d- c \frac{b}{c} \end{bmatrix} = \begin{bmatrix} 1 & \frac{b}{a} \\ 0 & \frac{ad-bc}{a} \end{bmatrix}
      \]
 \end{enumerate}
 \begin{enumerate}[label=\textbf{Caso 1.\arabic*}]
   \item $a \neq 0$ y $ad-bc=0$. \\
     Si $ad-bc=0$, entonces la matriz se encuentra reducida por filas y la ultima fila es $0$, luego en ese caso no es invertible. 
   \item $a \neq 0$ y $ad-bc \neq 0$. \\
     Entonces \[
       \begin{bmatrix} 1 & \frac{b}{a} \\ 0 & \frac{ad-bc}{a} \end{bmatrix}  \xrightarrow{a/(ad-bc)F_2} \begin{bmatrix} 1&  \frac{b}{a} \\ 0 & 1 \end{bmatrix} \xrightarrow{F_1-b/aF_2} \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}.
     \]
     Luego, en este caso $a \neq 0$, $ad-bc \neq 0$ hemos reducido por filas la matriz $A$ a la identidad y por lo tanto $A$ es invertible.
   \end{enumerate}
   Ademas, podemos encontrar $A^{-1}$ aplicando a $Id_2$ las mismas operaciones elementales que reducian a $A$ a la identidad: \[
     \begin{aligned}
       \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow{F_1/a} &\begin{bmatrix} \frac{1}{a} & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow {F_2-cF_1} \begin{bmatrix} \frac{1}{a} & 0 \\ 
       -\frac{c}{a} & 1 \end{bmatrix} \xrightarrow{a/(a-bc)F_2}\begin{bmatrix}\frac{1}{a} & 0 \\ -\frac{c}{ad-bc} & \frac{a}{ad-bc}\end{bmatrix} \xrightarrow{F_1-b/aF_2} \\
       \xrightarrow{F_1-b/aF_2} & \begin{bmatrix}  \frac{1}{a} + \frac{bc}{a(ad-bc)} & -\frac{b}{ad-bc} \\ -\frac{c}{ad-bc} & \frac{a}{ad-bc} \end{bmatrix} = \begin{bmatrix}\frac{d}{ad-bc} & -\frac{b}{ad-bc} \\ - \frac{c}{ad-bc} & \frac{a}{ad-bc} \end{bmatrix}.
   \end{aligned}
   \]
   Concluyendo, en el caso $a \neq 0$, $ad-bc\neq0$, $A$ es invertible y \begin{equation}
     A^{-1} = \frac{1}{ad-bc}\begin{bmatrix}d & -b \\ -c & a \end{bmatrix}.
   \end{equation}
   \textbf{Caso 2.} $a = 0$.\\\\
   Si $c =0$ o $b=0$, entonces la matriz no es invertible, pues en ambos casos nos quedan matrices que no pueden ser reducidas por fila a la identidad. \\\\ Luego la matriz puede ser invertible si $bc \neq 0$ y en este caso la reduccion por filas es: \[
     \begin{bmatrix} 0 & b \\ c & d \end{bmatrix} \xrightarrow{F_1 \leftrightarrow F_2} \begin{bmatrix}c & d \\ 0 & b \end{bmatrix} \xrightarrow{F_1 / c } \begin{bmatrix} 1 & \frac{d}{c} \\ 0 & b \end{bmatrix} \xrightarrow{F_2 / b} \begin{bmatrix} 1 & \frac{d}{c} \\ 0 & 1 \end{bmatrix} \xrightarrow{F_1-d/cF_2} \begin{bmatrix}1 & 0 \\ 0 & 1 \end{bmatrix}.
   \]
   Luego, $A$ es invertible y aplicando estas mismas operaciones elementales a la identidad obtenemos la inversa: \[
     \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \xrightarrow{F_1 \leftrightarrow F_2} \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \xrightarrow{F_1/c} \begin{bmatrix}0 & \frac{1}{c} \\ 1 & 0 \end{bmatrix} \xrightarrow{F_2 / b} \begin{bmatrix} 0 & \frac{1}{c} \\ \frac{1}{b} & 0 \end{bmatrix} \xrightarrow{F_1-d / cF_2 } \begin{bmatrix}-\frac{d}{bc} & \frac{1}{c} \\ \frac{1}{b} & 0 \end{bmatrix}.
   \]
   Es decir, en el caso que $a=0$, entonces $A$ invertible si $bc \neq 0$ y su inversa es \begin{equation}
     A^{-1}=\begin{bmatrix}-\frac{d}{bc} & \frac{1}{c} \\[2ex] \frac{1}{b} & 0 \end{bmatrix} = \frac{1}{-bc} \begin{bmatrix}d & -b \\ -c & 0 \end{bmatrix}. 
   \end{equation}
   Es decir, la expresion de la inversa es igual a (1) (considernado que $a=0$).
   \\\\
   Reuniendo los dos casos: de (1) y (2) se deduce: \[
     \begin{bmatrix} a & b \\ c & d \end{bmatrix} \text{ es invertible } \Leftrightarrow \; ad-bc\neq 0,
   \]
   y en ese caso, su inversa viene dada por \[
     \begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix}d & -b \\ -c & a \end{bmatrix}.
   \]
   \begin{obs}
     Definiremos $det(A):=ad-bc$. Luego, \begin{list}{$\circ$}{}  
\item  $A$ invertible si y solo si $det(A)\neq0$. 
\item Veremos mas adelante que el uso de determinantes permitira establecer la generalizacion de este resultado para matrices $n \times n$ con $ n \geq 1$.
\end{list}
   \end{obs}
  \section{Determinantes.}
  El determinante es una funcion que a cada matriz cuadrada $n \times n$ con coeficientes en $\mathbb{K}$, le asocia un elemento de $\mathbb{K}$. \[
    \begin{array}{ccc} 
      det : \mathbb{K}^n \times \mathbb{K}^{n} & \xrightarrow{\quad\quad\quad\quad\quad\quad} & \mathbb{K} \\
      \begin{bmatrix}a_{ij}\end{bmatrix} & \xrightarrow{\quad\quad\quad\quad\quad\quad} & det\left(\begin{bmatrix}a_{ij}\end{bmatrix}\right) 
    \end{array}
  \]
\begin{center}
\textbf{Formas de definir determinante.}
\end{center}
\begin{list}{$\circ$}{}  
\item  Una forma de definir determinante es con una formula cerrada que usa el grupo de permutaciones. Esta forma de definir determinante esta fuera del alcance de este curso.
\item La forma que usaremos nosotros para definir determinante es mediante una definicion recursiva.
\item Es decir para calcular el determinante de una matriz $n \times n$, usaremos el calculo del determinante para matrices $n-1 \times n-1$,
\begin{list}{$\circ$}{}  
\item que a su vez se calcula usando el determinante de matrices $n-2\times n-2$
\begin{list}{$\circ$}{}  
\item y asi sucesivamente hasta llegar al caso base, que es el caso de matrices $1 \times 1$.
\end{list}
\end{list}
\end{list}
Del determinante, permite, entre otras cosas
\begin{list}{$\circ$}{}  
\item determinar si una matriz cuadrada es invertible,
\item dar una formula cerrada para la inversa de una matriz invertible.
\end{list}
Para dar la definicion del determinante de una matriz cuadrada $A$ usaremos submatrices de la matriz $A$.
\pagebreak

\begin{defi}
  Sea $A \in \mathbb{K}^{n \times n}$ y sean $i,j$ tales que $1 \leq i,j \leq n$. Entonces \[
    A(i|j) \in \mathbb{K}^{n-1 \times n-1}
  \]
  es la matriz que se obtiene eliminando la fila $i$ y la columna $j$ de $A$.
\end{defi}
\[
  A(i|j)=\begin{bNiceMatrix}
    a_{11} & \cdots & \cdots & a_{1j} & \cdots & a_{1n} \\
    \vdots & & & \vdots & & \vdots \\
    a_{i1} & \cdots & \cdots & a_{ij} & \cdots & a_{in} \\
    \vdots & & & \vdots & & \vdots \\
    \vdots & & & \vdots & & \vdots \\
    a_{m1} & \cdots & \cdots & a_{mj} & \cdots & a_{mn} 
\CodeAfter
 \tikz \draw [opacity=.20,line width=6mm,line cap=round,color=rojop2] 
              (3-1.center) -- (3-6.center);
  \tikz \draw [opacity=.20,line width=6mm,line cap=round,color=azulp2] 
              (1-4.center) -- (6-4.center);
  \end{bNiceMatrix}
\]
(tacho la fila $i$ y la columna $j$.)\\\\
Mas precisamente, \[
  A(i|j)=\begin{bmatrix} a_{11} & \cdots & \cdots & a_{1j-1} & a_{1j+1} & \cdots & a_{1n} \\
    \vdots & &  & \vdots & \vdots & & \vdots \\
    a_{i-11} & \cdots & \cdots & a_{i-1j-1} & a_{i-1j+1} & \cdots & a_{i-1n} \\
a_{i+11} & \cdots & \cdots & a_{i+1j-1} & a_{i-1j+1} & \cdots & a_{i+1n} \\

\vdots &  & & \vdots & \vdots & & \vdots \\

    \vdots & & &  \vdots & \vdots & & \vdots \\
    a_{m1} & \cdots & \cdots & a_{mj-1} & a_{mj+1} & \cdots &a_{mn}
\end{bmatrix}
\]
\begin{ej}\;\\
  Si $A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$, entonces $A(1|2)= \begin{bmatrix}4 & 6 \\ 7 & 9\end{bmatrix}$, $A(1|1)=\begin{bmatrix} 5 & 6 \\ 8 & 9 \end{bmatrix}$.\\\\
  Si $A=\begin{bmatrix}-1 & 3 & 2 & 5 \\ -2 & 7 & 2 & 6 \\ -3 & 2 & 1 & 11 \\ -4 & 5 & 3 & -3 \end{bmatrix}$, entonces $A(3|2)=\begin{bmatrix}-1 & 2 & 5 \\ -2 & 2 & 6 \\ -4 & 5 & -3  \end{bmatrix}$
\end{ej}
\pagebreak

\begin{defi}
  Sea $n \in \mathbb{N}$ y $A=\begin{bmatrix}a_{ij}\end{bmatrix} \in \mathbb{K}^{n \times n}$, entonces el determinante de $A$, denotado $det(A)$ o $|A|$, se define como 
  \begin{enumerate}[label=\arabic*.]
    \item Si $n = 1$, $detA=a_{11}$
    \item Si $ n > 1 $, \[
        detA=a_{11}detA(1|1))-a_{21}detA(2|1)+\cdots+(-1)^{1+n}a_{n1}detA(n|1),
      \]
      o
      \[
        detA=\sum_{i=1}^{n}{(-1)^{1+i}a_{i1}\;detA(i|1)}.
      \]
  \end{enumerate}
\end{defi}
Si $ 1 \leq i,j \leq n$, \begin{list}{$\circ$}{}  
\item $detA(i|j) :$ menor $i,j$ de $A$. 
\item $C_{ij} := (-1)^{i+j}detA(i|j) :$ cofactor $i,j$ de $A$. \\
  Luego \[
    detA=\sum_{i=1}^{n}A_{i1}C_{i1}.
  \]
\end{list}
Este es el calculo del determinante por desarrollo de la primera columna pues para calcular el determinante estamos usando los coeficientes de la primera columna: \[
  a_{11},a_{21},\cdots,a_{n1}.
\]
\begin{center}
\textbf{Determinante $2 \times 2$.}
\end{center}
Calculemos el determinante de las matrices $2 \times 2$. Sea \[
  A=\begin{bmatrix}a & b \\ c & d \end{bmatrix},
\]
entonces \[\begin{aligned}
  detA&=a\;det\begin{bmatrix}d\end{bmatrix}-c\;det\begin{bmatrix}b\end{bmatrix}\\
  &=ad-bc.
\end{aligned}
\]
\begin{obs}
  Si $A=\begin{bmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}$, entonces $det(A)=a_{11}a_{22}-a_{12}a_{21}$\\(solo es notacion).
\end{obs}
\begin{ej}
  Si $A=\begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}$, entonces $det(A)=1 \cdot 4 - 3 \cdot 2 = -2$.
\end{ej}
\begin{ej}
  Si \[
    A=\begin{bmatrix}cos\theta & -sen\theta \\ sen\theta & cos\theta \end{bmatrix},
  \]
  entonces \[
det(A)=cos^2\theta+sen^2\theta=1.
  \]
  \pagebreak

  Hemos visto en la clase anterior que $A$ es invertible si y solo si $ad-bc \neq 0$, es decir \[
    A \text{ es invertible si y solo si } detA \neq 0.
  \]
  Mas aun, la formula de la inversa de $A$ es \[
    A^{-1} = \frac{1}{det(A)} \begin{bmatrix}d & c \\ b & a \end{bmatrix}.
  \]
  Se puede reescribir como \[
    A^{-1}=\frac{1}{det(A)}\begin{bmatrix}C_{11} & C_{12} \\ C_{21} & C_{22} \end{bmatrix}.
  \]
  Esta formula se puede generalizar a matrices $ n \times n$.
\end{ej}
\begin{center}
\textbf{Determinantes $3 \times 3$.}
\end{center}
Sea \[
  A=\begin{bmatrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix},
\]
entonces
\[
  detA=a_{11}\begin{vmatrix}a_{22}&a_{23} \\ a_{32}&a_{33} \end{vmatrix}-a_{21}\begin{vmatrix}a_{12} & a_{13} \\ a_{32} & a_{33} \end{vmatrix} + a_{31} \begin{vmatrix}a_{12} & a_{13} \\ a_{22} & a_{23} \end{vmatrix}
\]
\[
  \begin{aligned}
    &= a_{11}(a_{22}a_{33}-a_{23}a_{32})&&-a_{21}(a_{12}a_{33}-a_{13}a_{32})&&+a_{31}(a_{12}a_{23}-a_{13}a_{22})\\
    &=a_{11}a_{22}a_{33}-a_{11}a_{22}a_{32}&&-a_{12}a_{21}a_{33}+a_{13}a_{21}a_{32}&&+a_{12}a_{23}a_{31}-a_{13}a_{22}a_{31}.
    \end{aligned}
\]
\begin{obs}
  Observar que el determinante de una matriz $3 \times 3$ es una sumatoria de seis terminos cada uno de los cuales es de la forma $\pm a_{1i_{1}}a_{2i_{2}}a_{3i_{3}}$ e $ i_{1} i_{2}i_{3}$ puede ser cualquier permutacion de $123$.\\\\
Es decir \[
  |A|=\sum_{\sigma \in \mathbb{S}_{3}}{\pm a_{1\sigma(1)}a_{2\sigma(2)}a_{3\sigma(3)}},
\]donde $\mathbb{S}_3$ son las permutaciones de $3$ elementos.
\end{obs}
La formula \[
|A|=a_{11}a_{22}a_{33}-a_{11}a_{22}a_{32}-a_{12}a_{21}a_{33}+a_{13}a_{21}a_{32}+a_{12}a_{23}a_{31}-a_{13}a_{22}a_{31},
\]
no es facil de recordar, pero existe un procedimiento sencillo que nos permite obtenerla.\pagebreak 

Calculo de $|A|$: \begin{enumerate}[label=\arabic*.]
  \item a la matriz original le agregamos las dos primeras filas al final, 
  \item sumamos cada ``producto'' de las diagonales descendentes y 
  \item restamos cada ``producto de las diagonales ascendentes.''
  \end{enumerate}
  \begin{center}
\tikzset{node style ge/.style={circle}}
\begin{tikzpicture}[baseline=(A.center)]
  \tikzset{BarreStyle/.style =   {opacity=.3,line width=5 mm,line cap=round,color=#1}}
    \tikzset{SignePlus/.style =   {above left,inner sep=1.5pt,opacity=.3,circle,fill=azulp2}}
    \tikzset{SigneMoins/.style =   {below left,inner sep=-0.5pt,opacity=.3,circle,fill=rojop2}}
% the matrices
\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
{ a_{11} & a_{12} & a_{13}  \\
  a_{21} & a_{22} & a_{23}  \\
  a_{31} & a_{32} & a_{33}  \\
  a_{11} & a_{12} & a_{13} \\
  a_{21} & a_{22} & a_{23}\\
};

\draw [BarreStyle=azulp2] (A-1-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-3-3.south east) ;
 \draw [BarreStyle=azulp2] (A-2-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-4-3.south east)  ;
 \draw [BarreStyle=azulp2] (A-3-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-5-3.south east) ;
 
 \draw [BarreStyle=rojop2]  (A-3-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-1-3.north east) ;
\draw [BarreStyle=rojop2]  (A-4-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-2-3.north east) ;
\draw [BarreStyle=rojop2]  (A-5-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-3-3.north east) ;
\end{tikzpicture}
\end{center}
Es decir, \begin{enumerate}[label=(\alph*)]
  \item se suman $a_{11}a_{22}a_{33}$, $a_{21}a_{32}a_{13}$, $a_{31}a_{12}a_{23}$, y 
  \item se restan $a_{31}a_{22}a_{13}$, $a_{11}a_{32}a_{23}$, $a_{21}a_{12}a_{33}$
  \end{enumerate}
  obteniendose nuevamente \[
|A|=a_{11}a_{22}a_{33}-a_{11}a_{22}a_{32}-a_{12}a_{21}a_{33}+a_{13}a_{21}a_{32}+a_{12}a_{23}a_{31}-a_{13}a_{22}a_{31}
  \]
  \begin{ej}
    Si \[
      A=\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 6 & 8 & 9 \end{bmatrix}.
    \]
  \end{ej}
    Calculamos:
  \begin{center}
\tikzset{node style ge/.style={circle}}
\begin{tikzpicture}[baseline=(A.center)]
  \tikzset{BarreStyle/.style =   {opacity=.3,line width=3 mm,line cap=round,color=#1}}
    \tikzset{SignePlus/.style =   {above left,inner sep=1.5pt,opacity=.3,circle,fill=azulp2}}
    \tikzset{SigneMoins/.style =   {below left,inner sep=-0.5pt,opacity=.3,circle,fill=rojop2}}
% the matrices
\matrix (A) [matrix of math nodes, nodes = {node style ge},,column sep=0 mm] 
{
  1 & 2 & 3 \\ 
  4 & 5 & 6 \\
  7 & 8 & 9 \\
  1 & 2 & 3 \\
  4 & 5 & 6 \\
};

\draw [BarreStyle=azulp2] (A-1-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-3-3.south east) ;
 \draw [BarreStyle=azulp2] (A-2-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-4-3.south east)  ;
 \draw [BarreStyle=azulp2] (A-3-1.north west) node[SignePlus=green] {$\textcolor{black}{+}$} to (A-5-3.south east) ;
 
 \draw [BarreStyle=rojop2]  (A-3-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-1-3.north east) ;
\draw [BarreStyle=rojop2]  (A-4-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-2-3.north east) ;
\draw [BarreStyle=rojop2]  (A-5-1.south west) node[SigneMoins=red] {\strut \textcolor{black}{$-$}} to (A-3-3.north east) ;
\end{tikzpicture}
\end{center}
Entonces $det(A)=45+96+84-105-48-72=0$.
\begin{ej}
  Calculemos el determinante de una matriz $ 4 \times 4$ \[\begin{aligned}
\begin{vmatrix}1 & 1 & 1 & 1 \\ 1 & 2 & 3 & 0 \\ 0 & 1 & 2 & 3 \\ 2 & 2 & 1 & 1 \end{vmatrix}
                     &= 1 \begin{vmatrix}2 & 3 & 0 \\ 1 & 2 & 3 \\ 2 & 1 & 1 \end{vmatrix} - 1 \begin{vmatrix}1 & 1 &1 \\ 1 & 2 & 3 \\ 2 & 1 & 1 \end{vmatrix} + 0 \begin{vmatrix}1 & 1 & 1 \\ 2 & 3 & 0 \\ 2 & 1 & 1 \end{vmatrix} -2 \begin{vmatrix} 1 & 1 & 1 \\ 2 & 3 & 0 \\ 1 & 2 & 3 \end{vmatrix} \\
                     &=13-3+0-8 \\
                     &=2.
  \end{aligned}
  \]
\end{ej}
\begin{ej}
  Las formulas para $n=2$ y $n=3$ son un caso particular de la formula \[
det(A)=\sum_{\sigma \in \mathbb{S}_{n}}sgn(\sigma)a_{1\sigma(1)}a_{2\sigma(2)}\cdots a_{n\sigma(n)}
  \]
  valida para todo $n$, la cual tiene $n!$ terminos.\\\\
  ($\mathbb{S}_{n}$ es el conjunto de permutaciones de $\left\{1,\dots,n\right\}$ y $sgn : \mathbb{S}_{n} \to \{1,-1\}$ es una funcion, llamada funcion signo.)
\end{ej}
\begin{obs}\;
  \begin{list}{$\circ$}{}  
\item Del modo que lo presentamos, el determinante no es mas que una formula que le aplicamos a una matriz. Pero aqui solo estamos viendo el producto final de años y años de estudio.
\item De hecho, el determinante existio antes que las matrices y se lo utilizaba para ``determinar'' cuando un sistema de $n$ ecuaciones con $n$ incognitas tiene solucion unica (si y solo si el determinante es no nulo).
\item Tambien tiene otras aplicaciones. 
\item Se puede googlear ``determinante'' (o ``determinant'' en ingles) o leer la pagina de Wikipedia para saber mas.
\end{list}
\end{obs}
Viendo la formula del determinante \[
  det(A)=a_{11}detA(1|1)-a_{21}detA(2|1)+\cdots+(-1)^{1+n}a_{n1}detA(n|1)
\]
notamos que mientras mas ceros tenga la primera columna (o sea, mas $a_{i1}$'s iguales a $0$), menos cuentas deberemos hacer.\\\\
Por ejemplo, si $A$ es triangular superior o una $MERF$.
\pagebreak

\begin{center}
\textbf{Determinante de una matriz triangular superior.}
\end{center}
\begin{prop}
  El determinante de una matriz triangualr superior es el producto de los elementos de la diagonal \[
    \begin{vmatrix}
 a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
  0 & a_{22} & a_{23} & \cdots & a_{2n} \\
  0 & 0 & a_{33} & \cdots & a_{3n} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & 0 & \cdots & a_{nn}
  \end{vmatrix}=a_{11}\cdot a_{22} \cdot a_{33} \cdots a_{a_{nn}}
  \]
  (esto aplica tambien a las matrices diagonal).
\end{prop}
\begin{demo}
  Podemos demostrar el resultado por induccion sobre $n$. \\\\
  Si $n=1$, es decir si $A=\begin{bmatrix}d_{1}\end{bmatrix}$, el determinante vale $d_1$.\\\\
  Si $n>1$, observemos que $A(1|1)$ es tambien triangular superior con valores $d_2,\dots,d_n$ en la diagonal principal.\\\\
  Por definicion de determinante observamos que el desarrollo por la primera columna solo tiene un termino ($d_1$) en la primera posicion. \\\\
  Por lo tanto, \[
    det(A)=d_1 \; detA(1|1)\overset{(HI)}{=}d_1 \cdot (d_2 \cdots d_n). 
  \]
  \qed
\end{demo}
\begin{center}
\textbf{Casos particulares.}
\end{center}
\begin{corol2}
  \[
det(Id_n)=1
  \]
\end{corol2}
\begin{corol2}
  Si $R \in \mathbb{K}^{n \times n}$ es una $MERF$, entonces \[det(R)=\begin{dcases} 1 & \text{si } R \text{ no tiene filas nulas} \\
  0 & \text{si } R \text{ tiene filas nulas}
\end{dcases}\]
\end{corol2}\pagebreak

Volvamos a la formula del determinante \[
  det(A)=a_{11}detA(1|1)-a_{21}detA(2|1)+\cdots + (-1)^{1+na_{n1}}detA(n|1)
\]
y a la observacion de que con mas ceros en la primera columna menos cuentas deberemos hacer.\\\\
Con las operaciones elementales por filas podemos anular las entradas no nulas como lo haciamos para transformar una matriz en $MERF$.\\\\
Entonces deberiamos analizar como estas operaciones afectan en el calculo del determinante.
\begin{teo}[E1]
  Sea $A \in \mathbb{K}^{n \times n}$ y $c \in \mathbb{K}$ no nulo. Sea \[
    A \xrightarrow{cF_{i}}B,
  \]
  entonces, \[
det(B)=c\;det(A).
  \]
\end{teo}
\begin{ej}[verificar cuentas]
  \[
    A=\begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix} \xrightarrow{10F_1}B=\begin{bmatrix}10 & 20 \\ 3 & 4 \end{bmatrix},
  \]
  y
  \[
    det\begin{bmatrix}10 & 20 \\ 3 & 4 \end{bmatrix}=-2 = 10 det \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}.
  \]
\end{ej}
\begin{teo}[E2]
  Sean $A \in \mathbb{K}^{n \times n}$ y $1 \leq s,t \leq n$ con $s \neq t$. Sea $t \in \mathbb{K}$ y \[
    A \xrightarrow{F_r+tF_s} B,
  \]
  entonces \[
det(B)=det(A).
  \]
\end{teo}
\begin{ej}[verificar cuentas]
  \[
    A=\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} \xrightarrow{F_2+10F_1} B =\begin{bmatrix}1 & 2 \\ 13 & 24 \end{bmatrix},
  \]
  y
  \[
    det \begin{bmatrix} 1 & 2 \\ 13 & 14 \end{bmatrix}=-2=det \begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix}.
  \]
\end{ej}
\begin{teo}[E3]
  Sean $A \in \mathbb{K}^{n \times n}$. Sea \[
    A \xrightarrow{F_r \leftrightarrow F_s} B,
  \]
  entonces \[
det(B)=-det(A)
  \]
\end{teo}
\begin{ej}[verificar cuentas]
  \[
    A=\begin{bmatrix}1 & 2 \\ 3 & 4 \end{bmatrix} \xrightarrow{F_2 \leftrightarrow F_1} B = \begin{bmatrix} 3  & 4 \\ 1 & 2 \end{bmatrix},
  \]
  y 
  \[
    det \begin{bmatrix} 3 & 4 \\ 1 & 2 \end{bmatrix} = 2 = -det \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}.
  \]
\end{ej}
  \begin{obs}\;
\begin{list}{$\circ$}{}  
\item  En todos los casos $det(B)=k \; det(A)$ para algun $k \in \mathbb{K}$ no nulo.
\item A partir de lo anterior podemos plantear una estrategia general para calcular el determinante.
\end{list}\end{obs}
\begin{prop}
  Sea $A$ matriz $n \times n$ y $C$ matriz que se obtiene de $A$ a partir de operaciones elementales de fila. Luego $det(C)=k \; det(A)$, para $k \neq 0$ en $\mathbb{K}$.
\end{prop}
\begin{center}
  \textbf{Estrategia para calcular el determinante de $A \in \mathbb{K}^{n \times n}$.}
\end{center}
\begin{enumerate}[label=\arabic*.]
  \item Realizando $l$ operaciones elementales de fila obtenemos a partir de $A$ una matriz triangular superior $C$.
  \item Luego, \[
det(C)=k_l\cdots k_1 det(A)
    \]
  \item Como $C$ es triangular superior, el determinante de $C$ es el producto de la diagonal. 
  \item Entonces, podemos despejar \[
      det(A)=\frac{1}{k_l \cdots k_1} det(C)
    \]
  \end{enumerate}
  \pagebreak

\begin{ej}
  Calcular el determinante de $A=\begin{bmatrix} 0 & 2 & 3 \\ 2 & -1 & 7 \\ 1 & 3 & 0 \end{bmatrix}$
\end{ej}
\textbf{Solucion.}\\\\
Primero, le aplicamos operaciones elementales a $A$ hasta obtener una matriz triangular superior. \[
  \begin{aligned}
    A=&\begin{bmatrix}0 & 2 & 3 \\ 2 & -1 & 7 \\ 1 & 3 & 0 \end{bmatrix} \xrightarrow{F_1 \leftrightarrow F_2} \begin{bmatrix}1 & 3 & 0 \\ 2 & -1 & 7 \\ 0 & 2 & 3 \end{bmatrix} \xrightarrow{F_2-2F_1} \begin{bmatrix} 1 & 3 & 0 \\ 0 & -7 & 7 \\ 0 & 2 & 3 \end{bmatrix} \\
    \xrightarrow{-\frac{1}{7}F_2} & \begin{bmatrix} 1 & 3 & 0 \\ 0 & 1 & -1 \\ 0 & 2 & 3 \end{bmatrix} \xrightarrow{F_3-2F_2}\begin{bmatrix}1 & 3 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 5 \end{bmatrix} =C
  \end{aligned}
\]
Entonces \[
C=e_4(e_3(e_2(e_1(A))))
\]donde $e_1, e_2, e_3$ y $e_4$ denotan las operaciones elementales aplicadas en cada paso. Ahora calculemos e determinante de $C$ usando Teoremas E$i$: \[
5=det\begin{bmatrix}1 & 3 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 5 \end{bmatrix} = det(e_4(e_3(e_2(e_1(A))))).
\]Donde $e_4=F_3-2F_2,\;e_3=-\frac{1}{7}F_2,\;e_2=F_2-2F_1,\;e_1=F_1\leftrightarrow F_3.$\\\\
Luego, \[
  \begin{aligned}
    5 &=&& det(e_4(e_3(e_2(e_1(A))))) \\
    \overset{\text{Teorema E2}}&{=}&& det(e_3(e_2(e_1(A)))) \\
    \overset{\text{Teorema E1}}&{=}&&-\frac{1}{7}det(e_2(e_1(A))) \\
    \overset{\text{Teorema E2}}&{=} &&-\frac{1}{7}det(e_1(A))\\
    \overset{\text{Teorema E3}}&{=} &&-\frac{1}{7}\cdot (-1)\cdot det(A)
  \end{aligned}
\]
De esta igualdad podemos despejar $det(A)$ \[
detA=7 \cdot 5 = 35. 
\]
\qed\pagebreak

Mas adelante veremos resultados de suma utilidad: 
\begin{teo}Sean $A$ y $B$ matrices $n \times n$. Entonces \begin{enumerate}[label=\arabic*.]
  \item $det(AB)=det(A)det(B)$,
  \item $A$ es invertible si y solo si $det(A) \neq 0$, 
  \item Si $A$ invertible, $det(A^{-1})=\frac{1}{det(A)}$.
\end{enumerate}
\end{teo}
Recordemos
\begin{teo} Sea $A \in \mathbb{K}^{n \times n}$. \begin{enumerate}[label=\textcolor{azulp2}{E\arabic*.}]
  \item Si $  c \in \mathbb{K}$ no nulo, \[
      A \xrightarrow{cF_i} B \quad \quad \Rightarrow \quad \quad det(B)=c\;det(A).
    \]
  \item Si $ 1 \leq s,t, \leq n$ con $ s \neq t$ y $t \in \mathbb{K}$: \[
      A \xrightarrow{F_r+tF_s} B \; \; \; \Rightarrow \quad \quad det(B)=det(A).
    \]
\item \[
    A \xrightarrow{F_r \leftrightarrow F_s} B  \;\;\quad \Rightarrow \quad \quad det(B) = -det(A).
  \]
\end{enumerate}
\end{teo}

\begin{corol}
  $A \in \mathbb{K}^{n \times n}$. \begin{enumerate}[label=\arabic*.]
    \item Si $A$ tiene dos filas iguales, entonces $det(A)=0$.
    \item Si $A$ tiene una fila nula, entonces $det(A)=0$.
    \end{enumerate}
\end{corol}
\begin{demo}\; \begin{enumerate}[label=\arabic*.]
  \item Si $F_r=F_s$ con $r \neq s$. Entonces \[\begin{array}{cccccccccccccc}
     A \xrightarrow{F_r \leftrightarrow F_s} A & \;\;\text{T. E3} &&& det(A)=-det(A).
 \end{array}
   \]
   Luego $det(A)=0$.
 \item 
   Si $F_r=0$, \[\begin{array}{cccccccccccc}
     A\xrightarrow{2F_r}A  &&& \text{T. E1}  & && det(A)=2det(A).
   \end{array}
   \]
   Luego $det(A)=0$.
\end{enumerate}
\qed
\end{demo}
\begin{corol}

  Sea $E=e(Id_n)$, matriz elemental en $\mathbb{K}^{n \times n}$. \begin{enumerate}[label=\textcolor{azulp2}{E\arabic*.}]
   \item Si $c \in \mathbb{K}$ no nulo, \[
      \begin{aligned}
        Id_n \xrightarrow{cF_i}E \quad \quad \;\Rightarrow \quad\quad \; det(E)=c.
      \end{aligned}
    \]
    \item Si $1 \leq s,t \leq $ n con $s \neq t$ y $t \in \mathbb{K}$:\[\begin{aligned}
      Id_n \xrightarrow{F_r+tF_s} E \quad\; \Rightarrow \quad\quad det(E)=1.\end{aligned}
      \]
    \item \[
        Id_n \xrightarrow{F_r \leftrightarrow F_s} E \quad\quad\;\; \Rightarrow \quad\quad det(E) = -1.
      \]
  \end{enumerate}

\end{corol}
\begin{demo}
  Se demuestra trivialmente considerando que en todos los casos $E=e(Id_n)$ donde $e$ es una operacion elemental por fila, considerando que $det(Id_n)=1$ y aplicando los teoremas E1, E2, y E3. \\ 
  \qed
\end{demo}
\begin{teo}
  Sea $A \in \mathbb{K}^{n \times n}$ y $E$ una matriz elemental $n \times n$. Entonces \begin{equation}\tag{1}
    det(EA)=detE\;detA.
  \end{equation}
\end{teo}
\begin{demo}
   En todos los casos $EA=e(A)$ donde $e$ es una operacion elemental por fila.\begin{enumerate}[label=(E\arabic*)]
\item Si $c \neq 0$ y $Id_n \xrightarrow{cF_r} E$, tenemos $det(E)=c$ y \[
det(EA)=det(e(A))=c\cdot det(A)=det(E)det(A).
\]
\item Si $Id_n \xrightarrow{F_r+c_Fs} E$, luego $det(E)=1$ y \[
det(EA)=det(e(A))=det(A)=det(E)=det(A).
\]
\item Ejercicio.
   \end{enumerate}\qed
\end{demo}
\begin{corol}
  Sea $A=E_1E_2\cdots E_kB$ donde $E_1,E_2,\cdots,E_k$ son matrices elementales. Entonces, \[
det(A)=det(E_1)det(E_2)\cdots det(E_k)det(B).
  \]
\end{corol}
\begin{demo}
\[
det(A)=det(E_1(E_2\cdots E_k B)) = det(E_1) det(E_2 \cdots E_kB), 
\]
y asi sucesivamente (induccion). \\ \qed
\end{demo}
\begin{corol}
  Sea $A=E_1E_2\cdots E_k$ producto de matrices elementales en $\mathbb{K}^{n \times n}$. Entonces \[
det(A)=det(E_1)det(E_2)\cdots det(E_k).
  \]
  ($B =Id_n.$)
  \qed
\end{corol}
\begin{teo}
  $A \in \mathbb{K}^{n \times n}$ es invertible si y solo si $det(A)\neq 0$.
\end{teo}
\begin{demo}[$\Rightarrow$] \;\\\\
$A$ invertible $A=E_1E_2\cdots E_k \Rightarrow det(A)=det(E_1)det(E_2)\cdots det(E_k)$. Como el determinante de matrices elementales es no nulo, $det(A) \neq 0$. 
\end{demo}\;

\begin{demo}[$\Leftarrow$]\;\\\\
Sean $E_1,E_2, \dots ,E_k$ matrices elementales tales que $R=E_1E_2 \cdots E_k A$ y $R$ es $MERF$. Luego, \[
det(R)=det(E_1)det(E_2)\cdots det(E_k) det(A).
\]
Como los determinantes de matrices elementales son no nulos \begin{equation}\tag{*}
\frac{det(R)}{det(E_1)det(E_2)\cdots det(E_k)}=det(A).
\end{equation}
Supongamos que $R$ no es la identidad. \\\\
Entonces $det(R)=0$ (ver enunciados anteriores) $\overset{(*)}{\Rightarrow} det(A)=0$, absurdo. \\\\
Luego, $R=Id_n \Rightarrow A$ es equivalente por filas a $Id_n \Rightarrow A$ invertible. \\ \qed
\end{demo}
\pagebreak

\begin{teo}
  Sean $A,B \in \mathbb{K}^{n \times n}$, entonces \[
det(AB)=det(A)det(B)
  \]
\end{teo}
\begin{demo}\;\\
\begin{enumerate}[label=-]
\item Si $A$ invertible $\Rightarrow A=E_1 \cdots E_k$ y $AB=E_1 \cdots E_kB$, luego por el Corolario 8 $det(AB)=det(E_1) \cdots det(E_k) det(B) = det(A) det(B)$. 
\item Si $A$ no invertible $\Rightarrow A=E_1 \cdots E_k R$ y $R$ $MERF$ con la ultima fila nula. 
\end{enumerate}Luego,
\begin{list}{$\circ$}{}  
\item $RB$ tiene la ultima fila nula $\Rightarrow det(RB)=0$ (corolario 5), 
\item $det(AB)=det(E_1 \cdots E_k RB) = det(E_1 \cdots E_k)det(RB)=0$. 
\item Como $A$ no invertible $\Rightarrow det(A)=0 \Rightarrow det(A) det (B) = 0$.
\end{list}\qed
\end{demo}
\begin{corol}
Sean $A$, $B$ matrices $n \times n$, entonces \begin{list}{$\circ$}{}  
\item  $det(A^{m})=det(A)^{m}$ para $m \in \mathbb{N}$. 
\item $det(AB)=det(BA)$.
\end{list}
\end{corol}
\begin{demo}
  \;
\begin{list}{$\circ$}{}  
\item $det(A^m)=det(A \cdot A^{m -1 })=det(A) \cdot det(A^{m - 1})$ y se demuestra por induccion. 
\item $det(AB)=det(A)det(B)=det(B)det(A)=det(BA)$. 
\end{list}
\qed
\end{demo}
\begin{defi}
  Sea $A \in \mathbb{R}^{m \times n}$. La transpuesta de $A$ es la matriz $A^t \in \mathbb{R}^{n \times m}$ cuyas entradas son definidas por \[
\begin{bmatrix}A^{t}\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ji}
  \]
\end{defi}
\begin{ej}
  Sea \[
A=\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix} \quad \quad \Rightarrow \quad \quad A^t = \begin{bmatrix}1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}
  \]
  $\left(\begin{bmatrix}A^t\end{bmatrix}_{12}=\begin{bmatrix}A\end{bmatrix}_{21}=4,\; \begin{bmatrix}A^t\end{bmatrix}_{13}=\begin{bmatrix}A\end{bmatrix}_{31}=3,etc.\right)$
\end{ej}
Para matrices cuadradas, en general: \[
A=\begin{bmatrix}a_{11} & \cdots & a_{i1} & \cdots & a_{1n} \\ 
\vdots & & \vdots && \vdots \\ a_{i1} & \cdots & a_{ii} & \cdots & a_{in} \\
\vdots & & \vdots && \vdots \\ a_{n1}&  \cdots & a_{ni} & \cdots & a_{nn} \end{bmatrix}
\]
Entonces \[
A^t=\begin{bmatrix} 
a_{11} & \cdots & a_{i1} & \cdots & a_{n1} \\
\vdots && \vdots && \vdots \\
a_{1i} & \cdots & a_{ii} & \cdots & a_{ni} \\
\vdots && \vdots && \vdots \\
a_{1n} & \cdots & a_{in} & \cdots & a_{nn}
\end{bmatrix}
\]
\begin{teo}
  El determinante de una matriz cuadrada es igual al determinante de su transpuesta. \\\\
  Es decir, si $A$ matriz $n \times n$, \[
det(A)=det(A^t).
  \]
  La demostracion esta en las notas del curso. 
\end{teo}
\textbf{Idea de la demostracion.} No es dificil ver que \begin{list}{$\circ$}{}  
\item para $E$ matriz elemental $det(E^t)=det(E)$, 
\item $(A_1 \cdot A_2 \cdots A_k)^t = A^t_k \cdots A^t_2 \cdot A^t_1 $.
\end{list}
Sea \[
  A=\begin{bmatrix} a_{11} & \cdots & 0 & \cdots & 0 \\ \vdots & & \vdots && \vdots \\ a_{i1} & \cdots & a_{ii} & \cdots & 0 \\ \vdots && \vdots && \vdots \\ a_{n1} & \cdots & a_{ni} & \cdots & a_{nn} \end{bmatrix}
\]
triangular inferior. Entonces, \[
  A^t=\begin{bmatrix} a_{11} & \cdots & a_{i1} & \cdots & a_{n1}  \\ \vdots && \vdots && \vdots \\ 0 & \cdots & a_{ii} & \cdots & a_{ni} \\ \vdots && \vdots && \vdots \\0 & \cdots & 0 & \cdots & a_{nn} \end{bmatrix}
\]
es triangular superior.
\begin{prop}
  El determinante de una matriz triangular inferior es igual al producto de los elementos de la diagonal.
\end{prop}
\begin{demo}
  La transpuesta de una matriz triangular inferior es una matriz triangular superior. \\\\ Entonces la proposicion es una consecuencia del teorema anterior y la proposicion referida al determinante de una triangular superior. \\ \qed
\end{demo}
\begin{obs}
  La transpuesta transforma filas en columnas y columnas en filas. \\\\ Gracias a esta observacion podemos deducir como cambia el determinante de una matriz al aplicarle ``operaciones elementales por columna''.
\end{obs}
\begin{ej}
  Si una matriz tiene una columna con muchos ceros, podemos intercambiarla con la primera columna. \[
    A=\begin{bmatrix} 1 & 5 & 7 & 1 \\ 2 & 0 & 7 & 1 \\ 3 & 0 & 8 & 1 \\ 4 & 0 & 1 & 1 \end{bmatrix} \longrightarrow B=\begin{bmatrix} 5 & 1 & 6 & 1 \\ 0 & 2 & 7 & 1 \\ 0 & 3 & 8 & 1 \\ 0 & 4 & 1 & 1 \end{bmatrix}
  \]
  Entonces \[
det(A)=-det(B)=-5\; detB(1|1)
  \]
\end{ej}
\begin{ej}
  Si una matriz tiene una fila con muchos ceros, entonces intercambio esta con la primera fila, luego transpongo y calculo el determinante. \[
    A=\begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 0 & 0 & 0 \\ 6 & 7 & 8 & 9 \\ 1 & 1 & 1 & 1 \end{bmatrix} \longrightarrow B=\begin{bmatrix} 5 & 0 & 0 & 0 \\ 1 & 2 & 3 & 4 \\ 6 & 7 & 8 & 9 \\ 1 & 1 & 1 & 1 \end{bmatrix} \longrightarrow B^t= \begin{bmatrix} 5 & 1 & 6 & 1 \\ 0 & 2 & 7 & 1 \\ 0 & 3 & 8 & 1 \\ 0 & 4 & 1 & 1 \end{bmatrix}
  \]
\end{ej}
Entonces \[
  det(A)=-det(B)=-det(B^t)=-5 \; detB^t(1|1)
\]
\pagebreak

El determinante se puede calcular desarrollando por cualquier columna o fila. 
\begin{teo}
  Sea $A$ matriz $n \times n$, entonces el determinante 
  \begin{list}{$\circ$}{}  
  \item  se puede calcular el determinante por la columna $j$ asi: \[
      det(A)=\sum_{i=1}^{n}(-1)^{i+j}a_{ij}detA(i|j),
    \]
  \item se puede calcular el determinante por la fila $i$ asi: \[
      det(A)=\sum_{j=1}^{n}(-1)^{i+j}a_{ij}detA(i|j)
    \]
\end{list}
    (la diferencia entre ambas formulas es la variable de la sumatoria)
\end{teo}
La demostracion de este teorema (que no la haremos), se basa en dos resultados que ya mencionamos. \begin{enumerate}[label=(\Alph*)]
  \item Teorema E3: \[
      A \xrightarrow{F_r \leftrightarrow F_s} B \quad \quad \Rightarrow \quad \quad det(B)=-det(A).
    \]
\item $$det(A^t)=det(A),$$
  \end{enumerate}
  y un resultado que no es dificil demostrar: \begin{enumerate}[label=(\Alph*), start=3]
    \item \[
        A \xrightarrow{C_r \leftrightarrow C_s} B \quad \quad \Leftrightarrow \quad \quad A^t \xrightarrow{F_r \leftrightarrow F_s } B ^t . 
      \]
    \end{enumerate}
    Luego, \begin{enumerate}[label=(\Alph*), start=4]
      \item \[
          det(B) \overset{\text{(B)}}{=}det(B^t) \overset{\text{(A)}}=-det(A^t)=\overset{\text{(B)}}-det(A).
        \]
    \end{enumerate}
    Usando estos resultados, y un poco de manipulacion de indices, se obtiene una demostracion del teorema.
    \pagebreak

    \section{Autovalores y autovectores.}
\begin{defi}
  Sea $A \in \mathbb{K}^{n \times n}$. Se dice que $\lambda \in \mathbb{K}$ es una autovalor de $A$ si existe $v \in \mathbb{K}^{n}$ no nulo tal que \[
Av=\lambda v.
  \]
  En ese caso decimos que $v$ es un autovector asociado a $\lambda$.
\end{defi}

\begin{ej}
  $1$ es un autovalor de $Id_n$ y todo $v \in \mathbb{K}^n$ es un autovector asociado a $1$ pues \[
Id_n \; v=1 \cdot v = v
  \]
\end{ej}

\begin{obs}
  El autovalor puede ser $0$ pero el autovector nunca puede ser $0$.
\end{obs}
\begin{obs}
  $0$ es un autovalor de $\begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$ y $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$ es un autovector asociado a $0$ pues \[
  \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix}1 \\ 0 \end{bmatrix}=\begin{bmatrix}0 \\ 0 \end{bmatrix} = 0 \begin{bmatrix}1 \\ 0 \end{bmatrix}
\]
\end{obs}
\begin{obs}
  La existencia de autovalores dependen del cuerpo donde estamos trabajando.
\end{obs}
\begin{ej}
 Sea $A \in \mathbb{R}^{2 \times 2}$ \[
   A=\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}.
 \]
\end{ej}
Entonces, $A$ no tiene autovalores reales.
\\\\
Veremos que si permitimos autovalores complejos entonces $A$ si tiene autovalores.
\begin{defi}
  Dado $i \in \{1, \dots , n\}$, se denota $e_i$ al vector de $\mathbb{K}^n$ cuyas coordenadas son todas ceros excepto la coordenada $i$ que es un $1$.\[
    e_i=\begin{bmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{bmatrix}
  \]
  El conjunto $\{e_1, \dots ,e_n\}$ se llama base canonica de $\mathbb{K}^n$.
\end{defi}
\begin{ej}
  En $\mathbb{K}^3$ la base canonica es $e_1=\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, \; e_2=\begin{bmatrix}0 \\ 1 \\ 0\end{bmatrix}, \; e_3 =\begin{bmatrix} 0 \\ 0 \\ 1\end{bmatrix}$. 
\end{ej}
\begin{ej}[Matriz diagonal.] \;\\
Sea $D \in \mathbb{K}^{n \times n}$ una matriz diagonal con entradas $\lambda_1, \lambda_2, \dots ,\lambda_n$. Entonces $e_i$ es un autovector con autovalor $\lambda_i$ $\forall i \in \{1, \dots ,n\}$
\end{ej}
\begin{demo}
  Recordar que la multiplicacion $De_i$ se corresponde con multiplicar cada fila de $e_1$ por el elemento correspondiente de la diagonal. \\\\ Como las filas (en este caso entradas) de $e_i$ son todas nulas excepto un $1$ en la entrada $i$ queda \[
    De_i =\begin{bmatrix} 0 \\ \vdots \\ \lambda_i \\ \vdots \\ 0 \end{bmatrix} = \lambda_i e_i
  \]
  \qed
\end{demo}
\begin{obs}
  Puede haber varios autovectores con el mismo autovalor. \\\\ Vimos esto en el ejemplo con $Id$ y en el caso de la diagonal si tiene entradas iguales sucede lo mismo. \\ \\ Mas aun el conjunto de todos los autovectores con un mismo autovalor es invariante por la suma y la multiplicacion por escalares. \\\\
  En particular los multiplos de un autovector son autovectores con el mismo autovalor.
\end{obs}
\begin{defi}
  Sea $A \in \mathbb{K}^{n \times n}$ y $\lambda \in \mathbb{K}$ un autovalor de $A$. El autoespacio asociado a $\lambda$ es \[
  V_{\lambda}=\{v \in \mathbb{K}^n | Av=\lambda v\}.
  \]
  Es decir, $V_{\lambda}$ es el conjunto formado por todos los autovectores asociados a $\lambda$ y el vector nulo.
\end{defi}
\begin{teo}
  Si $v$ y $w$ pertenecen al autoespacio de $A$ asociado a $\lambda$, entonces $v+tw$ tambien pertenece a $V_\lambda$.
\end{teo}
\begin{demo}
  \[
A(v+tw)=Av+tAw=\lambda v + t\lambda w = \lambda(v+tw).
  \]
  \qed
\end{demo}
\begin{prop}
  Un autovector no puede tener dos autovalores distintos. \\\\ Por lo tanto autovectores con autovalores distintos son distintos.
\end{prop}
\begin{demo}
  Supongamos que $Av= \lambda v$ y $Av=\mu v$. Entonces $\lambda v = \mu v$ y por lo tanto \[
    (\lambda - \mu ) v = \begin{bmatrix}(\lambda - \mu ) v_1 \\ \vdots \\ (\lambda - \mu) v_n \end{bmatrix} = \begin{bmatrix} 0 \\ \vdots \\ 0 \end{bmatrix}
  \]
\end{demo}
Como $v \neq 0$ por ser autovector, alguna de sus coordenadas es no nula. Entonces $\lambda - \mu$ tiene que ser $0$ (para anular esa coordenada) o dicho de otro modo $\lambda = \mu$.
\\ \qed
\\\\
\textbf{Problema.} Hallar los autovalores de $A \in \mathbb{K}^{n \times n}$ y para cada autovalor, describir explicitamente o parametricamente el autoespacio asociado.
\begin{list}{$\circ$}{}  
\item En otras palabras nos preguntamos que $\lambda \in \mathbb{K}$ y que $v \in \mathbb{K}^n$ satisfacen \[
Av=\lambda v \Longleftrightarrow Av-\lambda v = 0 \Longleftrightarrow (A-\lambda \; Id) v = 0.
  \]
\item La ultima igualdad es un sistema de ecuaciones lineales. Queremos ver entonces si existe un $v \in \mathbb{K}^n$ no nulo que sea solucion del sistema homogeneo \begin{equation}\tag{*} 
    (A- \lambda \; Id)X=0.
  \end{equation}
\item Un sistema $BX=0$ tiene solucion no trivial si y solo si $det(B)=0$. Por lo tanto (*) tiene solucion no trivial si y solo si \[
det(A-\lambda \; Id)=0.
  \]
\end{list}
\textbf{Conclusion.} $\lambda \in \mathbb{K}$ es un autovalor de $A$ y $v \in \mathbb{K}^n$ es un autovector asociado a $\lambda$ si y solo si  \begin{list}{$\circ$}{}  
\item $det(A - \lambda \; Id)=0$. 
\item $v$ es solucion del sistema homogeneo $(A-\lambda \; Id)X=0$
\end{list}
Esta es casi la respuesta a nuestro problema. Para dar una respuesta mas operativa introduciremos el siguiente polinomio.
\pagebreak

\begin{defi}
  Sea $A \in \mathbb{K}^{n \times n}$. El polinomio caracteristico de $A$ es \[
\chi_A (x)=det(A-x \;Id) 
  \]
\end{defi}
\begin{ej}
  El polinomio caracteristico de $Id_n$ es \[
  \chi_{Id_n} (x) = (1-x)^n  \]
\end{ej}
\begin{demo}
  $Id-x\;Id=(1-x)Id$ es una matriz diagonal con $(1-x)$ en todas las entradas de la diagonal. Entonces el determinante es el producto de la diagonal. \\ \qed
\end{demo}
En genera, si $A=\begin{bmatrix}a_{ij}\end{bmatrix}$ matriz $n \times n$, tenemos que \[
\chi_A(x)=det(A-x \; Id)=det\begin{bmatrix} 
  a_{11}-x & a_{12} & \cdots & a_{1n} \\ 
  a_{21} & a_{22}-x & \cdots & a_{2n} \\
  \vdots & \vdots & \ddots & \vdots \\
  a_{n1} & a_{n2} & \cdots & a_{nn}-x
\end{bmatrix}
\]
y el polinomio caracteristico de $A$ es un polinomio de grado $n$, mas precisamente \[
  \chi_A(x)=(-1)^nx^n+a_{n-1}x^{n-1}+ \cdots + a_{1}x + a_0.
\]
Esto se puede demostrar por induccion.
\begin{ej}
  El polinomio caracteristico de $A= \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$ es $\chi_A(x)=x^2$.
\end{ej}
\begin{demo}
  $A-x\; Id=\begin{bmatrix} - x & 1 \\ 0 & -x \end{bmatrix}$ es triangular superior. Entonces el determinante es el producto de la diagonal. \\ \qed
\end{demo}
\begin{ej}
  Si $A=\begin{bmatrix}a & b \\ c & d \end{bmatrix}$, entonces $\chi_A(x)=(a-x)(d-x)-bc$. 
\end{ej}
\begin{demo}
  $A-x \; Id=\begin{bmatrix} a -x & b \\ c & d-x \end{bmatrix}$ y usamos la formula del determinante de una $2 \times 2$. 
  \\ \qed
\end{demo}
\pagebreak

\begin{prop}
  Sea $A \in \mathbb{K}^{n \times n}$. Entonces $\lambda \in \mathbb{K}$ es autovalor si y solo si $\lambda$ es raiz del polinomio caracteristico. ($\chi_A(\lambda)=0$)
\end{prop}
\begin{demo}
  \[
    \begin{aligned}
      \lambda \text{ es autovalor } & \Leftrightarrow  \text{ existe } v \neq 0 \text{ tal que } Av=\lambda v \\
                                    & \Leftrightarrow 0 = Av-\lambda v = Av-\lambda \; Id \; v = (A-\lambda \; Id)v \\
                                    & \Leftrightarrow (A-\lambda \; Id) X = 0 \text{ tiene solucion no trivial} \\
                                    & \Leftrightarrow \chi_A(\lambda)=det(A-\lambda \; Id)=0 \\
                                    & \Leftrightarrow \lambda \text{ es raiz del polinomio caracteristico.}
    \end{aligned}
  \]
\end{demo}
\begin{center}
\textbf{Metodo para encontrar autovalores y autovectores de $A$.}
\end{center}
\begin{enumerate}[label=\arabic*.]
  \item Calcular $\chi_A(x)=det(A-x \; Id)$,
  \item Encontrar las raices $\lambda_1, \dots , \lambda_{k}$ de $\chi_A(x)$. \\
    (no siempre se puede. No hay una formula o metodo general para encontrar las raices de polinomios de grado $5$ o superior)
  \item Para cada $i$ con $i \leq i \leq k$ resolver el sistema de ecuaciones lineales: \[
      (A-\lambda_i \; Id) X = 0.
    \]
    Las soluciones no triviales de este sistema so nlos autovectores con autovalor $\lambda_i$.
\end{enumerate}
\begin{ej}
  Encontrar autovalores y autovectores de la matriz \[
    A=\begin{bmatrix} 3 & -2 \\ 1 & 0 \end{bmatrix}.
  \]
\end{ej}
\textbf{Solucion.} 
\begin{enumerate}[label=\arabic*.]
  \item $\chi_A(x)=det\begin{bmatrix} 3 - x & -2 \\ 1 & -x \end{bmatrix} = x^2-3x+2=(x-1)(x-2)$.
  \item Los autovalores de $A$ son las raices de $\chi_A(x)$: $1$ y $2$.
  \item Debemos resolver los sistemas de ecuaciones \[
      (A-Id)X=0, \quad \quad (A-2 \; Id)X=0.
    \]
    Es decir, debemos resolver los sistemas \begin{equation}\tag{S1}
      \begin{bmatrix} 3 - 1 & -2 \\ 1 & -1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ \end{bmatrix} \quad \quad \Rightarrow \quad \quad \begin{bmatrix}2 & -2 \\ 1 & -1 \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} 
    \end{equation}
\begin{equation}\tag{S2}
      \begin{bmatrix} 3 - 2 & -2 \\ 1 & -2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ \end{bmatrix} \quad \quad \Rightarrow \quad \quad \begin{bmatrix}1 & -2 \\ 1 & -2 \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix} 
      \end{equation}\begin{equation}\notag\begin{aligned}
      (\text{S1}) &\begin{bmatrix} 2 & -2 \\ 1 & -1 \end{bmatrix} \xrightarrow{F_1-2F_2}&&\begin{bmatrix}0 & 0 \\ 1 & -1 \end{bmatrix} \Rightarrow x_1 -x_2 = 0 \Rightarrow (t,t) \text{ es solucion}.
\\
      (\text{S2}) &\begin{bmatrix} 1 & -2 \\ 1 & -2 \end{bmatrix} \xrightarrow{F_2-F_1} &&\begin{bmatrix}  1 & -2 \\ 0 & 0 \end{bmatrix} \Rightarrow x_1 -2x_2 = 0 \Rightarrow (2t,t) \text{ es solucion.}
    \end{aligned}
    \end{equation}
\end{enumerate}
\textbf{Respuesta final.} 
\begin{list}{$\circ$}{}  
\item Los autovalores de $A$ son $1$ y $2$. 
\item El auto espacio correspondiente al autovalor $1$ es \[
    V_1=\{t(1,1,):t \in \mathbb{R}\}.
  \]
\item El auto espacio correspondiente al autovalor $2$ es 
  \[
    V_2=\{t(2,1): t \in \mathbb{R}\}.
  \]
  \qed
\end{list}
\begin{ej}
  Sea $A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \in \mathbb{R}^2$. Encontrar los autovalores reales de $A$. 
\end{ej}
\textbf{Solucion.}$A-x \; Id=\begin{bmatrix}-x & -1 \\ 1 & -x \end{bmatrix}$, luego \[
\chi_A(x)=x^2+1.
\]
El polinomio no tiene raices reales, por lo tanto no existen autovalores reales (obviamente no hay autovectores).
\\\\
Sin embargo si nos dicen \\\\ encontrar autovalores y autovectores scomplejos de la matriz $A=\begin{bmatrix}0 & -1 \\ 1 & 0 \end{bmatrix}$, la respuesta va a ser diferente.\\\\
Lo que ocurre es que \[
\chi_A(x)=x^2+1=(x+i)(x-i),
\]
y este polinomio si tiene raices (complejas): $i$ y $-i$.
\\\\
En este caso, entonces, $i$ y $-i$ son los autovalores y es facil ver que \[
  V_i=\{\omega(i,1): \omega \in \mathbb{C}\}, \quad \quad V_{-i} =\{\omega (-i,1) : \omega \in \mathbb{C} \}.
\]
Nunca esta de mas comprobar los resultados: \[
  \begin{aligned}
    \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix}i \\ 1 \end{bmatrix} &= \begin{bmatrix} -1 \\ i \end{bmatrix} = i \begin{bmatrix} i \\ 1 \end{bmatrix}. \\
    \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix}-i \\ 1 \end{bmatrix} &= \begin{bmatrix} -1 \\ -i \end{bmatrix} = (-i) \begin{bmatrix} -i \\ 1 \end{bmatrix}.
  \end{aligned}
\]
\pagebreak

\section{Espacios vectoriales.}
A continuacion 
\begin{list}{$\circ$}{}  
\item definiremos espacios vectoriales,
\item daremos ejemplos de espacios vectoriales,
\item veremos cuando un vector se puede escribir como combinacion lineal de otros.
\end{list}
La materia en general gira alrededor del problema \begin{list}{$\circ$}{}  
\item  Resolver sistemas de ecuaciones 
\item Los conjuntos de soluciones son subconjunto de $\mathbb{R}^n$
\end{list}
Al comienzo de la materia introdujimos dos operaciones en $\mathbb{R}$
\begin{list}{$\circ$}{}  
\item Los vectores de $\mathbb{R}^n$ se pueden sumar y multiplicar por escalares
\end{list}
Ademas vimos que los conjuntos de soluciones son invariantes por estas operaciones. Dicho de otro modo 
\begin{list}{$\circ$}{}  
\item Las soluciones de un sistema homogeneo se pueden sumar y multiplicar por escalares
\end{list}
Estas son algunas de las preguntas que se responderan en esta parte de la materia\\\\
\textbf{Preguntas.}
\begin{enumerate}[label=\arabic*.]
  \item ¿Podremos generar todas las soluciones de un sistema sumando y multiplicando por escalares algunas pocas soluciones? 

  \item ¿Cual es la minima cantidad de soluciones que generan todas las soluciones? 
  \item ¿Como podemos representar cada solucion usando el conjunto generador?
\end{enumerate} 
Por otro lado, hay otras estructuras matematicas que tienen suma y producto por escalar 
\begin{list}{$\circ$}{}  
\item Matrices 
\item Polinomios 
\item Funciones
\end{list}
\pagebreak

Las operaciones satisfacen las mismas propiedades que las operaciones en $\mathbb{R}^n$ 
\begin{list}{$\circ$}{}  
\item asociatividad 
\item conmutatividad
\item distributividad
\item neutro y opuesto
\end{list}
Entonces estudiaremos todas estas estructuras en abstracto, sin distinguir si son vectores, matrices, polinomios, funciones o lo que fuere.\\\\
Lo importante son las operaciones y las propiedades que satisfacen. 


\begin{defi}
  Un espacio vectorial (sobre $\mathbb{K}$) o un $\mathbb{K}$-espacio vectorial es un conjunto $V$ que tiene dos operaciones que satisfacen ciertos axiomas. Llamaremos a los elementos de $V$ vectores.
\end{defi}
\textbf{Operaciones}
\begin{list}{$\circ$}{}  
\item Suma de vectores: Dados $v,w \in V$ podemos formar el vector $v+w \in V$.
\item Producto por escalres: Dado $v \in V$ y $\lambda \in \mathbb{K}$ podemos formar el vector $\lambda \cdot v \in V$.
\end{list}
\textbf{Axiomas}
\begin{list}{$\circ$}{}  
\item $+$ es conmutativa, asociativa, existe neutro y opuesto 
\item $\cdot $ es asociativa, distributiva y tiene neutro.
\end{list}
Sean $u,v,w \in V$ y $\lambda,\mu \in \mathbb{K}$. Los axiomas son \\\\
$\begin{array}{lll} 
  \text{S1.} & v+w=w+v & (+ \text{ conmutativa}) \\\\
    \text{S2.} & (v+w)+u=v+(w+u) & (+ \text{ asociativa}) \\\\
    \text{S3.} & \exists ! \text{ vector } , \text{ tal que } 0 + v = v & (\text{neutro de la } +) \\\\
  \text{S4.} & \exists  ! -v \text{ tal que }v +(-v)=0 & \text{(opuesto)} \\\\
  \text{P1.} & 1 \cdot v = v \text{ para todo } v \in V & (\text{neutro de }\cdot) \\\\
  \text{P2.} & \lambda \cdot (\mu \cdot v) = (\lambda \mu ) \cdot v & (\cdot \text{ asociativo}) \\\\
  \text{D1.} & \lambda \cdot (v+w)=\lambda \cdot v + \lambda \cdot w & \text{(propiedad distributiva } 1) \\\\
  \text{D2.} & (\lambda + \mu ) \cdot v = \lambda \cdot v + \mu \cdot v & (\text{propiedad distributiva } 2)
\end{array}$
\pagebreak

\textbf{Convenciones}
\begin{list}{$\circ$}{}  
\item $\lambda v = \lambda \cdot v$ 
\item $-v$ se llama el opuesto de $v$
\item Gracias a la asociatividad de $+$ y $\cdot $ podemos obviar los parentesis
\item $w-v=w+(-v)$, en palabras ``$w$ menos $v$'' significa ``$w$ mas el opuesto de $v$''
\end{list}
\begin{ej}
El conjunto de los numeros reales es un $\mathbb{R}$-espacio vectorial con la suma y la multiplicacion \\\\
Como dijimos antes son los mismos axiomas. \\\\
Mas aun $\mathbb{C}$ tiene los mismos axiomas y podemos multiplicar reales por complejos. Entonces 
\end{ej}
\begin{ej}
  El conjunto de los numeros complejos es un $\mathbb{R}$-espacio vectorial con la suma y la multiplicacion.
\end{ej}
\begin{ej}
  El conjunto de matrices $\mathbb{K}^{m \times n}$ es un espacio vectorial con las operaciones que definimos anteriormente.\\\\
  Si $A,B \in \mathbb{K}^{m \times n}$ y $\lambda \in \mathbb{K}$ entonces 
\begin{list}{$\circ$}{}  
\item $A+B$ es la matriz con entradas $\begin{bmatrix}A+B\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ij}+\begin{bmatrix}B\end{bmatrix}_{ij}$ 
\item $\lambda \cdot A$ es la matriz con entradas $\begin{bmatrix}\lambda A \end{bmatrix}_{ij}=\lambda \begin{bmatrix} A \end{bmatrix}_{ij}$
\end{list}
Ya hemos visto que estas operaciones satisfacen los axiomas de la definicion. En particular 
\begin{list}{$\circ$}{}  
\item  El elemento neutro $0$ es la matriz con todas las coordenadas iguales a cero, 
\item El opuesto de $A$ es la matriz $(-1)\cdot A$
\end{list}
\end{ej}
\begin{ej}
  El conjunto de vectores filas $\mathbb{K}^{1 \times n}$ (o columnas $\mathbb{K}^{n \times 1})$ es un espacio vectorial con las operaciones que hemos definido anteriormente en estos apuntes. 
\begin{list}{$\circ$}{}  
\item  La suma coordenada a coordenada 
\item La multiplicacion coordenada a coordenada
\end{list}
Es un caso particular de las matrices.
\end{ej}
\pagebreak

\begin{ej}
  El conjunto de polinomios sobre $\mathbb{K}$ \[
  \mathbb{K}[x]=\{a_nx^n+ \cdots + a_1 x + a_0 \; | \; n \in \mathbb{N},\; a_{n}, \dots ,a_{0}\in \mathbb{R}\}
  \]
   con la suma y multiplicacion que ya conocemos: 
   \\\\
\begin{list}{$\circ$}{}  
\item  Suma coeficiente a coeficiente \[
    \begin{aligned}
    &(a_nx^n+\cdots +a_1x+a_0)+(b_nx^n+\cdots + b_1x+b_0)=
    \\
    = &(a_n+b_n)x^n+\cdots + (a_1+b_1)x+(a_0+b_0)
  \end{aligned}
  \]
\item Multiplicacion coeficiente a coeficiente \[
\lambda \cdot (a_nx^n+\cdots + a_1 x + a_0) = (\lambda a_n)x^n+\cdots + (\lambda a_1 ) x + (\lambda a_0)
  \]
\item El neutro es el polinomio $0$.
\item El opuesto del polinomio $a_nx^n+\cdots + a_1 x + a_0$ es el polinomio $$(-a_n)x^n+\cdots + (-a_1) x + (-a_0)$$
\end{list}
\end{ej}
\begin{obs}\;
\begin{list}{$\circ$}{}  
\item  Si $x^i$ no aparece en la expresion de un polinomio quiere decir que respectivo coeficiente $a_i$ es cero. Por ejemplo: \[
x^2+1=x^2+0x+1
  \]
\item Para sumar polinomios no es necesario que tengan el mismo grado.\\ Por ejemplo: \[
    (x^2+1)+(x^2+2x^2+5x+2)=x^2+3x^2+5x+3
  \]
\end{list}
\end{obs}
\pagebreak

\begin{ej}
  Sea $X$ un conjunto. El espacio vectorial de funciones de $X$ a $\mathbb{R}$ es el conjunto \[
    F(\mathbb{R})=\mathbb{R}^X=\{\text{las funciones } f : X \longrightarrow \mathbb{R}\}
  \]
  con la suma y producto por escalar ``punto a punto''.\\\\
  Es decir, si $f,g \in \mathbb{R}^X$ y $\lambda \in \mathbb{R}$, 
\begin{list}{$\circ$}{}  
\item $f+g:X \longrightarrow \mathbb{R}$ es la funcion definida por \[
    (f+g)(x)=f(x)+g(x)
  \]
\item $\lambda \cdot f : X \longrightarrow \mathbb{R}$ es la funcion definida por \[
    (\lambda \cdot f)(x)=\lambda f(x)
  \]
\end{list}
Si $f,g : X \longrightarrow \mathbb{R}$ y $\lambda \in \mathbb{R}$ entonces 
\begin{list}{$\circ$}{}  
\item el opuesto de $f$ es $-f: X \longrightarrow \mathbb{R}$, la funcion definida por \[
    (-f)(x)=-f(x)
  \]
\item el elemento neutro es la funcion constante igual a cero, es decir $f(x)=0$ para todo $ x \in X$, la cual denotamos $0$.
\end{list}
\end{ej}
\begin{obs}
  Si $X=\mathbb{R}$ entonces la suma y el producto por escalar es la misma definicion que se usa en Analisis Matematico I. \\\\ En este caso se suele denotar $F(\mathbb{R})=\mathbb{R}^\mathbb{R}$
\end{obs}
\begin{ej}
  El conjunto de los numeros reales positivos $\mathbb{R}_{>0}=(0,\infty)$ es un espacio vectorial con las siguientes operaciones: 
\begin{list}{$\circ$}{}  
\item $x \oplus y=x\cdot y$ (la ``suma'' es la multiplicacion) 
\item $\lambda \odot x = x^\lambda$ (la ``multiplicacion'' es la potenciacion)
\item El ``neutro'' es el $1$: $1 \oplus x  =1 \cdot x = x $ \item El ``opuesto'' es el inverso: $x^{-1} \oplus x = x^{-1}\cdot x = 1 $ 
\end{list}
\end{ej}
 \begin{obs}\;
   \begin{list}{$\circ$}{}  
   \item Definicion de $x^{\lambda} := e^{\lambda \; ln(x)}$
   \item Probemos \textbf{D2}: \[
       (\lambda + \mu ) \odot x = x^{\lambda + \mu } = x^\lambda  x^{\mu} = x^\lambda \oplus x^\mu = \lambda \odot x \oplus \mu \odot x .
     \]
   \end{list}
 \end{obs}
 \pagebreak

\begin{prop}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$. Entonces 
  \begin{enumerate}[label=\arabic*.] 
  \item $\lambda \cdot 0=0$ para todo $ \lambda \in \mathbb{K}$ 
  \item $0 \cdot v = 0$ para todo $v \in \mathbb{V}$ 
  \item Si $\lambda \cdot v = 0$  entonces $\lambda = 0$ ó $v=0$
  \item $(-1)\cdot v=-v$, en palabras, $-1$ por $v$ es igual al opuesto de $v$
\end{enumerate}
\end{prop}
\begin{obs}
  La demostracion es identica a las propiedades analogas de los numeros reales o los numeros enteros dado que lo unico que usamos son los axiomas.
\end{obs}
\begin{demo}[1.]\;
\begin{list}{$\circ$}{}  
\item $\lambda \cdot 0 = 0 $ para todo $ \lambda \in \mathbb{R}$ 
\begin{align}
 \lambda \cdot 0 &= \lambda \cdot (0 + 0) \tag{axioma elemento neutro} \\
 \lambda \cdot 0 &= \lambda \cdot 0 + \lambda \cdot 0 \tag{axioma distributividad} \\
 \Rightarrow 0 &= \lambda \cdot 0 \tag{sumando el opuesto de $\lambda \cdot 0$}
 \notag
 \end{align}

\end{list}\end{demo}
\begin{demo}[2.]\;
\begin{list}{$\circ$}{}  
\item  $0 \cdot v = 0$ para todo $v \in V$
\end{list}
es similar a la anterior.
\end{demo}
\begin{demo}[3.]\;
\begin{list}{$\circ$}{}  
\item  Si $\lambda \cdot v= 0$ entonces $\lambda = 0$ ó $v =0$
\end{list}
Si $\lambda=0$ no hay nada que demostrar. \\\\
Supongamos que $\lambda \neq 0$. Sea $\lambda ^{-1} \in \mathbb{R}$ su inverso multiplicativo. 
\begin{align}
  \lambda^{-1}\cdot 0 &= \lambda^{-1}\cdot (\lambda \cdot v)\tag{por hipotesis} \\
  0&= (\lambda^{-1}\lambda)\cdot v \tag{asociatividad} \\
  0 &= 1 \cdot v  \notag \\
  0&=v \tag{axioma neutro}
 \notag
 \end{align}
\end{demo}
\begin{demo}[4.] \;
\begin{list}{$\circ$}{}  
\item $(-1)\cdot v=-v$, en palabras, $-1$ por $v$ es igual al opuesto de $v$ 
\begin{align}
  0&=0\cdot v \tag{por 2.} \\
  0&=(1+(-1))\cdot v \notag \\
  0&=1 \cdot v + (-1) \cdot v \tag{distributividad} \\
  0&=v+(-1)\cdot v \tag{elemento neutro $\cdot$} \\
  -v+0 &= -v+v+(-1)\cdot v \notag \\
  -v &= 0 +(-1)\cdot v \tag{elemento neutro $+$ y opuesto} \\
  -v&=(-1)\cdot v \tag{elemento neutro $+$} 
 \notag
 \end{align}
 \qed
\end{list}
\end{demo}
\begin{center}
\textbf{Subespacios vectoriales.}
\end{center}
\begin{defi}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$. Diremos que $W \subset V$ es un subespacio de $V$ si $W \neq \emptyset$ y \begin{enumerate}[label=(\alph*)]
    \item si para cualesquiera $w_1, w_2 \in W$, se cumple que $w_1+w_2 \in W$ y 
    \item si $\lambda \in \mathbb{K}$ y $w \in W$, entonces $\lambda w \in W$.

  \end{enumerate}
\end{defi}
\begin{obs}\;
  Si $W$ subespacio de $V$. 
\begin{list}{$\circ$}{}  
\item $0 \in W$. 
\item Si $w \in W$, entonces $-w \in W$.
\end{list}
\end{obs}
\begin{demo}[$0 \in W$] \;\\\\
  Como $W\neq \emptyset$, existe $w \in W$. Por la condicion (b), $0 \cdot w \in W$. Ahora bien, hemos visto que $0 \cdot w = 0$, por lo tanto $0 \in W$.\\
\end{demo}
\begin{demo}[$-w \in W$]\;\\\\
Por la condicion (b), $(-1)\cdot w \in W$. Ahora bien, hemos visto que $(-1) \cdot w = -w$, por lo tanto $-w \in W$.
\end{demo}\;
\begin{obs}
  Sea $w \subset V,W \neq \emptyset$. Entonces \[
    W \text{ subespacio de } V \quad \Leftrightarrow \quad u+\lambda w \in W, \; \; \forall u,w \in W, \lambda \in \mathbb{K}.
  \]
  \end{obs}\begin{demo}[$\Rightarrow$] \;
  \begin{enumerate}[label=\phantom{(}$\circ$\phantom{)}]  
\item  Por (b) de la definicion, $\lambda w \in W$. 
\item Como $u \in W$ y $\lambda w \in W$, por $(a)$ de la definicion $u+\lambda w \in W$. 
\end{enumerate}
\end{demo}
\begin{demo}[$\Leftarrow$]\;
  \begin{enumerate}[label=(\alph*)]
    \item Sean $w_1+w_2 \in W$, luego $w_1+1\cdot w_2 = w_1 + w_2 \in W$. 
    \item Sea $\lambda \in \mathbb{K}$ y $w \in W$, entonces $0 + \lambda w = \lambda w \in W$.
  \end{enumerate}
\end{demo}
\qed
\pagebreak

\begin{teo}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y $W$ subespacio de $V$. Entonces $W$ con las operaciones suma y producto por escalares de $V$ es un espacio vectorial.
\end{teo}
\begin{demo}
  Para que $W$ sea espacio vectorial sus operaciones deben satisfacer los axiomas de la definicion de espacio vectorial. \[
    0 \in W \text{ y si } w \in W \Rightarrow -w \in W.
  \]
  Teniendo en cuenta estos dos hechos, y que las operaciones en $V$ satisfacen los axiomas de la definicion (y por lo tanto en $W$ tambien), queda demostrado que $W$, con las operaciones heredadas de $V$, es espacio vectorial. \\ \qed
  \end{demo}\begin{ej}[Ejemplos de subespacios vectoriales]\;
\begin{enumerate}[label=\arabic*.]
  \item Sea $V$ un $\mathbb{K}$-espacio vectorial, entonces $\{0\}:=0$ y $V$ son subespacios vectoriales de $V$. Suelen ser llamados los subespacios triviales de $V$.
  \item Sea $V$ un $ \mathbb{K}$-espacio vectorial y sea $v \in V$, entonces \[
      W=\{\mu v : \mu \in \mathbb{K}\}
    \]
    es un subespacio vectorial.\\\\ En efecto, si $\mu_1 v , \mu_2v \in W$, con $\mu _1 , \mu_2 \in \mathbb{K}$, entonces \[
\mu_1 v + \lambda \mu_2v =(\mu_1+\lambda\mu_2)v\in W,
    \]
    para todo $\lambda \in \mathbb{K}$. \\\\ El subespacio $W$ suele ser denotado $\mathbb{K}v$.
  \item Sea $A \in M_{m \times n}(\mathbb{K})$. Si $x = (x_1, \dots ,x_n) \in \mathbb{K}^n$, entonces $Ax$ denota \[
      Ax:=A\begin{bmatrix} x_1 \\ \vdots \\x_n \end{bmatrix}.
    \]
    Sea \[
      W=\{x \in \mathbb{K}^{n} : Ax=0\}.
    \]
    Es decir, $W$ es el subconjunto de las soluciones del sistema $Ax=0$. \\\\ Entonces, $W$ es un subespacio de $\mathbb{K}^n$: sean $x,y \in W$ y $\lambda \in \mathbb{K}$, es decir $Ax=0$, $Ay=0$ y $\lambda \in \mathbb{K}$, entonces \[
A(x+\lambda y)=Ax+A(\lambda y)=Ax+\lambda Ay=0+\lambda \cdot 0 = 0.
    \]
   \end{enumerate}
 Es decir \\\\
  El conjunto de soluciones de un sistema de ecuaciones homogeneo es un subespacio vectorial de $\mathbb{K}^{m},$
\\\\
En particular, 
\begin{list}{$\circ$}{}  
\item Las rectas en el plano que pasan por el origen son subespacios de $\mathbb{R}^2$. 
\item Los planos en el espacio que pasan por el origen son subespacios de $\mathbb{R}^3$.
\end{list}
\begin{enumerate}[label=\arabic*. , start=4]
  \item Sean $V=\mathbb{K}^{n}$ y $1 \leq j \leq n$. Definimos \[
      W=\{(x_1, x_2, \dots ,x_n) : x_i \in \mathbb{K} \; ( 1 \leq i \leq n), x_j=0\}.
    \]
    Es decir, $W$ es el subconjunto de $V$ de todas las $n$-tuplas con la coordenada $j$ igual a $0$. Por ejemplo si $j=1$ \[
      W=\{(0,x_2,\dots ,x_n) : x_i \in \mathbb{K}  \; (2 \leq i \leq n)\}.
    \]
    Veamos que este ultimo es un subespacio. \\\\
    Si $(0,x_2,\dots ,x_n),(0,y_2,\dots, y_n) \in W$ y $\lambda \in \mathbb{K}$, entonces \[
      (0,x_2,\dots,x_n)+\lambda(0,y_2,\dots,y_n)=(0,x_2+\lambda y_2,\dots,x_n+\lambda y_n) \in W.
    \]
    La demostracion para $j>1$ es completamente analoga.
  \item Sea $Sim_n(\mathbb{K})=\{A \in M_{n \times n}(\mathbb{K}) : A^t=A\}$. \\\\ Es claro que : $A \in Sim_n(\mathbb{K}) \Leftrightarrow \begin{bmatrix}A\end{bmatrix}_{ij}=\begin{bmatrix}A\end{bmatrix}_{ji} \forall i,j$.
    \begin{prop}
      $A \in Sim_{n}(\mathbb{K})$ es subespacio de $M_n(\mathbb{K})$
    \end{prop}
    \begin{demo}
      Sean $A = \begin{bmatrix}a_{ij}\end{bmatrix}$, $B=\begin{bmatrix}b_{ij}\end{bmatrix}$ tales que $A=A^t$ y $B=B^t$ y sea $\lambda \in \mathbb{K}$, entonces debemos verificar que: $A+\lambda B \in Sim_{n}(\mathbb{K})$. 
      \begin{align}
        \begin{bmatrix}(A+\lambda B)^t \end{bmatrix}_{ij}&= \begin{bmatrix}(A+\lambda B)\end{bmatrix}_{ji} && \text{(definicion de transpuesta)} \notag
                      \\                                 &= \begin{bmatrix}A\end{bmatrix}_{ji}+\lambda \begin{bmatrix}B\end{bmatrix}_{ji} &&\text{(def. de suma y prod. por escalar)} \notag \\
                                                         &=\begin{bmatrix}A\end{bmatrix}_{ij}+\lambda\begin{bmatrix}B\end{bmatrix}_{ij} &&\text{($A$ y $B$ simetricas)} \notag \\
                                                         &= \begin{bmatrix}A+\lambda B \end{bmatrix}_{ij} && \text{(def. de suma y prod. por escalar)} \notag
      \end{align}
    
      Luego $A+\lambda B \in Sim_{n}(\mathbb{K})$. \\ \qed
    \end{demo}
  \item El conjunto $\mathbb{R}[x]=\{P(x):P(x) \text{ es polinomio en }\mathbb{R}\}$, es subespacio de $F(\mathbb{R})$, pues $\mathbb{R}[x] \subset F(\mathbb{R})$ y las operaciones de suma y producto por un escalar son cerradas en $\mathbb{R}[x]$.
  \item Sea $C(\mathbb{R})$ las funciones continuas de $\mathbb{R}$ en $\mathbb{R}$. Entonces, $C(\mathbb{R})$ es subespacio de $F(\mathbb{R})$. \begin{demo}
      Sean $f,g$ funciones continuas, es decir $\lim_{x \to a}f(x)=f(a)$ y $\lim_{x \to a}g(x)=g(a), \forall a \in \mathbb{R}$. Sea $\lambda \in \mathbb{R}$. Por las propiedades de los limites \[
        \lim_{x \to a}(f+\lambda g)(x)=\lim_{x \to a } f(x)+ \lambda \lim_{ x \to a }g(x) = f(a)+\lambda g(a)=(f+\lambda g)(a)
      \] \qed
  \end{demo}
  De forma analoga, el conjunto $\mathbb{R}[x]$ es subespacio de $C(\mathbb{R})$.
\end{enumerate}
\begin{center}
\textbf{Combinaciones lineales.}
\end{center}
\begin{defi}
  Sea $V$ espacio vectorial sobre $\mathbb{K}$ y $v_1, \dots,v_n$ vectores en $V$. Dado $v \in V$, diremos que $v$ es combinacion lineal de los $v_1, \dots, v_n$ si existen escalares $\lambda _1 , \dots ,\lambda_n$ en $\mathbb{K}$, tal que \[
v=\lambda_1 v_1 + \cdots + \lambda_n v_n.
  \]
\end{defi}

\begin{ej}
  Sean $v_1 = (1,0)$, $v_2=(0,1)$ en $\mathbb{C}^2$ ¿es $v=(i,2)$ combinacion lineal de $v_1,v_2$? La respuesta es si, pues \[
v=iv_1+2v_2.
  \]
  Observar ademas que es la unica combinacion lineal posible, pues si \[
v=\lambda_1 v_1 + \lambda_2 v_2 , 
  \]
  entonces \[
    (i,2)=(\lambda_1, 0)+(0, \lambda_2)=(\lambda_1, \lambda_2),
  \]luego $\lambda_1=i$ y $\lambda_2=2$.
\end{ej}

\begin{ej}
 Puede ocurrir que un vector sea combinacion lineal de otros vectores de varias formas diferentes. Por ejemplo, si $v=(i,2)$ y $v_1=(1,0)$, $v_2=(0,1)$, $v_3=(1,1)$, tenemos que 
 \begin{align}
   v &= iv_1 + 2v_2 + 0 v_3 , \quad\quad\quad \text{y tambien}  \notag \\
   v &= (i-1)v_1 + v_2 + v_3 . \notag
 \end{align}
\end{ej}

\begin{ej}
  Sean $(0,1,0)$, $(0,1,1)$ en $\mathbb{C}^3$ ¿es $(1,1,0)$ combinacion lineal de $(0,1,0), (0,1,1)$? La respuesta es no, pues si \[
    (1,1,0)=\lambda_1 (0,1,0)+\lambda_2 (0,1,1)=(0,\lambda_1,0)+(0 , \lambda_2 , \lambda_2) =(0,\lambda_1 + \lambda_2 , \lambda_2 ), 
  \]
  luego, la primera coordenada nos dice que $1=0$, lo cual es absurdo.
\end{ej}
\end{ej}
\pagebreak

\begin{obs}[muy importante] \; \\\\
  ¿Es $v=(b_1, \dots, b_m) \in \mathbb{K}^{m}$ combinacion lineal de vectores $v_1, \dots ,v_n \in \mathbb{K}^m$? \\\\
  Sea $v_i=(a_{1i}, \dots ,a_{mi})$ $(1 \leq i \leq n )$ entonces, $v=\lambda _1 v_1 + \cdots + \lambda _n v_n \Rightarrow$ \[
    \begin{aligned}
    (b_1, \dots ,b_m) &= \lambda_1 (a_{11}, \dots ,a_{m1} )+ \cdots + \lambda_n(a_{1n}, \dots ,a_{mn}) \\
                      &= (\lambda_1 a_{11} + \cdots + \lambda_n a_{1n},\;\dots\; , \lambda_1 a_{m1}+\cdots + \lambda_n a_{mn}).
  \end{aligned}
  \]
  Luego, 
  \begin{tcolorbox}[
    ams align,
    colback=azulp2!20!white,
    colframe=white
    ]
    v \text{ es combinacion lineal de los vectores } v_1, \dots ,v_n \in \mathbb{K}^m \notag
  \end{tcolorbox}
  si y solo si tiene solucion el siguiente sistema de ecuaciones: 
  \begin{tcolorbox}[
    ams align,
    colback=azulp2!20!white,
    colframe=white
    ]
    \begin{matrix}
      a_{11}\lambda_{1} & + & a_{12}\lambda_{2} & + &\cdots & + & a_{1n}\lambda_{n} & = & b_1 \\
      \vdots & & \vdots & & & & \vdots \\
      a_{m1} \lambda_{1} & + & a_{m2} \lambda_{2} & + & \cdots & + & a_{mn} \lambda_{n} & = & b_{m}.
    \end{matrix} \notag
  \end{tcolorbox}
\end{obs}
\begin{ej}
  Demostrar que $(5,12,5)$ es combinacion lineal de los vectores $(1,-5,2),(0,1,-1),(1,2,-1)$. 
\end{ej}
\textbf{Solucion}\\\\
Planteamos la ecuacion: \[
  \begin{aligned}
    (5,12,5) &= \lambda_{1}(1,-5,2)+\lambda_{2}(0,1,-1)+\lambda_{3}(1,2,-1) \\
             &= (\lambda_{1},-5\lambda_{1},2 \lambda_{1})+(0,\lambda_2,-\lambda_2)+(\lambda_{3},2\lambda_3,-\lambda_3) \\
             &= (\lambda_1 + \lambda_3 , -5\lambda_1+\lambda_2+2\lambda_3,2\lambda_1 - \lambda_2 -  \lambda_3).
  \end{aligned}
\]
  Por consiguiente, esta ecuacion se resuelve con el siguiente sistema de ecuaciones \[
    \begin{aligned}
      \lambda_1 +\lambda_3 &= 5 \\
      -5\lambda_1 + \lambda_2 + 2\lambda_3 &= 12 \\
      2 \lambda_1 - \lambda_2 - \lambda_3 &= 5.
    \end{aligned}
  \]
  \pagebreak

  Ahora bien, usando el metodo de Gauss \[
    \begin{aligned}
  &\begin{bmatrix}[rrr|r]
    1 & 0 & 1 & 5 \\
    -5 & 1 & 2 & 12 \\
    2 & -1 & -1 & 5 
  \end{bmatrix}\underset{F_3-2F_1}{\xrightarrow{F_2+5F_1}} \begin{bmatrix}[rrr|r]
    1 & 0 & 1 & 5 \\
    0 & 1 & 7 & 37 \\
    0 & -1 & -3 & -5 
    \end{bmatrix}\xrightarrow{F_3+F_2} \begin{bmatrix}[rrr|r]
    1 & 0 & 1 & 5 \\
    0 & 1 & 7 & 37 \\
  0 & 0 & 4 & 32 \end{bmatrix} \\ 
      \xrightarrow{F_3 / 4 } & \begin{bmatrix}[rrr|r] 
        1 & 0 & 1 & 5 \\ 0 & 1 & 7 & 37 \\ 
        0& 0 & 1 & 8 \end{bmatrix} \underset{F_2-7F_3}{\xrightarrow{F_1-F_3}} \begin{bmatrix}[rrr|r]1 & 0 & 0 & -3 \\ 0 & 1 & 0 & -19 \\ 0 & 0 & 1 & 8  \end{bmatrix}.
\end{aligned}
\]
Luego, $\lambda_1=-3$, $\lambda_2=19$ y $\lambda_3=8$, es decir \[
  (5,12,5)=-3(1,-5,2)-19(0,1,-1)+8(1,2,-1).
\]
\qed
\begin{prop}
  Sea $W$ subespacio de $V$ y $w_1, \dots ,w_k \in W$, entonces cualquier combinacion lineal de los $w_1, \dots ,w_k$ pertenece a $W$.
\end{prop}
\begin{demo}
  Debemos probar que, para cualesquiera $\lambda_1 , \dots ,\lambda_k \in \mathbb{K}$, se cumple que $\lambda_1 w_1 + \cdots + \lambda_k w_k \in W$. \\\\ Ahora bien, como $W$ es subespacio, $\lambda_i w_i \in W$ para $1 \leq i \leq k$. \\\\ Por un argumento inductivo, como $W$ es subespacio, no es dificil probar que la suma de $k$ terminos en $W$ es un elemento de $W$, por lo tanto \\$\lambda_1 w_1 + \cdots + \lambda_k w_k \in W$. \\  \qed
\end{demo}
\begin{teo}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $v_1, \dots ,v_k \in V$. Entonces \[
    W=\{\lambda_1  v_1 + \cdots + \lambda_k v_k : \lambda_1, \dots ,\lambda_k \in \mathbb{K}\}
  \]
  es un subespacio vectorial. Es decir, el conjunto de las combinaciones lineales de $v_1, \dots ,v_k$ es un subespacio vectorial.
\end{teo}
\begin{demo} 
  Sean $\lambda_1 v_1 + \cdots + \lambda_k v_k, \; \mu_1 v_1 + \cdots + \mu_k v_k$ dos combinaciones lineales de $v_1, \dots ,v_k$ y $\lambda\in \mathbb{K}$, entonces 
  \[\begin{aligned}
&(\lambda_1 v_1 + \cdots + \lambda_k v_k) + \lambda(\mu_1 v_1 + \cdots + \mu_k v_k) \\ 
&= \lambda_1 v_1 + \lambda\mu_1 v_1 + \cdots + \lambda_k v_k + \lambda \mu_k v_k \\
&= (\lambda_1 + \lambda\mu_1 ) v_1 + \cdots + (\lambda_k+\lambda\mu_k)v_k,
\end{aligned}\]
que es una combinacion lineal de $v_1,\dots,v_k$ y por lo tanto pertence a $W$.
\end{demo} \pagebreak

\begin{defi}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $v_1, \dots ,v_k \in V$. Al subespacio vectorial $W=\{\lambda_1 v_1 + \cdots + \lambda_k v_k : \lambda_1, \dots ,\lambda_k \in \mathbb{K}\}$ de las combinaciones lineales de $v_1, \dots ,v_k$ se lo denomina subespacio generado por $v_1, \dots ,v_k$ y se lo denota \[
    W=\langle v_1, \dots ,v_k \rangle =gen\{v_1, \dots , v_k\}=span \{v_1, \dots ,v_k\}.
  \]
  Ademas, en este caso, diremos que el conjunto $S=\{v_1, \dots ,v_k\}$ general al subespacio $W$ o que los vectores $v_1,\dots, v_k$ generan $W$.
\end{defi}
\begin{obs}
  Un caso especial, que sera de suma importancia, es el caso en que consideramos todo $V$. \\\\
  Estudiaremos mas adelante conjuntos de generadores de $V$ llamados bases.
\end{obs}
\begin{center}
  \textbf{Determinacion ``implicita'' de un subespacio de $\mathbb{K}^n$.}
\end{center}
Nos interesa tener una manera de decidir rapidamente si un vector esta en el subespacio generado o no. \\\\
Una forma de lograr esto es tener el subespacio descripto por escuaciones que solo tienen como solucion a los vectores pertenecientes a este.\\\\
Ejemplificaremos con los siguientes vectores en $\mathbb{R}^4$: \[
  \begin{aligned}
    v_1&= (3,1,2,-1), && v_2=(6,2,4,-2), \\
    v_3&=(3,0,1,1), && v_4=(15,3,8,-1)
  \end{aligned}
\]
\textbf{Problema.} Caracterizar mediante ecuaciones el subespacio $\langle v_1,v_2,v_3,v_4 \rangle$. \\\\ 
En otras palabras, queremos describir implicitamente el conjunto de los \\$b=(b_1,b_2,b_3,b_4) \in \mathbb{R}^4$ tales que $b \in \langle v_1, v_2 ,v_3 ,v_4 \rangle$. \\\\ 
O sea, los $b=(b_1,b_2,b_3,b_4) \in \mathbb{R}^4$ tales que \[
  b=\lambda_1 v_1 + \lambda_2 v_2 + \lambda_3 v_3 + \lambda_4 v_4 \tag{*}
\]
con $\lambda_1, \lambda_2, \lambda_3, \lambda_4 \in \mathbb{R}$. \pagebreak

Planteamos la formula $(*)$ en coordenadas, pero es conveniente hacerlo con vectores columna: \[
  \lambda_1 \begin{bmatrix}[r]3 \\ 1  \\ 2 \\ -1 \end{bmatrix} + \lambda_2 \begin{bmatrix}[r]6 \\ 2 \\ 4\\ -2 \end{bmatrix} + \lambda_3 \begin{bmatrix}3 \\ 0 \\ 1 \\ 1 \end{bmatrix} + \lambda_4  \begin{bmatrix}[r]15 \\ 3 \\ 8 \\ -1 \end{bmatrix}=\begin{bmatrix}b_1 \\ b_2 \\b_3 \\b_4 \end{bmatrix}
\]
Luego \[
  \begin{bmatrix}[r]3\lambda_1+6\lambda_2+3\lambda_3 + 15\lambda_4 \\
  \lambda_1+2\lambda_2 + 3\lambda_4 \\
  2 \lambda_1 + 4\lambda_2 + \lambda_3 + 8\lambda_4 \\
  -\lambda_1-2\lambda_2+\lambda_3 -\lambda_4 
  \end{bmatrix}=\begin{bmatrix}b_1 \\ b_2 \\ b_3 \\ b_4 \end{bmatrix}
\]
En forma de producto de matrices podemos reescribirla asi: \[
  \begin{bmatrix}[rrlr]3 & 7 & 3 & 15 \\ 1 & 2 & 0 & 3 \\ 2 & 4 & 1 & 8 \\ -1 & -2 & 1 & -1 \end{bmatrix}\begin{bmatrix} \lambda_1 \\ \lambda_2 \\ \lambda_3 \\ \lambda_4 \end{bmatrix}=\begin{bmatrix} b_1 \\ b_2 \\b_3 \\b_4 \end{bmatrix}
\]
En forma de sistema de ecuaciones esto es: \[
  \begin{dcases} 
    3\lambda_1 + 6\lambda_2 + 3\lambda_3 + 15\lambda_4 = b_1 \\
    \lambda_1 + 2\lambda_2 + 3\lambda_4 = b_2 \\
    2 \lambda_1 + 4\lambda_2 + \lambda_3 + 8\lambda_4 = b_3 \\
    -\lambda_1-2\lambda_2 + \lambda_3 - \lambda_4 = b_4
  \end{dcases}\tag{**}
\]
\textbf{Conclusion.} $b \in \langle v_1, v_2 ,v_3 ,v_4 \rangle $ si y solo si el sistema anterior tiene solucion.
\\\\
El sistema $(**)$ tiene solucion si el siguiente sistema la tiene (es cambio de notacion solamente)
\[
  \begin{dcases} 
    3x + 6y + 3z + 15w = b_1 \\
    x + 2y + 3w = b_2 \\
    2 x + 4y + z + 8w = b_3 \\
    -x-2y + z - w = b_4
  \end{dcases}
\]
Este es exactamente el ejercicio $2$ de la Tarea $2$. Entonces la respuesta a nuestro problema es \\\\
\textbf{Respuesta}\[
  \langle v_1,v_2,v_3,v_4 \rangle = \{(b_1,b_2,b_3,b_4)\in\mathbb{R}^4 \;|\; b_1 + 3 b_2 - 3b_3 = 0, \; b_1 - 6 b_2 - 3 b_4 = 0\}.
\]\pagebreak

Notemos que podemos repetir todo el razonamiento anterior para cualesquiera vectores $v_1, \dots ,v_k$ en cualquier $\mathbb{R}^n$ y cualquier $b \in \mathbb{R}^n$. \\\\ Solo hay que tener presente que multiplicar una matriz por un vector columna es lo mismo que hacer una combinacion lineal de las columnas de la matriz: \\\\ Es decir, si \[
  A=\begin{bmatrix} | & | & & | \\ v_1 & v_2 & \cdots & v_k \\ | & | && | \end{bmatrix},
\]
entonces \[
  A\begin{bmatrix}\lambda_1 \\ \vdots \\ \lambda_k \end{bmatrix} = \lambda_1 v_1 + \cdots + \lambda_k v_k
\]
\textbf{Conclusion} Sean $v_1, \dots ,v_k \in \mathbb{K}^n$ y $A \in \mathbb{K}^{n \times k}$ la matriz cuyas columnas son los vectores $v_1, \dots ,v_k$ es decir \[
  A=\begin{bmatrix} | &| & & | \\ v_1 & v_2 & \cdots & v_k \\ | & | && | \end{bmatrix}.
\]
Entonces 
\begin{list}{$\circ$}{}  
\item El subespacio vectorial $\langle v_1, \dots,v_k \rangle$ es igual al conjunto de los $b \in \mathbb{K}^n$ para los cuales el sistema $AX=b$ tiene solucion. En particular, el ultimo punto nos dice cuando los vectores generan todo el espacio.
\item Las ecuaciones vienen dadas por las filas nulas de la $MERF$ equivalente a $A$. En particular, si no tiene filas nulas entonces $\langle v_1, \dots ,v_k \rangle = \mathbb{K}^n$ porque el sistema $AX=b$ siempre tiene solucion.
\end{list}
\pagebreak

\begin{center}
\textbf{Interseccion y suma de subespacios vectoriales.}
\end{center}
\begin{teo}
Sea $V$ un espacio vectorial sobre $\mathbb{K}$. Entonces la interseccion de subespacios vectoriales es un subespacio vectorial.
\end{teo}
\begin{demo}
  Veamos el caso de la interseccion de dos subespacios. \\\\ Debemos probar que si $W_1,W_2$ subespacios $\Rightarrow W_1 \cap W_2$ es subespacio. \\\\ 
  Observemos: $w \in W_1 \cap W_2 \Leftrightarrow w \in W_1 \land W \in W_2$. \[
    \begin{aligned}
      \text{Sea } \lambda \in \mathbb{K}.\; u,v \in W_1 \cap W_2 \quad &\Rightarrow\quad u,v \in W_1 \; \land \; u,v \in W_2 \\ 
                                                               &\Rightarrow\quad u+\lambda v \in W_1 \; \land \; u+\lambda v \in W_2 \\
                                                               &\Rightarrow\quad  u+\lambda v \in W_1 \cap W_2 .
    \end{aligned}
  \]
\end{demo}
Luego $W_1 \cap W_2$ es subespacio. \\ \qed
\begin{ej}
 Sean \[
   W_1=\{(x,y,z) : -3x+y+2z=0\}
 \]
 y
 \[
   W_2=\{(x,y,z):x-y+2z=0\}.\phantom{-1}
 \]
\end{ej}
Encontrar generadores de $W_1 \cap W_2$. \\\\
\textbf{Solucion} \\\\ Es claro que \[
  W_1\cap W_2 = \{(x,y,z): -3x+y+2z=0 \; \land \; x-y+2z=0\}.
\]
Por lo tanto debemos resolver el sistema de ecuaciones \[
  \begin{dcases}
    -3x+y+2z=0 \\
    x-y+2z=0 
  \end{dcases}
\]
Reduzcamos la matriz del sistema a una MRF: \[
  \begin{bmatrix}[rrr]-3&1 &2 \\ 1 & -1 & 2 \end{bmatrix} \xrightarrow{F_1+3F_2}\begin{bmatrix}[rrr]0 & -2 & 8 \\ 1 & -1 & 2 \end{bmatrix} \xrightarrow{F_1 / (-2)} \begin{bmatrix}[rrr]0 & 1 & -4 \\ 1 & -1 & 2 \end{bmatrix} \xrightarrow{F_2+F_1} \begin{bmatrix}[rrr]0 & 1 & -4 \\ 1 & 0 & -2 \end{bmatrix}
\]
por lo tanto, $x_2-4x_3=0$ y $x_1-2x_3=0$, es decir $x_2=4x_3$ y $x_1=2x_3$.
\\\\
Luego, \[
  W_1 \cap W_2 = \{(2t,4t,t) : t \in \mathbb{R} \} = \{t(2,4,1) : t \in \mathbb{R}\}. \]
La respuesta es entonces: $(2,4,1)$ es generador de $W_1 \cap W_2$. \\ \qed \pagebreak

\begin{obs}
  Si $V$ es un $\mathbb{K}$-espacio vectorial, $S$ y $T$ subespacios de $V$. \\\\ Entonces $S \cup T$ no es necesariamente un subespacio de $V$. \\\\ En efecto, consideremos en $\mathbb{R}^2$ los subespacios \[
    S=\mathbb{R}(1,0) \quad \text{ y } \quad T=\mathbb{R}(0,1).
  \]
\begin{list}{$\circ$}{}  
\item $(1,0) \in S$ y $(0,1) \in T \Rightarrow  (1,0),(0,1) \in S \cup T$.
\item Ahora bien $(1,0) + (0,1)=(1,1,) \notin S \cup T$, puesto que $(1,1) \in S$ y $(1,1) \notin T$.
\end{list}
\end{obs}
\begin{teo}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $v_1, \dots ,v_k \in V$. Entonces, la interseccion de todos los subespacios vectoriales que contienen a $v_1, \dots, v_k$ es igual a $\langle v_1, \dots ,v_k \rangle$.
\end{teo}
\begin{demo}
  Denotemos \begin{list}{$\circ$}{}  
  \item $U=\bigcap$ de todos los subespacios vectoriales $\supseteq \{v_1, \dots ,v_k\}$.
\end{list}
Probaremos que $U=\langle v_1, \dots,v_k \rangle $ con la doble inclusion, es decir probando que \[
  U \subseteq \langle v_1, \dots, v_k \rangle \quad \text{ y } \quad \langle v_1, \dots ,v_k \rangle \subseteq U.
\]
($U \subseteq \langle v_, \dots ,v_k \rangle)$ \\\\ 
Primero, $U \subseteq \langle v_1, \dots ,v_k \rangle $ vale puesto que $\langle v_1, \dots ,v_k \rangle $ es un subespacio que contiene a $\{v_1, \dots ,v_k\}$. \\\\ 
($\langle v_1, \dots ,v_k \rangle \subseteq U)$ \\\\
$U$ es interseccion de subespacios $\Rightarrow$ (teor. 44) $U$ es un subespacio. \\\\ Luego, $\{v_1, \dots ,v_k\} \subset U \Rightarrow \lambda_1 v_1 + \cdots + \lambda_k v_k \in U$, $\quad \forall \lambda_1, \dots ,\lambda_k \in \mathbb{K}$. \\ \qed
\end{demo} \pagebreak

\begin{defi}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $S_1, \dots ,S_k$ subconjuntos de $V$. Definimos \[
    S_1 + \cdots + S_k := \{s_1+\cdots + s_k : s_i \in S_i , \; 1 \leq i \leq k\}.
  \]
  el conjunto suma de los $S_1, \dots , S_k$.
\end{defi}
\begin{teo}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $W_1, \dots ,W_k$ subespacios de $V$. Entonces $W=W_1+\cdots + W_k$ es un subespacio de $V$.
\end{teo}\begin{demo}
Sean $v=v_1+\cdots + v_k$ y $w = w_1 + \cdots + w_k$ en $W$ y $\lambda \in \mathbb{K}$. Entonces \begin{enumerate}[label=(\alph*)] 
  \item $v+w=(v_1+w_1)+\cdots + (v_k+w_k) \in W_1+\cdots + W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $v_i+w_i \in W_i$. 
  \item $\lambda v = \lambda (v_1+\cdots + v_k) = \lambda v_1 + \cdots + \lambda v_k \in W_1+\cdots + W_k$, pues como $W_i$ es subespacio de $V$, tenemos que $\lambda v_i \in W_i$. 
\end{enumerate}
\qed
\end{demo}
\begin{prop}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y sean $v_1, \dots ,v_r$ elementos de $V$. Entonces \[
\langle v_1, \dots ,v_r \rangle = \langle v_1 \rangle + \cdots + \langle v_r \rangle .
  \]
\end{prop}
\begin{demo}
  Probemos el resultado viendo que los dos conjuntos se incluyen mutuamente. \\\\
  ($\subseteq$) Sea $w \in \langle v_1, \dots ,v_r \rangle$, luego $w=\lambda_1 v_1 + \cdots + \lambda_r v_r$. Como $\lambda_i v_i \in \langle v_i \rangle, \\ 1\leq i \leq r$, tenemos que $w \in \langle v_1 \rangle + \cdots + \langle v_r \rangle $. \\\\ En consecuencia, $\langle v_1, \dots, v_r \rangle  \subseteq \langle v_1 \rangle + \cdots + \langle v_r \rangle $. \\\\
  ($\supseteq$) Si $w \in \langle v_1 \rangle + \cdots + \langle v_r \rangle$, entonces $w=w_1 + \cdots + w_r$ con $w_i \in \langle v_i \rangle $ para todo $i$. Por lo tanto, $w_i = \lambda_i v_i$ para algun $\lambda i \in \mathbb{K}$ y $w=\lambda_1 v_1 + \cdots + \lambda_r v_r \in \langle v_1, \dots ,v_r \rangle.$ \\\\ En consecuencia, $\langle v_1 \rangle + \cdots + \langle v_r \rangle \subseteq \langle v_1, \dots ,v_r \rangle$. \\
\qed
\end{demo} \pagebreak

\begin{center}
\textbf{Dependencia lineal.}
\end{center}
\begin{defi}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$. Un subconjunto $S$ de $V$ se dice linealmente dependiente o simplemente, $LD$ o dependiente, si existen vectores distintos $v_1, \dots ,v_n \in S$ y escalares $\lambda_1, \dots ,\lambda_n$ de $\mathbb{K}$, no todos nulos, tales que \[
\lambda_1 v_1 + \cdots + \lambda_n v_n =0.
  \]
\end{defi}
\begin{obs}
 Si el conjunto $S$ tiene solo un numero finito de vectores $v_1, \dots, v_n$, se dice, a veces, que los $v_1, \dots, v_n$ son dependientes o $LD$, en vez de decir que $S$ es dependiente.
\end{obs}
\begin{list}{$\circ$}{}  
\item  Anteriormente vimos el concepto de que las combinaciones lineales de un conjunto de vectores generan un subespacio vectorial. 
\item Dado un subespacio vectorial: ¿Cual es el numero minimo de vectores que generan el subespacio? 
\item En general, dado un espacio vectorial ¿Cual es el numero minimo de vectores que generan el espacio y que propiedades tienen esos generadores? 
\end{list}
Estas preguntas seran respondidas mas adelante, pero ahora veremos algunas herramientas que nos permitiran prepararnos para estos resultados.
\begin{prop}
Sea $V$ un espacio vectorial y $v_1, \dots ,v_n \in V$. Entonces $v_1, \dots, v_n$ son $LD$ si y solo si alguno de ellos es combinacion lineal de los otros.
\end{prop}
\begin{demo}
  \;\\\\
  $(\Rightarrow)$ 
  Supongamos que son $LD$.\\\\ Entonces $\lambda_1 v_1 + \cdots + \lambda_n v_n=0$ donde algun escalar es no nulo. Digamos que tal escalar es $\lambda_i$. Podemos entonces despejar $v_i$, es decir escribirlo como combinacion lineal de los otros: \[
    v_i=-\frac{\lambda_1}{\lambda_i}v_1-\cdots -\frac{\lambda_n}{\lambda_i}v_n
  \]
  ($\Leftarrow$) Supongamos que $v_i$ es combinacion lineal de los otros, es decir \[
    \begin{aligned}
      v_i &= \lambda_1 v_1 + \cdots + \lambda_n v_n \\
\Rightarrow 0 &= \lambda_1 v_1 + \cdots - v_i + \cdots + \lambda_n v_n
    \end{aligned}
  \]
  como $-1 \neq 0$ esta multiplicando a $v_i$, la ultima igualdad dice que los vectores son $LD$. \\ \qed
\end{demo}
\pagebreak
A continuacion veremos que todo espacio vectorial tiene una base, que es un conjunto de generadores minimo. En el caso que este conjunto sea finito, todo otro conjunto de generadores como minimo tendra el mismo numero de elementos, y este numero sera llamado dimension. \\\\ Los temas a continuacion se ordenan de la siguiente forma: 
\begin{list}{$\circ$}{}  
\item Definicion de independencia lineal. 
\item Definicion de base (un conjunto linealmente independiente que genera el espacio).
\item Ejemplos de bases de espacios vectoriales. 
\item Propiedades de las bases y dimension.
\end{list}
\begin{center}
\textbf{Independencia lineal.}
\end{center}
\begin{defi}
 Sea $V$ un espacio vectorial sobre $\mathbb{K}$. Un subconjunto $S$ de $V$ se dice linealmente independiente (o simplemente, $LI$ o independiente) si no es linealmente dependiente.
\end{defi}
\begin{obs}
  Si el conjunto $S$ tiene solo un numero finito de vectores $v_1, \dots ,v_n$, se dice, a veces, que los $v_1, \dots ,v_n$ son independientes o $LI$, en vez de decir que $S$ es independiente.
\end{obs}
\begin{obs}
  Sea $V$ un espacio vectorial sobre $\mathbb{K}$ y $v_1, \dots ,v_n \in V$. \\\\ $v_1, \dots ,v_n$ son $LD$ $\;\Leftrightarrow\;$ $\exists \lambda_i$'s $\in \mathbb{K}$, alguno no nulo, tal que $\lambda_1 v_1 + \cdots + \lambda_n v_n = 0$.
\end{obs}
\begin{obs}
  Por definicion, un conjunto $v_1, \dots ,v_n$ es $LI$ si se cumple cualquiera de las dos afirmaciones siguientes: \begin{enumerate}[label=(\alph*)]
    \item $\forall \lambda_1 , \dots ,\lambda_n$ en $\mathbb{K}$ tal que $\lambda_i \neq 0$ para algun $i$, entonces\\ $\lambda_1 v_1 + \cdots + \lambda_n v_n \neq 0$, o 
    \item si $\lambda_1, \dots ,\lambda_n$ en $\mathbb{K}$ tal que $\lambda_1 v_1 + \cdots + \lambda_n v_n = 0$, entonces\\ $0 = \lambda_1 = \cdots = \lambda_n$.
  \end{enumerate}
  El enunciado (a) se deduce negando la definicion de linealmente dependiente. \\\\ El enunciado (b) es el contrarreciproco de (a).
\end{obs}\pagebreak

\begin{ej}
  En $\mathbb{R}^3$ los vectores $(1,-1,1)$ y $(-1,1,1)$ son $LI$, pues si\\ $\lambda_1(1,-1,1)+\lambda_2 (-1,1,1)=0$, entonces \\$0=(\lambda_1,-\lambda_1,\lambda_1)+(-\lambda_2,\lambda_2,\lambda_2)=(\lambda_1-\lambda_2,-\lambda_1+\lambda_2,\lambda_1+\lambda_2)$, y esto es cierto si \[
    \begin{aligned}
&\lambda_1 -\lambda_2 &&=&& 0 \\ 
      -&\lambda_1 + \lambda_2  &&=&& 0 \\
       &\lambda_1 + \lambda_2 &&=&& 0 
\end{aligned}.
  \]
  Luego $\lambda_1=\lambda_2$ y $\lambda_1=-\lambda_2$, por lo tanto $\lambda_1=\lambda_2=0$. Es decir, hemos visto que \[
\lambda_1(1,-1,1)+\lambda_2(-1,1,1)=0 \quad \Rightarrow \quad \lambda_1=\lambda_2=0,
  \]
  y, por lo tanto, $(1,-1,1)$ y $(-1,1,1)$ son $LI$. \\ \qed
\end{ej}
\begin{ej}
  Sea $\mathbb{K}$ cuerpo. En $\mathbb{K}^3$ los vectores \[
    \begin{aligned}
      v_1=(\phantom{-}3,0,-3) \\
      v_2=(-1,1,\phantom{-}2) \\
      v_3 = (\phantom{-}4,2,-2) \\
      v_4=(\phantom{-}2,1,\phantom{-}1)
    \end{aligned}
  \]
  son linealmente dependientes, pues \[
2v_1+2v_2-v_3+0\cdot v_4 = 0.
  \]
  Por otro lado, los vectores \[
    \begin{aligned}
      e_1=(1,0,0) \\
      e_2 = (0,1,0) \\
      e_3 = (0,0,1) 
    \end{aligned}
  \]
  son linealmente independientes. \\ \qed
\end{ej}
Las siguientes afirmaciones son consecuencias casi inmediatas de la definicion. \begin{enumerate}[label=\arabic*.]
  \item Todo conjunto que contiene un conjunto linealmente dependiente es linealmente dependiente. ($S' \subset S \land S' \text{ es } LD \Rightarrow S \text{ es } LD$)  \\
    \textbf{\textcolor{azulp2}{Dem.}} En el conjunto ``mas chico'' hay una combinacion lineal no trivial que lo anula, luego, en el ``mas grande'' tambien. \qed  
  \item Todo subconjunto de un conjunto linealmente independiente es linealmente independiente. ($S' \subset S \land S \text{ es } LI \Rightarrow S' \text{ es } LI$) 
    \\ \textbf{\textcolor{azulp2}{Dem.}} Si el subconjunto tiene una combinacion lineal no trivial que lo anula, el conjunto tambien. \qed 
  \item Todo conjunto que contiene el vector $0$ es linealmente dependiente. 
    \\ \textcolor{azulp2}{\textbf{Dem.}} En efecto, $1 \cdot 0 = 0$.  \qed
\end{enumerate}
\pagebreak

\begin{obs}[muy importante]
  En general, en $\mathbb{K}^m$, si queremos determinar si $v_1, \dots ,v_n$ es $LI$, planteamos la ecuacion \[
\lambda_1v_1 + \cdots + \lambda_n v_n = (0, \dots ,0).
  \]
Viendo esta ecuacion coordenada a coordenada, es equivalente a un sistema de $m$ ecuaciones lineales con $n$ incognitas (que son $\lambda_1, \dots ,\lambda_n$). \\\\ Si la unica solucion es la trivial entonces $v_1, \dots ,v_n$ es $LI$. \\\\ Si la hay alguna solucion no trivial, entonces $v_1, \dots ,v_n$ es $LD$.
\end{obs}
\begin{defi}
  Sea $V$ un espacio vectorial. Una base de $V$ es un conjunto $\mathcal{B} \subseteq V$ tal que \begin{enumerate}[label=\arabic*.] 
    \item $\mathcal{B}$ genera a $V$, y 
    \item $\mathcal{B}$ es $LI$.
    \end{enumerate}
    El espacio $V$ es de dimension finita si tiene una base finita, es decir con un numero finito de elementos.
\end{defi}
\begin{ej}
  Base canonica de $\mathbb{K}^n$. \\\\ Sea el espacio vectorial $\mathbb{K}^n$ y sean \[
    \begin{aligned}
      e_1 =(1,&0,0,\dots,0) \\
      e_2 = (0,&1,0,\dots,0) \\
               &\dots \dots \\
      e_n = (0,&0,0,\dots ,1)
    \end{aligned}
  \]
  ($e_i$ es el vector con todas su coordenadas iguales a cero, excepto la coordenada $i$ que vale $1$). Entonces veamos que $e_1,\dots,e_n$ es una base de $\mathbb{K}^n$. \begin{enumerate}
    \item Si $(x_1,\dots,x_n) \in \mathbb{K}^n$, entonces \[
        (x_1, \dots ,x_n) = x_1 e_1 + \cdots + x_n e_n.
      \]
      Por lo tanto, $e_1, \dots, e_n$ genera a $\mathbb{K}^n$.
    \item Si \[
x_1e_1 + \cdots + x_ne_n = 0,
      \]
      entonces \[
        \begin{aligned}
          (0,\cdots , 0) &= x_1(1,0,\dots , 0) + x_2(0,1,\dots , 0)+\cdots + x_n(0,0,\dots , 1) \\
                         &= (x_1, 0 , \dots ,0 ) + (0, x_2 , \dots ,0) + \cdots + + (0,0,\dots ,x_n) \\
                         &= (x_1, x_2,\dots ,x_n). 
        \end{aligned}
      \]
      Luego, $x_1=x_2= \cdots = x_n = 0.$ \\\\ Por lo tanto $e_1,\dots,e_n$ es $LI$. \\\\ 
      Para $1 \leq i \leq n$, al vector $e_i$ se lo denomina el $i$-esimo vector canonico y a la base $\mathcal{B}_n=\{e_n,\dots,e_n\}$ se la denomina la base canonica de $\mathbb{K}^n$. \qed
    \end{enumerate}
\end{ej}
\begin{ej}
  Vectores columna de una matriz invertible. \\\\ Sea $P$ una matriz $ n \times n$ invertible con elementos en el cuerpo $\mathbb{K}$. Sean $C_1, \dots ,C_n$ los vectores columna de $P$. \\\\ Entonces, $\mathcal{B}=\{C_1,\dots , C_n \}$, es una base de $\mathbb{K}^n$.
\end{ej}
\begin{demo}
  Si $X=(x_1,\dots,x_n) \in \mathbb{K}^n$, lo podemos ver como vector columna y \[
PX=x_1C_1+ \cdots + x_n C_n. 
  \]
  $PX=0$ tiene solo solucion $x=0 \Rightarrow \mathcal{B}=\{C_1, \dots ,C_n\}$ es $LI$. \\\\ ¿Por que generan $\mathbb{K}^n$? Sea $Y \in \mathbb{K}^n$, si $X=P^{-1}Y$, entonces $Y=PX$, esto es \[
Y=x_1C_1+\cdots + x_n C_n. 
  \]
  Asi, $\{C_1, \dots, C_n\}$ es una base de $\mathbb{K}^n$. \qed
\end{demo}
\begin{ej}
  Polinomios de grado $ < n$. \\\\ Sea $\mathbb{K}_n[x]$ el conjunto de polinomios de grado menor que $n$ con coeficientes en $\mathbb{K}$: \[
    \mathbb{K}_n[x]=\left\{a_0+a_1x+a_2x^2+\cdots +a_{n-1}x^{n-1}: a_0, \dots , a_{n-1} \in \mathbb{K}\right\}.
  \]
  Entonces, $1,x,^2,\dots,x^{n-1}$ es una base de $\mathbb{K}_n[x]$. \\\\ Es claro que los $1,x,x^2,\dots, x^{n-1}$ generan $\mathbb{K}_n[x]$. \\\\ Por otro lado, si $\lambda_0 +\lambda_1x+ \lambda_2 x^2 + \cdots + \lambda_{n-1}x^{n-1}=0$, tenemos que \\$\lambda_0=\lambda_1=\lambda_2=\cdots =\lambda_{n-1}=0$.
\end{ej}\pagebreak

\begin{ej}
  Polinomios (base infinita). \\\\ Sea $\mathbb{K}[x]$ el conjunto de polinomios con coeficientes en $\mathbb{K}$: \[
    \mathbb{K}[x]=\left\{a_0+a_1x+a_2x^2+\cdots + a_nx^n : n \in \mathbb{N}, \; a_0,\dots ,a_n \in \mathbb{K}\right\}.
  \]
  Entonces $\mathcal{B}=\{1,x,x^2,\dots ,x^i,\dots \}= \{x^i : i \in \mathbb{N}_0\}$ es una base de $\mathbb{K}[x]$. \\\\ Es claro que los $x^i$ generan $\mathbb{K}[x]$. \\\\ Por otro lado, supongamos $\mathcal{B}$ sea $LD$, luego existe un subconjunto finito $S$ de $\mathcal{B}$ con el cual puedo hacer una combinacion lineal no trivial que de $0$. \\\\ Sea $n$ tal que $S \subset \{1, x, x^2, \dots , x^n \}.$ Entonces existen $\lambda_i$ no todos nulos tal que $\lambda_0+\lambda_1x+\lambda_2 x^2 + \cdots + \lambda_{n}x^{n}=0$. Absurdo. \\\\ Por lo tanto $\mathcal{B}$ es base. \qed 
\end{ej}
\begin{ej}
  Base canonica de $M_{m \times n}(\mathbb{K})$. \\\\ Sean $1 \leq i \leq m, \; i \leq j \leq n$ y $ E_{ij} \in M_{m \times n}(\mathbb{K})$ definida por \[
    \begin{bmatrix}E_{ij}\end{bmatrix}_{kl}=\begin{dcases}
    1 & \text{si }i = k \text{ y } j = l,\\
    0 & \text{otro caso.}
    \end{dcases}
  \]
  Es decir $E_{ij}$ es la matriz cuyas entradas son todas iguales a $0$, excepto la entrada $ij$ que vale $1$. En el caso $2 \times 2$ tenemos las matrices \[
    E_{11}=\begin{bmatrix}1 & 0 \\ 0 & 0 \end{bmatrix},
    \quad E_{12}=\begin{bmatrix}0 & 1 \\ 0 & 0 \end{bmatrix}, \quad E_{21}=\begin{bmatrix}0 & 0 \\ 1 & 0 \end{bmatrix}, \quad E_{22}=\begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}.
  \]
  Volviendo al caso general, \[
  \mathcal{B}=\left\{E_{ij} : 1 \leq i \leq m, \; 1 \leq j \leq n\right\}
  \]
  (son $mn$ vectores) es una base de $M_{m \times n}(\mathbb{K})$ y se la denomina la base canonica de $M_{m \times n}(\mathbb{K})$. \\\\ La demostracion es analoga al caso $\mathbb{K}^n$.
\end{ej}
\pagebreak

Si $S$ es un conjunto finito denotemos $|S|$ al cardinal de $S$ es decir, la cantidad de elementos de $S$. \\\\ 
\textbf{Preguntas}
\begin{list}{$\circ$}{}  
\item Dado $V$ espacio vectorial ¿Existe una base de $V$? \\
  \textbf{Respuesta:} si. La respuesta la da la teoria de conjuntos (Lema de Zorn). 
\item Sea $V$ espacio vectorial y $\mathcal{B}$, $\mathcal{B}'$ bases finitas de $V$ ¿Es $|\mathcal{B}|=|\mathcal{B}'|$? \\ \textbf{Respuesta:} si. Es lo que veremos mas adelante.
\end{list}
\begin{center}
\textbf{¿Todo espacio vectorial tiene una base ``explicita''?}
\end{center}
\begin{list}{$\circ$}{}  
\item Vimos en los ejemplos de las paginas anteriores bases de distintos espacios vectoriales. 
\item Vimos que hay bases finitas y bases infinitas, pero todas las bases que consideramos eran explicitas. 
\item Por el Lema de Zorn existe una base $\mathcal{B}$ de $F(\mathbb{R})=\{f: \mathbb{R} \to \mathbb{R}\}$. 
\item ¿Se puede dar en forma relativamente explicita una base de $F(\mathbb{R})$? 
\item Respuesta: NO.
\end{list}
\begin{teo}
  Sea $V$ un espacio vectorial generado por un conjunto finito de vectores $w_1, \dots ,w_m$. Entonces \[
    S \subset V \text{ es }  LI \Rightarrow |S| \leq m.
  \]
\end{teo}
\begin{demo}
  Para demostrar este teorema es suficiente probar el contrarreciproco del enunciado, es decir: \[
    \text{si } |S| > m \Rightarrow S \text{ es } LD,
  \]
  Sea $S = \{v_1, \dots ,v_n\}$ con $n > m$. \\\\ Como $w_1, \dots ,w_m$ generan $V$, existen escalares $a_{ij}$ en $\mathbb{K}$ tales que \[
    v_{j}=\sum_{i=1}^{m}a_{ij}w_{i},  \quad \quad (1\leq j \leq n).
  \]
  Probaremos ahora que existen $x_1, \dots ,x_n \in \mathbb{K}$ no todos nulos, tal que \\ $x_1v_1 + \cdots + x_nv_n=0$ ($S$ es $LD$). \\\\ Ahora bien, para cualesquiera $x_1,\dots ,x_n \in \mathbb{K}$ tenemos \begin{align*}
    x_1v_1 + \cdots + x_n v_n &= \sum_{j=1}^{n}x_jv_j \\
                           &= \sum_{j=1}^{n}x_{j}\sum_{j=1}^{m}a_{ij}w_{i} \\ 
                           &= \sum_{j=1}^{n}\sum_{i=1}^m(x_{j}a_{ij})w_{i} \\
                           &= \sum_{i=1}^{m}\underbrace{\left(\sum_{j=1}^{n}x_{j}a_{ij}\right)}_{c_i}w_i.   \tag{*}\end{align*}

                           Si cada coeficiente $c_i$ es nulo $\Rightarrow x_1 v_1 + \cdots + x_n v_n =0$. \\\\ Vamos a ver ahora que $\exists x_1, \dots ,x_n$ no todos nulos tal que $c_i=0,\forall i$.  \\\\ Esto se debe a que el sistema de ecuaciones \[
                          \sum_{j=1}^{n}x_{j}a_{ij}=0, \quad \quad (1 \leq i \leq m)
                        \]
                        tiene $m$ ecuaciones y $n$ incognitas con $n>m \Rightarrow $ existen soluciones no triviales (quedan variables libres). \\\\ Es decir, existen escalares $x_1, \dots, x_n \in \mathbb{K}$ no todos nulos, tal que \[
                          \sum_{j=1}^{n}x_ja_{ij}=0, \quad (1 \leq i \leq m) 
                        \]
                        y, por (*), tenemos que \[
x_1v_1+\cdots + x_n v_n=0. 
                        \]
                        Esto quiere decir que los $v_1, \dots ,v_n$ son $LD$. \qed 
                        \end{demo}
                        \pagebreak

                       



Dado $V$ espacio vectorial de dimension finita, veremos 
\begin{list}{$\circ$}{}  
\item la definicion de dimension, 
\item todo subconjunto $LI$ puede ser completado a una base, 
\item de todo subconjunto de generadores se puede extraer una base.
\end{list}
Veremos tambien como en forma practica podemos encontrar a partir de un conjunto $S$ de generadores de un subespacio $W$ de $\mathbb{K}^n$
\begin{list}{$\circ$}{}  
\item una base de $W$.
\item un subconjunto $S$ de generadores de $W$.
\end{list}

Recordemos este importante resultado de la pagina anterior: \\\\ Sea $V$ un espacio vectorial y $T \subset V$, finito tal que $\langle T \rangle = V$. Sea $S \subset V$. \\\\ Entonces \begin{equation}
  \langle T \rangle =V, \quad S \text{ es } LI \Rightarrow |S| \leq |T|. \tag{P1}
\end{equation}
El contrarreciproco tambien nos resultara de utilidad \begin{equation}
  \langle T \rangle = V, \quad |S| > |T| \Rightarrow S \text{ es }LD. \tag{P2}
\end{equation}
\begin{corol}
  Si $V$ es un espacio vectorial de dimension finita, entonces dos bases cualesquiera de $V$ tienen el mismo numero de elementos.
\end{corol}
\begin{demo} \; \\\\
  $V$ es de dimension finita $\Rightarrow \exists \; \mathcal{B}$ base con $|\mathcal{B}|<\infty$. \\\\ Sea $\mathcal{B}'$ otra base de $V$. \\\\ Como $\mathcal{B}$ es base $\Rightarrow \langle \mathcal{B} \rangle =V$ y $\mathcal{B}'$ es $LI \overset{(\text{P1})}{\Rightarrow} |\mathcal{B'}| \leq |\mathcal{B}|$. \\\\ 
Como $\mathcal{B}'$ es base $\Rightarrow \langle \mathcal{B}' \rangle =V$ y $\mathcal{B}$ es $LI$ $\overset{\text{(P1)}}{\Rightarrow} |\mathcal{B}| \leq |\mathcal{B}'|. $
\\\\
En consecuencia $|\mathcal{B}|=|\mathcal{B}'|.$ \qed 
\end{demo} \pagebreak

Hemos demostrado: si $V$ es un espacio vectorial de dimension finita y $\mathcal{B}, \mathcal{B}'$ dos bases de $V$, entonces $|\mathcal{B}|=|\mathcal{B}'|$. \\\\ Esto nos permite hacer la siguiente definicion.\begin{defi}
                          Sea $V$ espacio vectorial de dimension finita. \\\\ Diremos que $n$ es la dimension de $V$ y denotaremos $dim \; V = n$, si existe una base de $V$ de $n$ vectores. \\\\ Si $V=\{0\}$, entonces definimos $dim \; V = 0$.
                        \end{defi}
                        \begin{ej}
                          Sean $m,n \in \mathbb{N}$. \begin{enumerate}[label=(\arabic*)]
                            \item $dim \; \mathbb{K}^n=n$, pues la base canonica tiene $n$ elementos. 
                            \item $dim \; M_{m \times n}(\mathbb{K})=mn$, pues la base canonica de $M_{m \times n}(\mathbb{K})$ tiene $mn$ elementos. 
                            \item $dim \; \mathbb{K}_{n}[x]=n$, pues $1,x,x^2,\dots ,x^{n-1}$ es una base.
                          \end{enumerate}
                        \end{ej}

                        \begin{corol} \; \\
                          Sea $V$ un espacio vectorial de dimension finita y sea $n=dim\; V$. Entonces \begin{enumerate}[label=(\arabic*)]
                            \item $S \subset V$ y $|S| > n \Rightarrow S$ es $LD$.
                            \item $S \subset V$ y $|S| < n \Rightarrow \langle S \rangle \subsetneq V$.
                          \end{enumerate}
                        \end{corol}
                        \begin{demo} \;
                          \begin{enumerate}[label=(\arabic*)]
                          \item Como $\mathcal{B}$ es base $\Rightarrow \langle \mathcal{B}\rangle = V$ y $|S| > |\mathcal{B}| \overset{\text{(P2)}}{\Rightarrow} S $ es $LD$.          
                          \item Supongamos que $\langle S \rangle = V$. \\\\ Como $\mathcal{B}$ es base $\Rightarrow \mathcal{B}$ es $LI$. \\\\ $\langle S \rangle = V$ y $\mathcal{B}$ es $LI \overset{(P1)}{\Rightarrow} n = |\mathcal{B}| \leq |S|$. Absurdo. \\ \qed 
      \end{enumerate}
                        \end{demo}
\pagebreak

\begin{lema}
  Sea $S$ un subconjunto linealmente independiente de un espacio vectorial $V$. \\\\ Sea $W$ tal que $w \notin \langle S \rangle $. \\\\ Entonces $S \cup \{w\}$ es $LI$.
\end{lema}
\begin{demo}
  Suponga que $v_1, \dots , v_n$ son vectores distintos de $S$ y $\lambda_i, \lambda \in \mathbb{K}$ tales que \begin{equation}
    \lambda_{1}v_1+ \cdots + \lambda_nv_n+\lambda w = 0. \tag{1}
\end{equation}
  Debemos probar que $\lambda_i = 0, \; 1 \leq i \leq n$, y $\lambda=0$.\\\\
  Supongamos que $\lambda \neq 0$, entonces podemos dividir la ecuacion por $\lambda$ y haciendo pasaje de termino obtenemos \[
    w=\left(-\frac{\lambda_1}{\lambda}\right)v_1 + \cdots +\left(-\frac{\lambda_n}{\lambda}\right)v_n.
  \]
  Luego $w$ estaria en el subespacio generado por $S$, lo cual contradice la hipotesis. \\\\ Por lo tanto $\lambda=0$ y, en consecuencia \[
    \lambda_1v_1+\cdots + \lambda_{n}v_n=0.
  \]
  Como $S$ es un conjunto linealmente independiente, todo $\lambda_i=0$. \\\\ Entonces $\{v_1, \dots ,v_n, w\}$ es $LI$.\qed
\end{demo}
\begin{teo}
 Sea $V$ espacio vectorial de dimension finita $n$ y $S_0$ un subconjunto $LI$ de $V$. Entonces $S_0$ es finito y existen $w_1, \dots ,w_m$ vectores en $V$ tal que $S_0 \cup \{w_1, \dots , w_m\}$ es una base de $V$.
\end{teo}
\begin{corol}
Sea $W$ un subespacio de un espacio vectorial con dimension finita $n$ y $S_0$ un subconjunto $LI$ de $W$. Entonces, $S_0$ se puede completar a una base de $W$. 
\end{corol}
\begin{corol}
  Sea $V$ espacio vectorial de dimension finita y $V \neq \{0\}$, entonces $dim \; V>0$.
\end{corol}\pagebreak

\begin{corol}
  Si $W$ es un subespacio propio de un espacio vectorial de dimension finita $V$, entonces $W$ es de dimension finita y $dim \; W < dim \; V$. 
\end{corol}
\begin{obs}
  $W$ subespacio propio de $V$ implica $W \subset V \land W \neq V$.
\end{obs}
\begin{demo}\;\\\\ 
  Si $W = \{0\}$, entonces $dim \; W=0$, como $W \subsetneq V$, tenemos que $V$ es no nulo y por lo tanto $dim \; W=0 <dim \; V$. \\\\ Si $W \neq \{0\}$, sea $\mathcal{B}'$ base de $W$. $\mathcal{B}'$ es $LI$ en $W$ y tambien en $V$. \\\\ Sea $\mathcal{B}$ base de $V$, luego $\langle \mathcal{B} \rangle =V$. \\\\ $\langle \mathcal{B} \rangle = V,\; \mathcal{B}'$ es $LI \overset{(\text{P1})}{\Rightarrow} |\mathcal{B}'| \leq |\mathcal{B}|$. Es decir $dim \; W < dim \; V$. \\\\ (Como se completa $\mathcal{B}'$ a base de $V \Rightarrow |\mathcal{B}'|<|\mathcal{B}|$) \\\qed

\end{demo}
Hemos visto que si $V$ es un espacio de dimension finita, entonces todo conjunto $LI$ se puede extender a una base. Tambien vale: \begin{teo}
  Sea $V \neq 0$ espacio vectorial y $S$ un conjunto finito de generadores de $V$ entonces existe un subconjunto $\mathcal{B}$ de $S$ que es una base. \[
    \langle S \rangle = V \Rightarrow \exists \mathcal{B} \subseteq S \text{ tal que } \mathcal{B} \text{ es base}.
  \]
\end{teo}
El siguiente resultado relaciona dimension con suma e interseccion de subespacios. 
\begin{teo}
  Si $W_1$, y $W_2$ son subespacios de dimension finita de un espacio vectorial, entonces $W_1+W_2$ es de dimension finita y \[\begin{aligned}
  dim \; W_1 + dim \; W_2 &= dim(W_1 \cap W_2 )+ dim(W_1+W_2) \\ dim\;(W_1+W_2)&=dim(W_1)+dim(W_2)-dim(W_1 \cap W_2).\end{aligned}
  \]
\end{teo} \pagebreak

\begin{defi}
  Si $V$ es un espacio vectorial de dimension finita, una base ordenada de $V$ es una sucesion finita de vectores linealmente independiente y que genera $V$. \\\\ La diferencia entre la definicion de ``base'' y ``base ordenada'', es que en la ultima es importante el orden de los vectores de la base. \\\\ Se incurrira en un pequeño abuso de notacion y se escribira \[\mathcal{B}=\{v_1, \dots ,v_n\}\] diciendo que $\mathcal{B}$ es una base ordenada de $V$.
\end{defi}
\begin{prop}
  Sea $V$ espacio vectorial de dimension finita y sea $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$. Entonces, para cada $v \in V$, existen unicos \mbox{$x_1, \dots ,x_n \in \mathbb{K}$} tales que \[
v=x_1v_1+ \cdots + x_n v_n.
\]
\end{prop}
\begin{demo}
  Como $v_1, \dots ,v_n$ generan $V$, es claro que existen $x_1, \dots ,x_n \in \mathbb{K}$ tales que \[
v=x_1v_1 + \cdots + x_nv_n.
  \]
  Sean $y_1, \dots ,y_n \in \mathbb{K}$ tales que \[
v=y_1v_1 + \cdots + y_nv_n.
  \]
  Veremos que $x_i=y_i$ para $1 \leq i \leq n$. \\\\ Como $v=\sum_{i=1}^{n}x_i v_i$ y $v=\sum_{i=1}^{n}y_i v_i$, restando miembro a miembro obtenemos \[
    0=\sum_{i=1}^{n}(x_i-y_i)v_i.
  \]
  Ahora bien, $v_1, \dots ,v_n$ son $LI$, por lo tanto todos los coeficientes de la ecuacion anterior son nulos. \\\\ Es decir $x_i-y_i=0$ para $1 \leq i \leq n$. \\\\ Entonces $x_i =y_i$ para $1 \leq i \leq n$. \\ \qed 
\end{demo}
\pagebreak

\begin{center}
\textbf{Dimension de subespacios.}
\end{center}
\begin{list}{$\circ$}{}  
\item Si $A$ matriz $m \times n$, en donde $W=\{x : Ax=0\}$ es un subespacio. 
\item ¿Cual es la dimension de W? ¿Que relacion tiene con $R$, la $MRF$ equivalente a $A$? 
\item Veremos que si $r$ es la cantidad de filas no nulas de $R$, entonces \\ $dim(W)=n-r$.
\end{list}
\begin{ej}
  Encontrar una base del subespacio \[
    W=\left\{(x,y,z,w) \in \mathbb{R} : \quad 
      \begin{array}{rcr}
        x-y-3z+\phantom{3}w  & = &  0 \\
        y + 5z+3w  & = &  0 
\end{array}\right\}.
  \]
  \textbf{Solucion}\\\\
  $W$ esta definido implicitamente y usando el metodo de Gauss podemos describirlo parametricamente, pues: \[
    \begin{bmatrix}1 & -1 & -3 & 1 \\ 0 & 1 & 5 & 3 \end{bmatrix} \xrightarrow{F_1+F_2}\begin{bmatrix}1 & 0 & 2 & 4 \\ 0 & 1 & 5 & 3 \end{bmatrix}.
  \]
  Por lo tanto, el sistema de ecuaciones que define $W$ es equivalente a \[
    \begin{array}{rcr}x+2z+4w & = & 0\phantom{,}\\
      y+5z+3w & = & 0,
    \end{array}
  \]
  es decir
  \[
    \begin{array}{rcr}
      x & = & -2z- 4w \phantom{,}\\
      y &=& - 5z- 3w ,
    \end{array}
  \]
   y entonces \[
     \begin{aligned}
       W &= \{(-2z-4w,-5z-3w,z,w) : z,w \in \mathbb{R}\} \\
         &=\{(-2,-5,1,0)z+(-4,-3,0,1)w:z,w \in \mathbb{R}\} \\ &= \langle (-2,-5,1,0),(-4,-3,0,1)\rangle. 
     \end{aligned}
   \]
   Concluimos entonces que $(-2,-5,1,0),(-4,-3,0,1)$ es una base de $W$ y, por lo tanto, su dimension es $2$. \qed
\end{ej}
\pagebreak

\begin{defi} \; \\\\
  Sea $A=\begin{bmatrix}a_{ij}\end{bmatrix}\in M_{m \times n}(\mathbb{K})$.\begin{list}{$\circ$}{}  
\item El vector fila $i$ es el vector $(a_{i1},\dots ,a_{in})\in \mathbb{K}^n$.
\item El espacio fila de $A$ es el subespacio de $\mathbb{K}^n$ generado por los $m$ vectores fila de $A$.
\item El vector columna $j$ es el vector $(a_{1j}, \dots ,a_{mj}) \in \mathbb{K}^m$.
\item El espacio columna de $A$ es el subespacio de $\mathbb{K}^m$ generado por los $n$ vectores columna de $A$.
\end{list}
\end{defi}
\begin{ej}
 Sea \[
   A=\begin{bmatrix}1 & 2 & 0 & 3 & 0 \\ 0 & 0 & 1 & 4 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{bmatrix}. 
 \]
 El vector fila $1$ es $(1,2,0,3,0)$, el vector columna $4$ es $(3,4,0)$, etc. \\\\ Sea $W$ el espacio fila de $A$. Entonces \[
W=\langle (1,2,0,3,0),(0,0,1,4,0),(0,0,0,0,1)\rangle.
 \]
 Sea $U$ el espacio columna de $A$. Entonces: \[
   U=\langle (1,0,0),(2,0,0),(0,1,0),(3,4,0),(0,0,1) \rangle = \mathbb{R}^3.
 \]
\end{ej} \pagebreak

\begin{teo}
  Sean $A$ matriz $m \times n$ con coeficientes en $\mathbb{K}$, $P$ matriz $m \times m$ invertible y $B=PA$. Entonces el espacio fila de $A$ es igual al espacio fila de $B$.
\end{teo}
\begin{demo}
  Sea $W_1$ espacio fila de $A$ y $W_2$ espacio fila de $B$. \\\\ Sea $A=\begin{bmatrix}a_{ij}\end{bmatrix}, P=\begin{bmatrix}p_{ij}\end{bmatrix}$ y $B=\begin{bmatrix}b_{ij}\end{bmatrix}$. Como $B=PA$, tenemos que la fila $i$ de $B$ es 
  \begin{align}
    (b_{i1}, \dots ,b_{in}) &= (F_i(P)\cdot C_1 (A),\; \dots \; , F_i(P) \cdot C_n(A)) \notag \\ 
                            &= \left(\sum_{j=1}^{m}p_{ij}a_{j1}, \dots ,\sum_{j=1}^{m}p_{ij}a_{jn} \right) \notag \\
                            &= \sum_{j=1}^{m}p_{ij}(a_{j1},\dots ,a_{jn}). \tag{*}
  \end{align}
\begin{list}{$\circ$}{}  
\item  por (*) cada vector fila de $B$ se puede obtener como combinacion lineal de los vectores fila de $A$. 
\item Por lo tanto el espacio fila de $B$ esta incluido en el espacio fila de $A$: $W_2 \subset W_1$. 
\item $P$ invertible $\Rightarrow \exists P^{-1}$. 
\item $P^{-1}B=P^{-1}PA=A.$ \item Un razonamiento analogo al (*) anterior $\Rightarrow$ espacio fila de $A$ esta incluido en el espacio fila de $B$: $W_1 \subset W_2$. \[
W_2 \subset w_1 \quad \land \quad W_1 \subset W_2 \quad \Rightarrow \quad W_1 = W_2. 
  \]\qed
\end{list}
\end{demo}
\begin{corol}
 Sean $A$ matriz $m \times n$ y $R$ la $MRF$ equivalente por filas a $A$. Entonces, \begin{enumerate}[label=(\arabic*)]
   \item el espacio fila de $A$ es igual al espacio fila de $R$, 
   \item las filas no nulas de $R$ forman una base del espacio fila de $A$.
 \end{enumerate}
\end{corol}
\begin{demo}\; \begin{enumerate}[label=(\arabic*)]
\item $R$ la $MRF$ equivalente por filas a $A \Rightarrow R = PA$ con $P$ invertible $\overset{\text{Teo. ant.}}{\Rightarrow}$ espacio fila de $A=$ espacio fila de $R$.
\item $R$ es $MRF \Rightarrow$ cada fila no nula comienza con un $1$ y en esa coordenada todas las demas filas tienen un $0 \Rightarrow $ las filas no nulas de $R$ son $LI \Rightarrow $ las filas no nulas de $R$ son base.
\end{enumerate}\qed
\end{demo}
\pagebreak

\begin{corol}
  Sean $A$ matriz $n \times n$. Entonces, $A$ es invertible si y solo si las filas de $A$ son una base de $\mathbb{K}^n$.
\end{corol}
\begin{demo}
  Si $A$ es invertible entonces la $MERF$ de $A$ es la identidad, por lo tanto el espacio fila de $A$ genera $\mathbb{K}^n$. \\\\ Por otro lado, si el espacio fila de $A$ genera $\mathbb{K}^n$, el espacio fila de la $MERF$ es $\mathbb{K}^n$ y por lo tanto la $MERF$ de $A$ es la identidad y en consecuencia $A$ es invertible. \\\\ 
  Hemos probado que $A$ es invertible si y solo si las $n$ filas de $A$ generan $\mathbb{K}^n$. \\\\ Como $dim \; \mathbb{K}^n=n$, todo conjunto de $n$ generadores es una base. \qed
\end{demo}\pagebreak

\begin{center}
\textbf{Bases de subespacios.}
\end{center}
El corolario 15 nos permite encontrar facilmente la dimension de un subespacio de $\mathbb{K}^n$ generado explicitamente por $m$ vectores. 
\begin{list}{$\circ$}{}  
\item Sea $v_1, \dots , v_m \in \mathbb{K}^n$ y $W=\langle v_1, \dots ,v_m \rangle$, 
\item Consideramos la matriz \[
    A=\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_m \end{bmatrix} 
  \]

  \item Calculamos $R$, una $MRF$ equivalente por filas a $A$. 
  \item $W=$ espacio fila de $R$. 
  \item Si $R$ tiene $r$ filas no nulas, las $r$ filas no nulas son una base de $W$. 
  \item Por consiguiente, $dim \; W=r$.
\end{list}\begin{ej}
  Encontrar una base de $W=\langle (1,0,1),(1,-1,0),(5,-3,2)\rangle.$ \\\\
  \textbf{Solucion}\\\\ 
  Formemos la matriz cuyas filas son los vectores que generan $W$, es decir \[
    A=\begin{bmatrix}1 & 0 & 1 \\ 1 & -1 & 0 \\ 5 & -3 & 2 \end{bmatrix}.
  \]
  Entonces \[
    \begin{bmatrix}1 & 0 & 1 \\ 1 & -1 & 0 \\ 5 &-3 & 2 \end{bmatrix}\underset{F_3-5F_1}{\xrightarrow{F_2-F_1}} \begin{bmatrix}1 & 0 & 1 \\ 0 & -1 & -1 \\ 0 & -3 & -3 \end{bmatrix}\xrightarrow{-F_2} \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & -3 & -3 \end{bmatrix} \xrightarrow{F_3-3F_2} \begin{bmatrix}1 & 0 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 \end{bmatrix}.
  \]
  Por lo tanto, $dim\; W=2$ y $(1,0,1),(0,1,1)$ es una base de $W$. \\ \qed
\end{ej}
\pagebreak
\begin{center}
\textbf{Subconjuntos $LI$ de un sistema de generadores.}
\end{center}
\begin{list}{$\circ$}{}  
\item Dado un conjunto de generadores de un subespacio $W$ ``sabemos'' encontrar una base de $W$.
\item Esa base de $W$, en general, utiliza otros vectores (no necesariamente los generadores).
\item Veremos a continuacion que dado $S=\{v_1, \dots ,v_m\}$ y $W=\langle S \rangle $, podemos encontrar facilmente un subconjunto de $S$ base de $W$.
\end{list}
\begin{teo}
  Sea $v_1, \dots ,v_r$ vectores en $\mathbb{K}^n$ y $W=\langle v_1, \dots ,v_r \rangle $. \\\\ Sea $A$ la matriz formada por las filas $v_1, \dots ,v_r$ y $R$ una $MRF$ equivalente por filas a $A$ que se obtiene \textbf{sin} el uso de permutaciones de filas. \\\\ Si $i_1, i_2 , \dots ,i_s$ filas no nulas de $R \Rightarrow v_{i_1},v_{i_{2}},\dots ,v_{i_{s}}$ base de $W$.
\end{teo}
\begin{demo}
  Se hara por induccion sobre $r$. \\\\ Si $r=1$ es trivial ver que vale la afirmacion. \\\\ Supongamos que tenemos el resultado probado para $r-1$ (hipotesis inductiva).\\\\ Sea $W'=\langle v_1, \dots ,v_{r-1} \rangle $ y sea $A'$ la matriz formada por las $r-1$ filas $v_1, \dots ,v_{r-1}$. Sea $R'$ la $MRF$ equivalente por filas a $A'$ que se obtiene sin usar permutaciones de filas. Por hipotesis inductiva, si $i_1, i_2, \dots, i_s$ son las filas no nulas de $R'$, entonces $v_{i_1},v_{i_{2}}, \dots ,v_{i_{s}}$ es una base de $W'$. \\\\ Sea \[
    R_0=\begin{bmatrix}R' \\ v_r \end{bmatrix}. 
  \]
  Si $v_r \in W'$, entonces $v_{i_{1}}, v_{i_{2}}, \dots ,v_{i_{s}}$ es una base de $W$ y \[
    R=\begin{bmatrix} R' \\ 0 \end{bmatrix}
  \]
  es la $MRF$ de $A$. \\\\ Si $v_r \notin W'$, entonces $v_{i_{1}}, v_{i_{2}}, \dots ,v_{i_{s}},v_r$ es una base de $W$ y la $MRF$ de $A$ tiene la ultima fila no nula. \qed
\end{demo}
\pagebreak

Finalmente, terminaremos con un teorema que resume algunas equivalencias respecto a matrices invertibles. 
\begin{teo}
Sea $A$ matriz $n \times n$ con coeficientes en $\mathbb{K}$. Entonces son equivalentes \begin{enumerate}[label=(\arabic*)]
  \item $A$ es invertible. 
  \item $A$ es equivalente por filas a $Id_n$. 
  \item $A$ es producto de matrices elementales. 
  \item El sistema $AX=Y$ tiene una unica solucion para toda matriz $Y$ de orden $n \times 1$. 
  \item El sistema homogeneo $AX=0$ tiene una unica solucion trivial. 
  \item $det \; A \neq 0$. 
  \item Las filas de $A$ son $LI$.
  \item Las columnas de $A$ son $LI$.
\end{enumerate}
\end{teo}
\pagebreak

\section{Transformaciones lineales.}
\begin{defi}
  Una transformacion lineal entre dos espacios vectoriales $V$ y $W$ es una funcion $T : V \to W$ tal que \begin{enumerate}[label=(\arabic*)]
    \item Preserva la suma: \[
T(v+v')=T(v)+T(v')\quad \forall v,v' \in V
      \]
    \item Preserva el producto por escalares \[
        T(\lambda v)=\lambda T(v) \quad \forall v \in V, \lambda \in \mathbb{R}
      \]
  \end{enumerate}
\end{defi}
\begin{obs}
 $T : V \to W$ es transformacion lineal $\Leftrightarrow$ \[
   T(v+\lambda v') = T(v) + \lambda T(v') \quad \forall v,v' \in V, \lambda \in \mathbb{K}.
 \]
 Ya conocemos algunas transformaciones lineales. Por ejemplo: \begin{list}{$\circ$}{}  
 \item  La derivada: \[
     (f+cg)'=f'+cg'
   \]
 \item La integral: \[
     \int_{a}^{b}(f+cg)dx=\int_{a}^{b}f\; dx+c\int_{a}^{b}g \; dx
     \]
   \item La multiplicacion por matrices: \[
A(v+\lambda v')=Av+\lambda Av'
   \]
\end{list}
\end{obs}
Un isomorfismo lineal es una transformacion lineal que es biyectiva. Por ejemplo, si definimos \[
  \begin{aligned} 
&T:\mathbb{R}_<n[x] \to \mathbb{R}^n \\
&T(a_{n-1}x^{n-1}+\cdots +a_{0})=(a_{n-1}, \dots , a_{0})
  \end{aligned}
\]
Esta identificacion es un isomorfismo. \\\\
Otro isomorfismo que hemos mencionado repetidas veces es \[
  \begin{array}{lccc}
  S : &  \mathbb{K}^{2 \times 2} & \to & \mathbb{K}^4 \\
      & \begin{bmatrix}a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} & \mapsto & (a_{11},a_{12},a_{21},a_{22})
  \end{array}
\]
\begin{teo}
  Sean $V$ y $W$ dos espacios vectoriales de dimension finita tal que $dim(V)=dim(W)$ entonces $V$ y $W$ son isomorfos.
\end{teo}
\pagebreak

\begin{center}
\textbf{Ejemplos de transformaciones lineales.}
\end{center}
\begin{ej}
  Sea $T : \mathbb{K}^3 \to \mathbb{K}^2$ definida por \[
T(x_1,x_2,x_3)=(2x_1-x_3,-x_1+3x_2+x_3).
  \]
  Entonces, $T$ es una transformacion lineal.
  \\\\ La demostracion es rutinaria y parte de un resultado mas general. \\\\ Observar que si \[
    A=\begin{bmatrix} 2 & 0 & -1 \\ -1 & 3 & 1 \end{bmatrix},
  \]
  entonces \[
    \begin{bmatrix}2 & 0 & -1 \\ -1 & 3 & 1 \end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}2x_1 -x_3 \\-x_1+3x_2+x_3\end{bmatrix}.
  \]
\end{ej}
\begin{obs}
  Sea $T: \mathbb{K}^n \to \mathbb{K}^m.$ En general si $T(x_1, \dots ,x_n)$ en cada coordenada tiene una combinacion lineal de los $x_1, \dots ,x_n$ entonces $T$ es una transformacion lineal. Mas precisamente, si $T$ esta definida por \[\begin{aligned}
    T(x_1,\dots ,x_n) &= (a_{11}x_1+\cdots +a_{1n}x_n, \; \dots \;, a_{m1}x_1+\cdots + a_{mn}x_n) \\
                      &= \left(\sum_{j=1}^na_{1j}x_j,\; \dots \;, \sum_{j=1}^na_{mj}x_j\right),
  \end{aligned}
  \]
  con $a_{ij}\in \mathbb{K}$, entonces $T$ es lineal.
\end{obs}
\begin{demo}
  Ejercicio (se vera mas adelante). \qed
\end{demo}
\begin{ej}
  Sea $C^{1}$ el espacio de funciones derivables. Entonces la derivada es una transformacion lineal pues: \[
    (f+g)'=f'+g' \quad \text{y} \quad (\lambda f)'=\lambda f'
  \]
\end{ej}
\begin{ej}
  Sea $V$ un espacio vectorial. La funcion identidad $Id : V \to V$, \[
Id(v)=v \quad \forall v \in V,
  \]
  es una transformacion lineal.
\end{ej}
\begin{ej}
  No todas las funciones son transformaciones lineales. La funcion $f(x)=x^2$ de $\mathbb{R}$ en $\mathbb{R}$ no es lineal. Probamos esto dando un ejemplo concreto donde no se verifique algunas de las propiedades. Por ejemplo: \[
    (1+1)^2=4\neq 2 = 1^2+1^2.
  \]
\end{ej}
Podemos dar una definicion equivalente de transformacion lineal que reuna las dos condiciones en solo una. \\\\ Algo similar a la definicion de subespacio. \begin{defi}
  Una transformacion lineal entre dos espacios vectoriales $V$ y $W$ es una funcion $T: V \to W$ tal que \begin{list}{$\circ$}{}  
  \item  $T(\lambda v + v' ) =\lambda T(v)+T(v') \quad \forall v,v' \in V, \lambda \in \mathbb{R}$
\end{list}
Podemos usar cualquiera de las dos definiciones para decidir si una funcion es transformacion lineal o no.
\end{defi}
\begin{obs}
  Sea $T : V \to W$ una transformacion lineal. Entonces $T(0_v)=0_w$ donde $0_v$ es el elemento neutro para la suma de $V$ y $0_w$ es el elemento neutro para la suma de $W$.
\end{obs}
\begin{demo}
  
    \begin{align}
      T(0)&=T(0+0) \tag{$0$ es elemento neutro} \\
          &= T(0)+T(0) \tag{$T$ es lineal} \\
      -T(0)+T(0) &= -T(0)+T(0)+T(0) \tag{logica} \\
      0 &= 0 + T(0) \tag{opuesto de la suma} \\ 
      0 &= T(0) \tag{neutro de la suma}
    \end{align}
 \qed 
\end{demo}
Entre otras cosas esta propiedad, es util como ``test'' para verificar si una funcion no es transformacion lineal.
\begin{ej}
  Sea $V$ un espacio vectorial y $v_0 \in V$ un vector no nulo. Entonces la funcion $f : V \to V $ dada por \[
f(v)=v+v_0 \quad \forall v \in V
  \]
  no es lineal dado que \[
f(0)=0+v_0=v_0 \neq 0.
  \]
\end{ej}
\begin{obs}
Las transformaciones lineales preservan combinaciones lineales, es decir si $T : V \to W$ es una transformacion lineal, $v_1, \dots ,v_k \in V$ y \mbox{$\lambda_1, \dots ,\lambda_k \in \mathbb{R},$} entonces \[
T(\lambda_1 v_1 + \cdots + \lambda_k v_k )= \lambda_1 T(v_1) + \cdots + \lambda_k T(v_k)
  \]
  \begin{demo}
    (Esquiema de la demostracion) \\\\ La demostracion sigue por induccion y aplicando la definicion de transformacion lineal. \begin{list}{$\circ$}{}  
    \item  \textbf{Caso base.} $T(\lambda_1 v_1) =\lambda_1 T(v_1)$. Lo cual es cierto porque es una de las condiciones de la definicion de transformacion lineal. 
    \item \textbf{Paso inductivo.} \[
        \begin{aligned}
          T(\lambda_1v_1+ \cdots + \lambda_k v_k)&= T(\lambda_1 v_1 ) + T(\lambda_2 v_2 + \cdots + \lambda_k v_k) && (T \text{ es t.l.}) \\
                                                &= \lambda_1 T (v_1 ) + \cdots + \lambda_k T(v_k) && (\text{C. base e HI})
        \end{aligned}
      \]
      \qed
\end{list}
  \end{demo}
\end{obs}
\begin{defi}
  Sea $T : V \to W$ una transformacion lineal. 
\begin{list}{$\circ$}{}  
\item La imagen de $T$ es el subconjunto de $W$ \[
    Im(T)=\{T(v) \; | \; v \in V\}=\{w \in W \; | \; T(v) = w\}
  \]
\item El nucleo de $T$ es el subconjunto de $V$ \[
    Nu(T)=\{v \in V \; | \; T(v) = 0\}
  \]
\end{list}
\end{defi}
\begin{obs} \;
\begin{list}{$\circ$}{}  
\item  $Im(T)$ se define como la imagen de cualquier funcion.
\item $Nu(T)$ serian las raices de la transformacion. 
\item $Nu(T)$ es definido de forma implicita al igual que la segunda expresion de $Im(T)$. 
\item La primera expresion de $Im(T)$ es de forma explicita o parametrica, donde el parametro es un vector.
\end{list}
\textbf{Notacion.} Si $T : V \to W$ transformacion lineal denotamos 
\[
  T(V):=\{T(v): v \in V \}= Im(V).
\]
\end{obs}
El nucleo y la imagen son importantes entre otras cosas por lo siguiente \begin{teo}
 Sea $T : V \to W$ una transformacion lineal. Entonces \begin{list}{$\circ$}{}  
\item  $Im(T)$ es un subespacio vectorial de $W$.
\item $Nu(T)$ es un subespacio vectorial de $V$.
\end{list}
\end{teo}
A continuacion haremos la demostracion. 
\begin{demo}
  $Nu(T)$ es subespacio.
\begin{list}{$\circ$}{}  
\item $Nu(T)\neq \emptyset$ pues $T(0)=0$ y por lo tanto $ 0 \in Nu(T)$. 
\item Si $v,w, \in V$ tales que $T(v)=0$ y $T(w)=0$, entonces \begin{list}{$\bullet$}{}  
\item  $T(v+w)=T(v)+T(w)=0+0=0 \Rightarrow v+w \in Nu(T)$.
\item Si $\lambda \in \mathbb{K}$, entonces $T(\lambda v)=\lambda T(v)=\lambda \cdot 0 = 0 \Rightarrow \lambda v \in Nu(T)$.
\end{list}
\end{list}

\end{demo}
\begin{demo}$Im(T)$ es subespacio. 
\begin{list}{$\circ$}{}  
\item $Im(T)\neq \emptyset$, pues $0 = T(0) \in Im(T)$. 
\item Si $T(v_1), T(v_2) \in Im(T)$ y $\lambda \in \mathbb{K}$, entonces \begin{list}{$\bullet$}{}  
\item $T(v_1)+T(v_2)=T(v_1+v_2) \in Im(T)$.
\item $\lambda T(v_1)=T(\lambda v_1 ) \in Im(T)$.
\end{list}
\end{list}
\end{demo}
\pagebreak

\begin{lema}
  Sea $T: V \to W$ una transformacion lineal con $V$ de dimension finita. Sea $\{v_1, \dots ,v_k\}$ una base de $V$. Entonces $\{T(v_1), \dots ,T(v_k)\}$ genera a $Im(T)$ y por lo tanto $Im(T)$ es de dimension finita.
\end{lema}
\begin{demo}
  Por hipotesis: $V=\langle v_1, \dots ,v_k \rangle =\{\lambda_1 v_1 + \cdots + \lambda_k v_k \; | \; \lambda_1, \dots , \lambda_k \in \mathbb{K}\}.$ \[
    \begin{aligned}
      \text{Luego, }\; Im(T) &= \{T(v) \; | \; v \in V\} \\
                           &= \{T(\lambda_1 v_1 + \cdots + \lambda_k v_k) \; | \; \lambda_1, \dots, \lambda_k \in \mathbb{K}\} \\
                           &=\{\lambda_1 T(v_1)+\cdots + \lambda_k T(v_k) \; | \; \lambda_1, \dots ,\lambda_k \in \mathbb{K}\} \\
                           &= \langle T(v_1), \dots T(v_k)\rangle.
    \end{aligned}
  \]
  Entonces $Im(T)$ es generado por $S=\{T(v_1), \dots , T(v_k)\}$. Por Teorema 51, existe un subconjunto $\mathcal{B}$ de $S$ que es base de $Im(T)$. En particular, $Im(T)$ es de dimension finita. \\ \qed 
\end{demo}
Sea $T: V \to W$ una transformacion lineal y supongamos que $V$ es de dimension finita.  \\\\
Como $Nu(T)$ es un subespacio de un espacio dimension $< \infty \Rightarrow $ \\ $dim(Nu\; T) < \infty$. 
\begin{defi}
  Sea $T : V \to W$ una transformacion lineal y supongamos que $V$ es de dimension finita. Entonces 
  \begin{list}{$\circ$}{}  
\item El rango de $T$ es la dimension de $Im(T)$.
\item La nulidad de $T$ es la dimension de $Nu(T)$.
\end{list}
\end{defi}
Todas las transformaciones lineales entre $\mathbb{R}^n$ y $\mathbb{R}^m$ son de la forma ``multiplicar por una matriz''.\\\\ Mas aun, toda transformacion lineal entre espacios vectoriales de dimension finita se puede epresar de esta forma. \\\\
Asi que analicemos un poco mas en detalle este tipo de transformaciones. \pagebreak \begin{obs}
  Sea $A \in \mathbb{R}^{m \times n}$ y consideramos la funcion \[
    \begin{array}{lccc}
      T: & \mathbb{R}^n & \to & \mathbb{R}^m \\
         &v & \mapsto &Av.
    \end{array}
  \]
  Entonces $T$ es una transformacion lineal.
\end{obs}
\begin{demo}
  Debemos ver que $T$ respeta suma y producto por escalares. \\\\ Sean $v_1, v_2 \in \mathbb{R}^n$ y $\lambda \in \mathbb{R}$ entonces \[
T(v_1+
\lambda v_2) = A(v_1 + \lambda v_2 )= Av_1 + \lambda Av_2 =T(v_1 ) + \lambda T(v_2).
  \]
  \qed
\end{demo}
\begin{defi}
  Sea $A \in \mathbb{R}^{m \times n}$ y sea $T$ la transformacion lineal \[
    \begin{array}{lccc}
      T: & \mathbb{R}^n & \to & \mathbb{R}^m \\
         &v & \mapsto &Av.
    \end{array}
  \]
  Diremos que $T$ es la transformacion lineal asociada a $A$ o la transformacion lineal inducida por $A$. \\\\ 
  Muchas veces denotaremos a esta transformacion lineal con el mismo simbolo que la matriz, es decir, en este caso con $A$.
\end{defi}
\begin{ej}
  Consideremos la matriz $A=\begin{bmatrix}1 & 1 & 1 \\ 2 & 2 & 2 \end{bmatrix}$. \\\\ Entonces si $v = (x,y,z)$, \[
  A(v)=\begin{bmatrix}1 & 1 & 1 \\ 2 & 2 & 2 \end{bmatrix}\begin{bmatrix}x \\ y \\ z \end{bmatrix} = \begin{bmatrix} x + y + z \\ 2x + 2y + 2z \end{bmatrix}
\]
En particular, $(1,-1,0) \in Nu(A)$ pues $A(1,-1,0)=0$ y \[
  \begin{aligned}
    A(1,0,0)&=(1,2) \in Im(A) \\
    A(0,1,\pi )&= (1+\pi , 2 +2\pi) \in Im(A)
  \end{aligned}
\]
\end{ej}
\begin{obs}
  Sea $T : \mathbb{K}^n \to \mathbb{K}^m$ definida por \[
    T(x_1, \dots ,x_n)=(a_{11}x_1+\cdots + a_{1n}x_n , \; \dots \; , a_{m1}x_1+\cdots +a_{mn}x_n)
  \]
  con $a_{ij}\in \mathbb{K}$, \[
    T(x)=\begin{bmatrix}a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21 } & a_{22} & \cdots &a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}
  \]
  Es decir, $T$ es la transformacion lineal inducida por la matriz $A=\begin{bmatrix}a_{ij}\end{bmatrix}$. \\\\ Esto demuestra la observacion de la pagina 151.
\end{obs}
\begin{prop}
  Sea $A \in \mathbb{R}^{m \times n}$ y $T : \mathbb{R}^n \to \mathbb{R}^m$ la transformacion lineal asociada. Entonces 
\begin{list}{$\circ$}{}  
\item  El nucleo de $T$ es el conjunto de soluciones del sistema homogeneo \\ $AX=0$
\item La imagen de $T$ es el conjunto de los $b \in \mathbb{R}^m$ para los cuales el sistema $A=b$ tiene solucion
\end{list}
\end{prop}
\begin{demo}
  Se demuestra facilmente escribiendo las definiciones de los respectivos subconjuntos. \[
    v \in Nu(T) \Leftrightarrow Av=0 \Leftrightarrow v \text{ es solucion de }AX = 0.
  \]
  \[
    b \in Im(T) \Leftrightarrow \exists v \in \mathbb{R}^n \text{ tal que }Av=b \Leftrightarrow AX=b \text{ tiene solucion.}
  \]
  \qed
\end{demo}
\begin{ej}
 Sea $T : \mathbb{R}^3 \to \mathbb{R}^4$, definida \[
   T(x,y,z)=(x+y,x+2y+z,3y+3z,2x+4y+2z).
 \]
 \begin{enumerate}[label=(\arabic*)]
   \item Describir $Nu(T)$ en forma parametrica y dar una base. 
   \item Describir $Im(T)$ en forma parametrica y dar una base.
 \end{enumerate}
\end{ej}
\textbf{Solucion.} La matriz asociada a esta transformacion lineal es \[
  A=\begin{bmatrix}1 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 3 & 3 \\ 2 & 4 & 2 \end{bmatrix}
\]
Debemos encontrar la descripcion parametrica de \[
  \begin{aligned}
    Nu(T)&=\{v=(x,y,z) : A\cdot v = 0\} \\
    Im(T)&=\{y=(y_1,y_2,y_3,y_4) : \text{ tal que } \exists v \in \mathbb{R}^3 , A \cdot v = y\}
  \end{aligned}
\]
En ambos casos, la solucion depende de resolver el sistema de ecuaciones cuya matriz asociada es $A$:\[
  \begin{aligned}
    \begin{bmatrix}[ccc|c]
      1 & 1 & 0 &y_1 \\
      1 & 2 & 1 & y_2 \\
      0 & 3 & 3 & y_3 \\
      2 & 4 & 2 & y_4 
  \end{bmatrix}
  \underset{F_4-2F_1}{\xrightarrow{F_2-F_1}}
  &\begin{bmatrix}[ccc|c]
    1 & 1 & 0 & y_1 \\
    0 & 1 & 1 & -y_1+y_2 \\
    0 & 3 & 3 & y_3 \\ 
    0 & 2 & 2 & -2y_1 + y_4 
  \end{bmatrix} \\
  \underset{\begin{subarray}{l} F_3-3F_2 \\ F_4 - 2F_2 \end{subarray}}{\xrightarrow{F_1-F_2}}
  &\begin{bmatrix}[ccc|c]
    1 & 0 & -1 & 2y_1-y_2 \\
    0 & 1 & 1 & -y_1 + y_2 \\
    0 & 0 & 0 &  3y_1-3y_2 + y_3 \\
    0 & 0 & 0 & -2y_2+y_4
  \end{bmatrix},
  \end{aligned}
  \]\[
  T(x,y,z)=(y_1,y_2,y_3,y_4) \quad \Leftrightarrow \quad \begin{matrix}[rcl]
    x-z & = & 2y_1 - y_2 \\
    y+z & = & -y_1 + y_2 \\
    0 & = & 3y_1 -3y_2 + y_3 \\
    0 & = & -2y_2 + y_4 
  \end{matrix}
\]
Si hacemos $y_1 = _2 = y_3 = y_4 = 0$, entonces las soluciones del sistema describen el nucleo de $T$, es decir \[
  \begin{aligned}
    Nu(T) &= \{(x,y,z): x-z=0,y+z=0\}=\{(s,-s,s) : s \in \mathbb{R}\} \\
          &= \{s (1,-1,1) : s \in \mathbb{R}\}
  \end{aligned}
\]
que es la forma parametrica. \\ \\ Una base del nucleo de $T$ es $\{(1,-1,1)\}$. \\\\\\ Seguimos usando lo mismo que calculamos antes \[
  T(x,y,z)=(y_1,y_2,y_3,y_4) \quad \Leftrightarrow \quad \begin{matrix}[rcl]
    x-z & = & 2y_1 - y_2 \\
    y+z & = & -y_1 + y_2 \\
    0 & = & 3y_1 -3y_2 + y_3 \\
    0 & = & -2y_2 + y_4 
  \end{matrix}
\]
Luego, \[
  Im(T)=\{(y_1,y_2,y_3,y_4) : \text{ tal que }0=3y_1-3y_2+y_3 \text{ y } 0=-2y_2+y_4\}
\]
Resolviendo este sistema, obtenemos \[
  \begin{aligned}
  Im(T) &= \left\{\left(-\frac{1}{3}s+\frac{1}{2}t,\frac{1}{2}t,s,t\right) : s,t \in \mathbb{R}\right\} \\
        &= \left\{s\left(-\frac{1}{3},0,1,0\right)+t\left(\frac{1}{2},\frac{1}{2},0,1\right) : s,t \in \mathbb{R}\right\}
\end{aligned}
\]
Luego $\left\{ \left(-\frac{1}{3},0,1,0\right),\left(\frac{1}{2},\frac{1}{2},0,1\right)\right\}$ es una base de $Im(T).$ \\ \qed
\pagebreak

Los dos teoremas que vamos a ver a continuacion son muy fuertes, en el sentido que dan mucha informacion por si solos y que ademas seran de utilidad para estudiar transformaciones inyectivas, suryectivas y biyectivas. \\\\ Las demostraciones son elegantes, en el sentido que solo requieren que razonemos pegando algunas ideas y resultados pero sin trabajar en cuentas largas y tediosas. \\\\ Las demostraciones no son dificiles, pero requieren concentracion y maduracion de ideas y conceptos; hacer ejercicios ayuda a asimilarlos.\\\\
El siguiente resultado relaciona las dimensiones del nucleo y la imagen. \begin{teo}
  Sea $T : V \to W$ una transformacion lineal. Si $V$ es de dimension finita entonces \[
dim\;V=dim\; Nu(T)+dim\; Im(T)
  \]
\end{teo}
  Resultados como este muestran la potencia de estudiar estructuras abstractas que despues se pueden aplicar a casos concretos.

  \begin{demo}
    Sea $\{v_1, \dots ,v_k\}$ una base de $Nu(T)$. \\\\ Sea $\{v_1, \dots ,v_k, w_1, \dots ,w_m\}$ una base de $V$ obtenida completando la base de $Nu(T)$. \\\\ Si probamos que $\{T(w_1), \dots ,T(w_m)\}$ es una base de $Im(T)$ el teorema queda demostrado. Pues, de ser asi, deducimos que \[
      \begin{aligned}
        dim \; V &= |\{v_1, \dots ,v_k, w_1, \dots ,w_m\} | \\
                 &= |\{v_1, \dots ,v_k\}| \quad + \quad |\{w_1, \dots ,w_m\}| \\
                 &= dim\;Nu(T) \quad \phantom{,,}+ \quad  |\{T(w_1),\dots ,T(w_m)\}| \\
                 &= dim \; Nu(T)\quad \phantom{,,} + \quad dim\; Im(T)
      \end{aligned}
    \]
    Esto se probara a continuacion.
  \end{demo}

Queremos ver que $\{T(w_1), \dots ,T(w_m)\}$ genera $Im(T)$ y es $LI$.\\
\begin{demo}$\{T(w_1), \dots ,T(w_m)\}$ genera $Im(T)$.
  \[
    \begin{aligned}
      Im(T) &= \langle T(v_1), \dots ,T(v_k), T(w_1), \dots ,T(w_m) \rangle  \\ 
            &= \langle T(w_1) , \dots ,T(w_m) \rangle . 
    \end{aligned}
  \]
  La primera igualdad la vimos anteriormente y la segunda vale porque \\ $v_i \in Nu(T)$. 
\end{demo}
\pagebreak 

\begin{demo} $\{T(w_1), \dots , T(w_m) \}$ es $LI$: \\\\ 
  Sea $\lambda_1, \dots ,\lambda_m$ tales que \[
\lambda_1 T (w_1 ) + \cdots + \lambda_m T(w_m) = 0 
  \]
  debemos ver que $\lambda_1 = \cdots = \lambda_m = 0$ \\\\
  Ahora bien \[
T(\lambda_1 w_1 + \cdots + \lambda_ m w_m )= \lambda_1 T(w_1) + \cdots + \lambda_m T(w_m) = 0.
  \]
Es decir $\lambda_1 w_1 + \cdots + \lambda_m w_m \in Nu(T)$. \\\\ 
$\Rightarrow$ \[
\lambda_1 w_1 + \cdots + \lambda_m w_m = \mu _1 v_1 + \cdots + \mu _k v_k.
\]
Luego, \[
0=-\mu_1v_1- \cdots - \mu_k v_k + \lambda_1 w_1 + \cdots + \lambda_m w_m = 0 
\]
Dado que $\{v_1, \dots ,v_k , w_1, \dots ,w_m\}$ es $LI$, la igualdad \[
-\mu v_1 - \cdots - \mu_k v_k + \lambda_1 w_1 + \cdots + \lambda_m w_m = 0
\]
implica que \[
\mu_1 = \cdots = \mu_k = \lambda_1 = \cdots = \lambda_m = 0
\]
implica que \[
\mu_1 = \cdots = \mu_k = \lambda_1 = \cdots = \lambda_m = 0
\]
como queriamos ver. \\ \qed
\end{demo}
El siguiente lema es importante por si mismo y ademas sera necesario mas adelante. \pagebreak

\begin{lema}
Sea $A \in \mathbb{R}^{m \times n}$ y $R$ la $MERF$ equivalente a $A$. Entonces la dimension del conjunto de soluciones del sistema homogeneo $AX=0$ es igual a la cantidad de variables libres de $RX=0$.
\end{lema}
\textbf{Idea de la demostracion.} Sea $r$ el numero de filas no nulas de $R$ y $k_1, \dots ,k_r$ las columnas donde aparecen los $1$'s principales. \\\\ Entonces, $k_1 < k_2 < \cdots < k_r$ y el sistema de ecuaciones asociado a $R$ es: \[
  \begin{matrix}
    x_{k_1} & + & \sum_{j \neq k_1 ,\dots ,k_r } b_{1j}x_{j} & = & 0 \\
    x_{k_2} & + & \sum_{j \neq k_1 ,\dots ,k_r } b_{2j}x_{j} & = & 0 \\
   \vdots & & & \vdots \\
   x_{k_r} & + & \sum_{j \neq k_1 ,\dots ,k_r } b_{rj}x_{j} & = & 0 \\
  \end{matrix}
\]
Sean $x_{j_{1}},x_{j_{2}}, \dots ,x_{j_{n-r}}$ las $n-r$ variables libres \\(es decir los $x_j$ con $j \neq k_1, \dots , k_r)$ \\\\ Luego, 
\[
  \begin{matrix}
    x_{k_1} & = & -\sum_{j \neq k_1 ,\dots ,k_r }^{n-r} b_{1j_i}x_{j_i} &  \\
    x_{k_2} & = & -\sum_{j \neq k_1 ,\dots ,k_r }^{n-r} b_{2j_i}x_{j_i} &  \\
   \vdots & &  \vdots \\
   x_{k_r} & = & -\sum_{j \neq k_1 ,\dots ,k_r }^{n-r} b_{rj_i}x_{j_i} &  \\
  \end{matrix}
\]
Es decir, el subespacio formado por las soluciones de $AX=0$, consta de $n$-uplas $(x_1, \dots ,x_2 , \dots ,x_n)$, donde 
\begin{list}{$\circ$}{}  
\item $x_k=x_{j_{i}}$ para algun $i=1,\dots ,n-r$, o 
\item $x_k = $ c.l. de los $x_{j_{i}}$. 
\end{list}
Por lo tanto, \[
  W=\left\{\sum_{i=1}^{n-r}x_{j_{i}}w_{i}:x_{j_{1}}, \dots ,x_{j_{n-r}} \in \mathbb{K}\right\}
\]
para algunos $w_1, \dots ,w_{n-r}$ que son $LI$. \\\\ 
Luego $dim(W)=n-r$. \\ \qed

\begin{defi}
  Sea $A \in \mathbb{R}^{m \times n}$. \begin{list}{$\circ$}{}  
  \item  El rango fila de $A$ es la dimension del subespacio de $\mathbb{R}^n$ generado por las filas de $A$. 
  \item El rango columna de $A$ es la dimension del subespacio de $\mathbb{R}^m$ generado por las columnas de $A$.
\end{list}
\end{defi}
\begin{teo}
  Sea $A \in \mathbb{R}^{m \times n}$. El rango fila de $A$ es igual al rango columna de $A$. \\\\ Notar que si $n \neq m$, estamos comparando subespacios de distintos espacios vectoriales.
\end{teo}
\begin{demo} \; \\\\
  Consideremos la transformacion lineal $T: \mathbb{R}^n \to \mathbb{R}^{m}$ dada por la multiplicacion por $A$. \\\\ Es decir, $T(v)=Av$ para todo $v \in \mathbb{R}^n$ \\\\ La demostracion consiste en comparar el nucleo y la imagen de $T$ con los espacios fila y columna de $A$. \\\\ 
  Primero, el espacio columna de $A$ es gual a la imagen de $T$. \\\\ Esto es por la forma en que multiplicamos matrices: \[
    A=\begin{bmatrix}[cccc]| & | & & | \\ v_1 & v_2 & \cdots & v_n \\ | & | & & | \end{bmatrix} \quad \Rightarrow \quad T(e_i)=A \begin{bmatrix}0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{bmatrix}=v_i 
  \]
  Luego  \[
    T((\lambda_1, \dots ,\lambda_n))=T\left(\sum \lambda_i e_i\right)=\lambda_1 v_1 + \cdots + \lambda_nv_n
  \]
  $\Rightarrow$ \[
Im(T)=\langle v_1, \dots ,v_n \rangle.
  \]
  Entonces, rango columna de $A = dim \; Im(T)$.
\end{demo} 
\pagebreak

Segundo, por el Corolario 15 las filas no nulas de la $MERF$ equivalente a $A$ forman una base del espacio fila de $A$. \\\\
Por lo tanto, el rango fila de $A$ es igual a la cantidad de $1$'s principales de la $MERF$. O dicho de otro modo, 
\begin{list}{$\circ$}{}  
\item Rango fila de $A$ es igual a $n$ menos la cantidad de variables libres. \\ ($n$ es la cantidad de columnas de $A$.)
\end{list}
Por otro lado, el $Nu(T)$ es igual al conjunto de soluciones de $AX=0$. \\ \\ Entonces, por el lema anterior, 
\begin{list}{$\circ$}{}  
\item  $dim\; Nu(T)$ es igual a la cantidad de variables libres
\end{list}
En resumen: sea $r$ la cantidad de variables libres: \begin{enumerate}[label=(\arabic*)]
  \item Rango columna de $A$ es igual a $dim \; Im(T)$
  \item Rango fila de $A$ es igual a $n$ menos la cantidad de variables libres: $n-r$. 
  \item $dim\;Nu(T)$ es igual a la cantidad de variables libres: $r$. 
  \item $dim\; \mathbb{R}^n= n$.
\end{enumerate}
Por lo tanto, por el teorema de la dimension, 
  \begin{align}
    dim(\mathbb{K}^n) &= dim\;Nu(T) + dim\;Im(T) \notag \\
    n&=r+rgcol(A) \tag{por (4), (3) y (1)} \\
    n&=n-rgfil(A)+rgcol(A) \tag{por (2)} \\
    0&=-rgfil(A)+rgcol(A). \notag
  \end{align} \\ \qed \pagebreak

  A continuacion estudiaremos transformaciones lineales inyectivas, sobreyectivas y biyectivas. \\\\ Este tipo de transformaciones nos dan informacion acerca de dimensiones, generadores y conjuntos $LI$. \\\\ Sea $T:V \to W$ lineal. Se veran, entre otros resultados: \begin{list}{$\circ$}{}  
\item  $T$ es inyectiva $\Leftrightarrow$ $Nu(T)=0$ $\Leftrightarrow $ $dim \; Nu(T)=0$. 
\item $T$ es inyectiva $\Leftrightarrow$ $T$ de $LI$ es $LI$.
\item $T$ es sobreyectiva $\Leftrightarrow$ $T$ de generadores de $V$ es generadores de $W$. 
\item $T$ biyectiva $\Leftrightarrow$ $T$ de base es base.
\end{list}
\begin{defi}
  Sean $V$, $W$ espacios vectoriales sobre un cuerpo $\mathbb{K}$ y sea $T:V \to W$ una transformacion lineal.\begin{list}{$\circ$}{}  
\item $T$ es epimorfismo si $T$ es suryectiva.  \\
  Es decir si $Im(T)=W$.
\item $T$ es monomorfismo si $T$ es inyectiva (o $1-1$). \\
  Es decir, $T(v_1)=T(v_2)\Rightarrow v_1 = v_2$.
\item $T$ es un isomorfismo si es un monomorfismo y epimorfismo (es decir si es inyectiva y suryectiva).
\end{list}
\end{defi}
\begin{obs}\;
\begin{list}{$\circ$}{}  
\item $T$ es epimorfismo si y solo si \[
    T \text{ es lineal y }\forall w \in W, \; \exists v \in V \text{ tal que } T(v)=w. 
  \]
  Esto se deduce inmediatamente de la definicion de funcion suryectiva y de $Im(T)$.
\item $T$ es monomorfismo si y solo si \[
     T \text{ es lineal y } \forall v_1, v_2 \in V : v_1 \neq v_2 \Rightarrow T(v_1) \neq T(v_2).
  \]
  Esto se obtiene aplicando el contrarreciproco a la definicion de funcion inyectiva.
\end{list}
\end{obs} \pagebreak 

\begin{prop}
Sea $T : V \to W$ una transformacion lineal. Entonces $T$ es monomorfismo si y solo si $Nu(T)=0$.
\end{prop}
\begin{demo}\; \\\\
  ($\Rightarrow$) Debemos ver que $T(v)=0\Rightarrow v = 0$. \\\\ $T(v)=0 \; \land \; T(0)=0 \quad \overset{\text{T mono}}{\Longrightarrow} \quad v =0$.\\\\\\ 
  ($\Leftarrow$) Sean $v_1, v_2 \in V$ tal que $T(v_1)=T(v_2)$. Entonces \\\\ $0=T(v_1)-T(v_2)  \overset{\text{ T lineal }}{=} T(v_1 -v_2) \quad \Rightarrow \quad v_1 -v_2 \in Nu(T)=\{0\}. \\\\\text{ Luego, }v_1 -v_2 = 0,\text{ es decir }v_1 = v_2$. \\ \qed
\end{demo}
\begin{obs}
Sea $T : V \to W$ transformacion lineal, \begin{enumerate}[label=(\arabic*)]
  \item $T$ es epimorfismo $\Leftrightarrow$ $Im(T)=W \Leftrightarrow dim \; Im(T)=dim \; W$.
  \item $T$ es monomorfismo $\Leftrightarrow$ $Nu(T)=0 \Leftrightarrow dim \;Nu(T)=0$.
  \end{enumerate}\;
\end{obs} 
\pagebreak

\begin{prop}
  Sea $T : V \to W$ transformacion lineal. Entonces, \begin{enumerate}[label=(\arabic*)]
    \item $T$ es monomorfismo si y solo si $T$ de un conjunto $LI$ es $LI$. 
    \item $T$ es epimorfismo si y solo si $T$ de un conjunto de generadores de $V$ es un conjunto de generadores de $W$.
  \end{enumerate}
\end{prop}
\begin{demo} \; \\\\
  $(1)$ $(\Rightarrow)$ Sea $\{v_1, \dots ,v_n\} \; LI$ en $V$ y $\lambda_1 , \dots ,\lambda_n \in \mathbb{K}$ tales que \[
\lambda_1 T(v_1)+\cdots + \lambda_n T(v_n)=0.
  \]
  Debemos probar que $\lambda_1 = \lambda_2 = \cdots = \lambda_n = 0$. \[
    \begin{array}{llll}
      0=\lambda_1 T(v_1)+ \cdots + \lambda_n T(v_n) &&& \text{(hipotesis)} \\
      \phantom{0}=T(\lambda_1 v_1 + \cdots + \lambda_n v_n) &&& \text{(linealidad de $T$)} \\
      \phantom{0}\Rightarrow \lambda_1 v_1 + \cdots + \lambda_n v_n = 0 &&& \text{($T$ mono)} \\
      \phantom{0}\Rightarrow \lambda_1 = \lambda_2 = \cdots = \lambda_n = 0 &&& \text{($\{v_1, \dots ,v_n \}\; LI$)}
    \end{array}
  \]
  Por lo tanto, $T(v_1), \dots , T(v_n)$ son $LI$.
\end{demo} \; \begin{demo}\;\\\\
  $(1)$ $(\Leftarrow)$ Si $Nu(T)=0$ \; $\Rightarrow$ \; $T$ es mono (Proposicion 62.) \\\\ Veamos, entonces, que $Nu(T)=0$, es decir : $T(v)=0 \Rightarrow v=0$. \\\\ Probemos el contrearreciproco: $v \neq 0 \Rightarrow T(v) \neq 0$ \begin{align}
    v \neq 0 &\Rightarrow v \text{ es }LI \notag \\
             &\Rightarrow T(v) \text{ es }LI &&\text{(hipotesis)} \notag \\
             &\Rightarrow T(v) \neq 0 \notag
  \end{align}
  Luego, \begin{align}
    (v \neq 0 \Rightarrow T(v) \neq 0) &\Rightarrow (T(v)=0\Rightarrow v =0) \notag \\
                                       &\Rightarrow Nu(T)=0\notag  \\
                                       &\Rightarrow T \text{ es mono}. \notag 
  \end{align}
\end{demo}
\pagebreak

\begin{demo}\;\\\\
  ($2$) ($\Rightarrow$) Sea $V=\langle v_1, \dots ,v_n \rangle$ y $w \in W$. \\\\ Debemos ver que $w \in \langle T(v_1), \dots ,T(v_n)\rangle$. \\\\ Como $T$ es epimorfismo, existe $v \in V$ tal que $T(v)=W$. 
  \begin{align}
    v&=\lambda_1 v_1 + \cdots + \lambda_n v_n &&(v_1, \dots ,v_n \text{ genera } V) \notag \\
     &\Downarrow \notag \\
    T(v)&=T(\lambda_1 v_1 + \cdots + \lambda_n v_n) && (\text{aplicamos }T) \notag \\
        &= \lambda_1 T(v_1 ) + \cdots + \lambda_n T(v_n) && (T\text{ lineal}) \notag \\ 
        &\Downarrow \notag \\
    w&=\lambda_1 T(v_1) + \cdots + \lambda_n T(v_n) && (w=T(v)) \notag \\
     &\Downarrow \notag \\
    w &\in \langle T(v_1), \dots ,T(v_n)\rangle. \notag
  \end{align}
\end{demo}
\begin{demo}\;\\\\
  $(2)$ $(\Leftarrow)$ Debemos ver que: $w \in W \Rightarrow$ existe $v \in V$ tal que $w = T(v)$. \\\\ Sea $\{v_1, \dots ,v_n\}$ una base de $V$. \\\\ Por hipotesis $T(v_1), \dots ,T(v_n)$ generan $W$. \\\\ Es decir dado cualquier $w \in W$, existen $\lambda_1, \dots ,\lambda_n \in \mathbb{K}$ tales que \[
w =\lambda_1 T(v_1)+\cdots + \lambda_nT(v_n),
  \]
  y por lo tanto \begin{align}
    w & = \lambda_1 T(v_1) + \cdots + \lambda_nT(v_n) \notag \\
      &= T(\lambda_1v_1+ \cdots + \lambda_nv_n) && (T \text{ lineal}) \notag \\
      &= T(v), \notag 
  \end{align}
  con \[
v=\lambda_1 v_1 + \cdots + \lambda_n v_n.
  \]
  \qed
\end{demo}
\pagebreak

\begin{corol}
Sea $T : V \to W$ transformacion lineal. Entonces $T$ es un isomorfismo si y solo si $T$ de una base de $V$ es una base de $W$.
\end{corol}
\begin{demo}\;\\\\
  ($\Rightarrow$) Sea $\mathcal{B}$ base de $V$. Como $T$ es isomorfismo, $T$ es mono y epi, luego por Proposicion 63, $T(\mathcal{B})$ es $LI$ y genera $W$, es decir, es base de $W$. \\\\
  ($\Leftarrow)$ Sea $\mathcal{B}$ base de $V$ y $T: V \to W$ transformacion lineal tal que $T(\mathcal{B})$ es base. Por lo tanto, manda un conjunto $LI$ a un conjunto $LI$ y un conjunto de generadores de $V$ a un conjunto de generadores de $W$. Por Proposicion 63, $T$ es mono y epi, por lo tanto $T$ es un isomorfismo. \\ \qed 
\end{demo}
\begin{corol}
  Sean $V$ y $W$ dos $\mathbb{K}$-espacios vectoriales de dimension finita tal que $V$ es isomorfo a $W$. Entonces $dim(V)=dim(W)$.
\end{corol}
Recordar que si una funcion es biyectiva entonces se puede definir la funcion inversa. 
 \begin{teo}
   Sea $T : V \to W$ un isomorfismo. Entonces la funcion inversa \[
     T^{-1}: W \to V 
   \]
   es tambien un isomorfismo. \\\\ Es decir, $T^{-1}$ es una transformacion lineal biyectiva.
 \end{teo}
 \begin{demo}
   Sean $w_1, w_2 \in W, \lambda \in \mathbb{K}$ probemos que \[
     T^{-1}(w_1+ \lambda w_2) = T^{-1}(w_1)+\lambda T^{-1}(w_2).
   \]
   Sean $v_i=T^{-1}(w_i)\Rightarrow T(v_i)=w_i$. \begin{align}
     T^{-1}(w_1+ \lambda w_2) &= T^{-1}(T(v_1)+\lambda T(v_2)) && (w_i=T(v_i)) \notag \\ 
                              &= T^{-1}(T(v_1+\lambda v_2)) && (T \text{ lineal}) \notag \\
                              &= (T^{-1} \circ T)(v_1+\lambda v_2) && \text{(def de }\circ)  \notag \\ 
                              &= v_1 + \lambda v_2 && (T^{-1}\circ T=Id) \notag \\
                              &= T^{-1}(w_1)+\lambda T^{-1}(w_2). && v_i=T^{-1}(w_i)) \notag 
   \end{align} \qed

 \end{demo}
 \begin{teo}
 Sea $T : V \to W$ una transformacion lineal con $dim \; V = dim \; W$. Entonces las siguientes afirmaciones son equivalentes \begin{enumerate}[label=(\arabic*)]
   \item $T$ es un isomorfismo. 
   \item $T$ es monomorfismo. 
   \item $T$ es epimorfismo. 
   \item $\{v_1, \dots , v_n\}$ base de $V$ $\Rightarrow \{T(v_1), \dots ,T(v_n)\}$ base de $W$.
   \end{enumerate}
 \end{teo}
 Vamos a probar \begin{list}{$\circ$}{}  
\item $(1)\Rightarrow(2) \Rightarrow (3) \Rightarrow (1)$, 
\item $(1) \Rightarrow (4) \; \land \; (4) \Rightarrow (1)$.
\end{list}
Del primer item obtenemos $(1) \Leftrightarrow (2) \Leftrightarrow (3)$. \\\\ Del segundo item obtenemos $(1) \Leftrightarrow (4)$.
\\
\begin{demo}\; \\\\
  $(1) \Rightarrow (2)$. Obvio, de la definicion de iso. \\\\
  $(2) \Rightarrow (3)$. Usaremos el teorema de la dimension del nucleo y la imagen: \begin{align}
    T \text{ mono } & \Rightarrow dim\; Nu(T) = 0 && (\text{proposicion 62}) \notag \\
                    &\Rightarrow dim\; Im(T)=dim \; V = dim\; W && \text{(teorema de la dimension)} \notag \\
                    &\Rightarrow Im(T) = W \notag \\
                    &\Rightarrow T \text{ epi } 
  \end{align}
  $(3)\Rightarrow (1)$. \begin{align}
    T \text{ epi } &\Rightarrow dim \; Im(T)=dim\;V=dim\;W \notag \\
                   &\Rightarrow dim \; Nu(T)=0 && \text{(teorema de la dimension)} \notag \\
                   &\Rightarrow Nu(T)=0 \notag \\
                   &\Rightarrow T \text{ mono } && \text{(proposicion 62)}
  \end{align}
  $T$ epi y $T$ mono $\Rightarrow T$ iso.\pagebreak

  $(1) \Rightarrow (4)$. Sea $\{v_1, \dots ,v_n\}$ una base de $V$. \\\\ Entonces $\{v_1, \dots ,v_n\}$ es $LI$ y genera $V$. \begin{align}
    \text{Proposicion 63 } \; \Rightarrow \; \; &\{T(v_1, \dots ,T(v_n)\} \text{ es } LI \notag \\
                                             &\{T(v_1),\dots ,T(v_n)\} \text{ genera } W. \notag
  \end{align}
  Por lo tanto $\{T(v_1),\dots ,T(v_n)\}$ es una base de $W$. \\\\
  $(4) \Rightarrow (1)$. Como $T$ de una base es una base, entonces \begin{list}{$\circ$}{}  
\item   $T$ de un conjunto $LI$ es un conjunto $LI$. 
\item $T$ de un conjunto de generadores de $V$ es un conjunto de generadores de $W$.
\end{list}
Por lo tanto, por proposicion 63, $T$ es isomorfismo y epimorfismo. \\\\ Luego $T$ es un isomorfismo. \\ \qed
\end{demo}
\begin{defi}
  Dos espacios vectoriales $V$ y $W$ se dicen isomorfos, en simbolos $V \cong W$, si existe un isomorfismo $T : V \to W$. 
\end{defi}
\begin{corol}[Teorema 65]
  Sean $V$ y $W$ espacios vectoriales de dimension finita. Entonces \[
dim \; V = dim\; W \quad \Rightarrow \quad V \cong W.
  \]
\end{corol}
\begin{demo}[Idea] Si $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base de $V$ y $\mathcal{B}'=\{w_1,\dots ,w_m\}$ una base de $W$ \[
  T: \mathcal{B} \to \mathcal{B}' \quad \text{ definida por } T(v_i)=w_i,
\]
se puede extender a un ismorfismo $T : V \to W$. \qed
\end{demo}
\begin{ej}
Recordemos: \[
  \mathbb{K}_{n}[x]=\{a_{0}+a_{1}x+\cdots + a_{n-1}x^{n-1}: a_{0},a_1,\dots,a_{n-1} \in \mathbb{K}\}.
\]
Entonces, \[
  \mathbb{K}_{n}[x]\cong \mathbb{K}^{n}.
\]
\end{ej}
\begin{demo}
  Es consecuencia inmediata del corolario anterior, pues ambos tienen dimension $n$. \\\\ Explicitamente, $1, x, \dots , x^{n-1}$ es base de $\mathbb{K}_{n}[x]$ y sea $e_1, \dots ,e_n$ la base canonica de $\mathbb{R}^n$, entonces un isomorfismo de $\mathbb{K}_n[x]$ a $\mathbb{K}^n$ viene dado por la unica transformacion lineal $T : \mathbb{K}_n[x] \to \mathbb{K}^n $ tal que \[
    T(x^i)=e_{i+1}, \quad \quad i=0,\dots ,n-1.
  \] \qed
\end{demo}
\pagebreak
\begin{center}
\textbf{Resultados muy importantes (a tener en cuenta).}
\end{center}
Sea $T : V \to W$ una transformacion lineal con $V, W$ de dimension finita. 
\begin{list}{$\circ$}{}  
\item   $\{v_1, \dots ,v_k\}$ genera $V$ $\Rightarrow$ $\{T(v_1),\dots ,T(v_k)\}$ genera $Im(T)$.
\item $dim\; V = dim \; Nu(T)+dim \; Im(T)$. 
\item $T$ mono $\Leftrightarrow$ $Nu(T)=0$. 
\item $T$ mono $\Leftrightarrow$ $T$ de $LI$ es $LI$.
\item $T$ epi $\Leftrightarrow T$ de generadores de $V = $ generadores de $W$.
\item $T$ iso $\Leftrightarrow$ $T$ de base de $V = $ base de $W$.
 \end{list}
 Si $T : \mathbb{R}^n \to \mathbb{R}^m$, sea $A$ la matriz $m \times n$ asociada a $A$ y $R$ una $MRF$ de $A$. \begin{list}{$\circ$}{}  
 \item  $Nu(T)=\{x : Ax=0\}, \; Im(T)=\{b: Ax=b, \text{ para algun }x \}$. 
 \item rango fila de $A$ $=$ rango columna de $A$. 
 \item $|$ filas no nulas de $R$ $|$ $=$ $rg$-$fil\;A = rg$-$col \; A = dim \; Im(A)$.
 \item $dim \; Nu(A) = |$ variables libres de $RX=0$ $|$ $ = n-rg$-$fil\;A$.
\end{list}
\section{Matriz de una transformacion lineal.}
A continuacion introduciremos coordenadas respecto a una base ordenada y la matriz de una transformacion lineal respecto a dos bases ordenadas.\\\\
Hasta aqui hemos estudiado espacios vectoriales de manera general, abstrayendo las propiedades de $\mathbb{R}^n$.\\\\
Esto nos facilito deducir propiedades validas, no solo para $\mathbb{R}^n$, sino tambien para polinomios, matrices, funciones, etc. sin tener que probar las propiedades en cada caso. \\\\ Sin embargo, cada vez que queremos operar en ejercicios particulares, si recurrimos al auxilio de numeros concretos. Por ejemplo, hacemos esto cada vez que en lugar de usar un polinomio nos quedamos con sus coeficientes. \\\\ Estos numeros concretos que determinan de manera precisa a un vector es lo que llamaremos coordenadas. En el caso de $\mathbb{R}^n$, obtendremos las coordenadas usuales. \\\\ Las coordenadas nos permiten hacer mas tangibles los vectores de un espacio vectorial abstracto.
\begin{defi}
  Sea $V$ un espacio vectorial de dimension finita y $\mathcal{B}$ una base de $V$. Se dice que $\mathcal{B}$ es una base ordenada si los vectores que la forman estan ordenados.
\end{defi}
\begin{ej}
  La base canonica de $\mathbb{R}^n : \mathcal{C}=\{e_1,e_2,\dots ,e_n\}$ es una base ordenada. \\\\ Si cambiamos el orden tenemos otra base ordenada. Por ejemplo, \[
    \mathcal{C}'=\{e_n,e_{n-1},\dots ,e_2,e_1\}
  \]
  es otra base ordenada distinta a $\mathcal{C}$.
\end{ej}
El orden es importante para luego definir las coordenadas. \\\\ \begin{prop} \; \\\\
  Sea $V$ un espacio vectorial de dimension finita y $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$. Entonces para cada $v \in V$, existen unicos escalares $x_1, \dots,x_n \in \mathbb{R}$ tales que \[v=x_1v_1+\cdots + x_nv_n\]
\end{prop} \begin{demo}
Dichos escalares existen porque $\mathcal{B}$ genera a $V$. \\\\ Veamos que son unicos. \\\\ Sean $y_1, \dots ,y_n \in \mathbb{R}$ otros escalares que satisfacen el enunciado. \\\\ Entonces tenemos que $$x_1v_1+\cdots + x_nv_n=v=y_1v_1+\cdots +y_nv_n$$ 
Restando la sumatoria de la derecha a la de la izquierda obtenemos: \[
  (x_1-y_1)v_1+\cdots + (x_n-y_n)v_n=0. 
\]
Dado que $\mathcal{B}$ es $LI$, $x_i-y_i=0 \; (1\leq i \leq n)$, o dicho de otro modo \[
x_i=y_i \quad \quad (1 \leq i \leq n).
\]
\qed
\end{demo}
La proposicion 66. permite, dada una base ordenada, asociar a cada vector una $n$-tupla que seran las coordenadas del vector en esa base.
\begin{defi}\; \\
  Sea $V$ espacio vectorial de dimension finita y sea $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$, si $v \in V$ y \[
v=x_1v_1+\cdots + x_nv_n,
  \]
  entonces $x_i$ es la coordenada $i$-esima de $v$ y denotamos \[
    [v]_\mathcal{B}=(x_1,\dots ,x_n).
  \]
  El vector $[v]_\mathcal{B}$ es el vector de la coordenadas de $v$ respecto a la base $\mathcal{B}$.\\\\ Tambien nos sera util describir a $v$ como una matriz $ n \times 1$ y en ese caso hablaremos de la matriz de $v$ en la base $\mathcal{B}$: \[
    [v]_\mathcal{B}=\begin{bmatrix}x_1 \\ \vdots \\ x_n \end{bmatrix}.
  \]
  (Usamos la misma notacion.)
\end{defi}
\begin{obs}
  Recordemos siempre: \[
    v=x_1v_1+\cdots + x_nv_n \quad \quad \Longleftrightarrow \quad \quad [v]_\mathcal{B}=(x_1,\dots ,x_n).
  \]
\end{obs}
\begin{ej}
  Las coordenadas de $v \in \mathbb{R}^n$ con respecto a la base canonica $\mathcal{C}$ son las coordenadas usuales de $\mathbb{R}^n$. \\\\ En efecto, si $v = (x_1, \dots ,x_n) \in \mathbb{R}^n$ entonces \[
    [v]_\mathcal{C}=(x_1, \dots ,x_n) \quad \quad \Longleftrightarrow \quad \quad v=x_1 e_1+ \cdots + x_ne_n.
  \]
\end{ej} \pagebreak 

\begin{ej}
  Sea $\mathcal{B}=\{(1,-1),(2,3)\}$ una base ordenada de $\mathbb{R}^2$. Encontrar las coordenadas de $(1,0) \in \mathbb{R}^2$ en la base $\mathcal{B}$.
\end{ej}
\textbf{Solucion} \\\\ Debemos encontrar escalares $x_1, x_2 \in \mathbb{R}$ tales que \[
  (1,0)=x_1(1,-1)+x_2(2,3)
\]
Entonces tenemos que resolver el sistema \[
  \begin{dcases}
    x_1+2x_2 & = 1 \\
    -x_1+3x_2 & = 0
  \end{dcases}
\]
La solucion es $x_1=\frac{3}{5}$ y $x_2 = \frac{1}{5}$. Es decir \[
    [(1,0)]_{\mathcal{B}}=\left(\frac{3}{5},\frac{1}{5}\right)
  \] \qed
  \begin{obs}
    Las coordenadas determinan un unico valor. 
  \end{obs}
  \begin{ej}
    ¿Que vector de $\mathbb{R}^2$ tiene coordenadas $1$ y $2$ en la base ordenada $\mathcal{B}=\{(1,-1),(2,3)\}$? Dicho de otro modo, \\\\¿Que $v \in \mathbb{R}^2$ tiene coordenadas $[v]_\mathcal{B}=(1,2)$?
  \end{ej}
  \textbf{Respuesta} \\\\ $v = (5,5) \in \mathbb{R}^2$. \\\\ En efecto, si $1$ y $2$ s onlas coordenadas de $v$ en la base $\mathcal{B}$ quiere decir que \[
v=1(1,-1)+2(2,3)=(5,5)
  \] La siguiente simple observacion suele ser muy util y la usaremos mas adelante. \\\\ \begin{obs}
  Sea $V$ un espacio vectorial de dimension finita y $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$. \\\\ Las coordenadas de $v_i\in \mathcal{B}$ son \[
    [v_i]_\mathcal{B}=(0,\dots ,1,\dots ,0),
  \]
  es decir, todas $0$ salvo un $1$ en la $i$-esima coordenada. \\\\ Pues, $v_i=0v_1+\cdots +1v_i+\cdots + 0v_n$.
  \end{obs}
  Sea $V$ un espacio vectorial de dimension finita y $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$. \\\\ El vector de coordenadas $[v]_\mathcal{B}$ es un vector en $\mathbb{R}^n$ entonces lo podemos sumar y multiplicar por escalares. \\\\ A continuacion veremos como se relacionan estas operaciones con las operaciones propias del espacio vectorial $V$.
  \begin{prop}
    Sea $\mathcal{B}=\{v_1, \dots ,v_n\}$ una base ordenada de $V$ un $\mathbb{K}$-espacio vectorial.\\\\ Entonces 
  \begin{enumerate}[label=(\arabic*)]
    \item Las coordenadas de la suma es la suma de las coordenadas: \[
        [v+w]_\mathcal{B}=[v]_{\mathcal{B}}+[w]_{\mathcal{B}} \quad \forall v , w \in V.
      \]
    \item Las coordenadas del producto por un escalar es igual a multiplicar las coordenadas por el escalar: \[
        [\lambda v]_{\mathcal{B}}\quad \forall v \in V, \lambda \in \mathbb{K}.
      \]
  \end{enumerate}

      A continuacion la demostracion
  \end{prop}
  \begin{demo}\;
    \begin{enumerate}[label=(\arabic*)] 
       \item Si $v=x_1v_1+ \cdots + x_nv_n$ \; y \; $w=y_1v_1+\cdots + y_nv_n$, entonces \[
v+w=(x_1+y_1)v_1+\cdots + (x_n+y_n)v_n,
      \]
      luego \[
        [v+w]_\mathcal{B}=\begin{bmatrix}x_1+y_1 \\ \vdots \\ x_n + y_n \end{bmatrix} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} + \begin{bmatrix}y_1 \\ \vdots \\ y_n \end{bmatrix}=[v]_{\mathcal{B}}+[w]_{\mathcal{B}}.
      \]
      \item Si $v=x_1v_1+ \cdots + x_nv_n$ \; y \; $\lambda \in \mathbb{K}$, entonces \[
\lambda v = (\lambda x_1 ) v_1 + \cdots + (\lambda x_n) v_n,
        \]
        luego \[
          [\lambda v ]_\mathcal{B}=\begin{bmatrix}\lambda x_1 \\ \vdots \\ \lambda x_n \end{bmatrix} = \lambda \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix} = \lambda [v]_\mathcal{B}.
        \]
    \end{enumerate}
    \qed
  \end{demo}
  \pagebreak 
  
  Existe una forma general de pasar de las coordenadas de un vector en una base ordenada $\mathcal{B}$ a las coordenadas de otra base ordenada $\mathcal{B}'$ (teorema 3.5.3 del apunte). \\\\ La teoria que vamos a comenzar a desarrollar, que esencialmente es poner en coordenadas una transformacion lineal, nos servira para mirar las transformaciones lineales como matrices. \\\\ Esta teoria, la matriz de una transformacion lineal, permite obtener resultados muy interesantes y nos indica que nuestra intuicion de transformaciones lineales como matrices es la correcta. \\\\ Veremos mas adelante, entre otros resultados, que la formula general para cambio de coordenadas se deduce de un resultado mas general (teorema 4.5.5 del apunte).
  \begin{defi}
    Sean $V$ y $W$ espacios vectoriales de dimension finita con bases ordenadas $\mathcal{B}=\{v_1, \dots ,v_n\}$ y $\mathcal{B}'=\{w_1, \dots ,w_n\}$, respectivamente. Sea $T : V \to W$ una transformacion lineal tal que \[
      Tv_{j}=\sum_{i=1}^{m}a_{ij}w_{i}.
    \]
    A $A$ la matriz $m \times n$ definida por $\begin{bmatrix}A\end{bmatrix}_{ij}=a_{ij}$ se la denomina la matriz de $T$ respecto a las bases ordenadas $\mathcal{B}$ y $\mathcal{B}'$; y se la denota \[
    \begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B}'}=A.
  \]
  \end{defi}
  Notar que $\begin{bmatrix} T \end{bmatrix}_{\mathcal{B} \mathcal{B}'} \in \mathbb{R}^{m \times n}$ con $n = dim \; V$ y $ m = dim \; W$.

\begin{ej}
  Sea $T : \mathbb{K}^3 \to \mathbb{K}^2$ definida por \[
T(x,y,z)=(2x-z,-x+3y+z)
  \]
  y $\mathcal{B}=\{(1,1,1),(0,1,1),(0,0,1)\}$ base ordenada de $\mathbb{K}^3$. \\\\ Calculemos la matriz de $T$ en la base $\mathcal{B}$ y la base canonica $\mathcal{C}_2$ de $\mathbb{K}^2$ \[
    \begin{aligned}
      T(1,1,1)&=(1,3)&&=1e_1+3e_2 \\
      T(0,1,1)&=(-1,4)&&=(-1)e_1+4e_2 \\
      T(0,0,1)&=(-1,1)&&=(-1)e_1+1e_2.
    \end{aligned}
  \] 
  Luego  \[
    \begin{bmatrix} T \end{bmatrix}_{\mathcal{B}{\mathcal{C}_2}}=\begin{bmatrix} 1 & -1 & -1 \\ 3 & 4 & 1 \end{bmatrix}.
  \]
\end{ej}
\begin{obs}
  \[
    Tv_j=\sum_{i=1}^ma_{ij}w_{i} \quad \Leftrightarrow \quad \begin{bmatrix}Tv_j\end{bmatrix}_{\mathcal{B}'}=\begin{bmatrix}a_{1j} \\ a_{2j} \\ \vdots \\ a_{nj} \end{bmatrix}
  \]
  Luego, \[
    \begin{bmatrix} T \end{bmatrix}_{\mathcal{B} \mathcal{B}'} = \begin{bmatrix}[cccc] | & | & & | \\ 
    \begin{bmatrix} T v_1 \end{bmatrix}_{\mathcal{B}'} & \begin{bmatrix} T v_2 \end{bmatrix}_{\mathcal{B}'} & \cdots & \begin{bmatrix} T v_n \end{bmatrix}_{\mathcal{B}'} \\
  | & | & & | \end{bmatrix}
  \]
  es decir, las columnas son los vectores de coordenadas de $Tv_i \in W$ con respecto a la base $\mathcal{B}'$.
\end{obs}
\begin{ej}
  Sean $\mathcal{C}=\{e_1, e_2, e_3\}$ y $\mathcal{C} = \{e_1, e_2\}$ las bases canonicas de $\mathbb{R}^3$ y $\mathbb{R}^2$, respectivamente. \\\\(Por abuso de notacion, denotamos $\mathcal{C}$ la base canonica de $\mathbb{R}^2$ y $\mathbb{R}^3)$\\\\ Sea $T : \mathbb{R}^3 \to \mathbb{R}^2$ una transformacion lineal tal que \[
T(e_1)=(1,1), \quad T(e_2)=(1,2), \quad T(e_3) = (1,3).
  \]
  La matriz de $T$ con respecto a las bases canonicas de $\mathbb{R}^3$ y $\mathbb{R}^2$. \[
    \begin{bmatrix} T (e_1) \end{bmatrix}_{\mathcal{C}}=\begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad \begin{bmatrix} T (e_2) \end{bmatrix}_{\mathcal{C}}=\begin{bmatrix} 1 \\ 2 \end{bmatrix}, \quad  \begin{bmatrix} T (e_3) \end{bmatrix}_{\mathcal{C}}=\begin{bmatrix} 1 \\ 3 \end{bmatrix}.
  \]
  Entonces, \[
    \begin{bmatrix}T \end{bmatrix}_{\mathcal{C} \mathcal{C}} = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \end{bmatrix}.
  \]
\end{ej}
\begin{ej}
  Sea $T : \mathbb{K}^3 \to \mathbb{K}^2$ definida por \[
T(x,y,z) = (2x-z,-x+3y+z).
  \]
  Entonces la matriz en las bases canonicas de $\mathbb{K}^3$ y $\mathbb{K}^2$ es \[
    \begin{aligned}
      \begin{bmatrix}T \end{bmatrix}_{\mathcal{C}_3 \mathcal{C}_2} &= \begin{bmatrix}[cccc] | & | & | \\ \begin{bmatrix}T(e_1)\end{bmatrix}_{\mathcal{C}_{2}} & \begin{bmatrix}T(e_2)\end{bmatrix}_{\mathcal{C}_{2}} & \begin{bmatrix}T(e_3)\end{bmatrix}_{\mathcal{C}_{2}} \\ | & | & |\end{bmatrix} \\
                                                                   &= \begin{bmatrix} 
                                                                     \quad \; \phantom{-}2 & \quad \quad \;\; \quad 0 & \quad \quad \quad -1 & \phantom{{}_{1_{1}}}\\
                                                                   \quad  \;-1 & \quad \quad \quad \;\;  3 & \quad \quad \quad \phantom{-}1 & \phantom{{}_{1_{1}}}\end{bmatrix}
    \end{aligned}
  \]
\end{ej}

Esta es la transformacion lineal que hemos considerado en paginas anteriores donde vimos que era igual a la transformacion lineal ``multiplicar por la matriz $A=\begin{bmatrix}T \end{bmatrix}_{\mathcal{C}_{3},\mathcal{C}_2}$''.

\begin{ej}
  Sea $T : \mathbb{K}^3 \to \mathbb{K}^2$ definida por \[
T(x,y,z)=(2x-z,-x+3y+z) 
\] y $\mathcal{B}=\{(1,1,1),(0,1,1),(0,0,1)\}$ base ordenada de $\mathbb{K}^3$. \\\\ La matriz de $T$ en la base $\mathcal{B}$ y la base canonica $\mathcal{C}_2$ de $\mathbb{K}^2$ es \[
    \begin{aligned}
      \begin{bmatrix}T \end{bmatrix}_{\mathcal{C}_3 \mathcal{C}_2} &= \begin{bmatrix}[cccc] | & | & | \\ \begin{bmatrix}T(1,1,1)\end{bmatrix}_{\mathcal{C}_{2}} & \begin{bmatrix}T(0,1,1)\end{bmatrix}_{\mathcal{C}_{2}} & \begin{bmatrix}T(0,0,1)\end{bmatrix}_{\mathcal{C}_{2}} \\ | & | & |\end{bmatrix} \\
                                                                   &= \begin{bmatrix} 
                                                                     \quad \; \phantom{-}\;\;1 & \quad \quad \quad \;\; \quad -1 & \quad \quad \quad \;\;\;\; -1 & \phantom{{}_{1_{1}}}\\
                                                                   \quad  \;\phantom{-}\;\;3 & \quad \quad \quad \quad \;\; \phantom{-} 4 & \quad  \; \quad \quad \;\;\; \phantom{-}1 & \phantom{{}_{1_{1}}}\end{bmatrix}
    \end{aligned}
\]
\end{ej}
\begin{prop}
  Sean $V$ y $W$ espacios vectoriales de dimension finita con bases ordenadas $\mathcal{B}=\{v_1, \dots ,v_n\}$ y $\mathcal{B}'=\{w_1, \dots ,w_m\}$, respectivamente. \\\\ Si $T : V : V \to W$ es una transformacion lineal, entonces \[
  \begin{bmatrix}T(v)\end{bmatrix}_{\mathcal{B'}}
  =  
  \begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B}'}
\begin{bmatrix}
v
\end{bmatrix}_{\mathcal{B}}
\]
  para todo $v \in V$.
\end{prop}
En palabras, el vector de coordenadas de $T(v)$ en la base $\mathcal{B}'$  es igual a multiplicar la matriz de $T$ en las bases $\mathcal{B}$ y $\mathcal{B}'$ por el vector de coordenadas de $v$ en la base $\mathcal{B}$.
\\\\ 
La demostracion sigue los mismos pasos del siguiente ejemplo pero escribiendo todo en forma mas abstracta: con letras y subindices en lugar de numeros concretos.
\begin{ej}
  Sean las bases ordenadas de $\mathbb{R}^3$ \[
    \mathcal{B}=\{(1,0,-1),(0,1,-1),(1,1,0)\} \quad \textbf{\text{y}} \quad \mathcal{B}'=\{e_1,e_2,e_3\}.
  \]
  Sea $T : \mathbb{R}^3 \to \mathbb{R}^2$ definida \[
T(x,y,z)=(x-2y+z,x+y,-x-y+z),
  \]
  y sea $v = (-4,4,6)$. Veamos que \[
    \begin{bmatrix}
T(v)
\end{bmatrix}_{\mathcal{B}'}
=
\begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B}'}
\begin{bmatrix}
v
\end{bmatrix}_{\mathcal{B}}
  \]
\end{ej}
\textbf{Solucion}\\\\
Primero debemos calcular (1) $    \begin{bmatrix}
T(v)
\end{bmatrix}_{\mathcal{B}'}$,  (2) $\begin{bmatrix}
v
\end{bmatrix}_{\mathcal{B}}$  y  (3) $\begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B}'}$. \pagebreak

Aplicando la definicion de $T$, calculamos \[
T(-4,4,6)=(-6,0,6)=-6(1,0,0)+0(0,1,0)+6(0,0,1).
\]
Es decir \begin{equation}
  \begin{bmatrix} T(v) \end{bmatrix}_{\mathcal{B}'}=\begin{bmatrix}-6 \\ 0 \\ 6 \end{bmatrix}.  \tag{1}
\end{equation}
Por otro lado \[
  (-4,4,6)=-6(1,0,-1)+1(0,1,-1)+3(1,1,0),
\]
Luego \begin{equation}
  \begin{bmatrix}
v
\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}-7 \\ 1 \\ 3\end{bmatrix}. \tag{2}
\end{equation}
Finalmente \[
  \begin{aligned}
    T(1,0,-1) &= (0,1,-2) \\
    T(0,1,-1) &= (-3,1,-2) \\
    T(1,1,0) &= (-1,2,-2) 
  \end{aligned}
\]
Luego \begin{equation}
  \begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B'}}=\begin{bmatrix} 0 &  -3 & 1 \\ 1 & 1 & 2 \\ -2 & -2 & -2 \end{bmatrix}. 
\tag{3}
\end{equation}
$\Rightarrow$
\[ 
  \begin{aligned}
  \begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B'}} 
\begin{bmatrix} v \end{bmatrix}_{\mathcal{B}}
&=
\begin{bmatrix}
0 & -3 & 1 \\ 1 & 1 & 2 \\ -2 & -2 & -2 
\end{bmatrix}
  \begin{bmatrix}-7 \\ 1 \\ 3 \end{bmatrix} 
  =
  \begin{bmatrix}-6 \\ 0 \\ 6 \end{bmatrix}
                    &=
                    \begin{bmatrix} T(v) \end{bmatrix}_\mathcal{B'}.
\end{aligned}
\]
\qed \pagebreak

¿Por que se cumple esta ``magica'' igualdad? \\\\ Escribiendo $v$ como combinacion lineal de la base $\mathcal{B}'$ y luego calculando $T(v)$ se desvela el ``misterio''. En nuestro caso \[
  \begin{aligned}
v =(-4,-4,6)=-7(1,0,-1)+1(0,1,-1)+3(1,1,0), 
\end{aligned}\]
  $\Rightarrow$ \[
    \begin{bmatrix}v \end{bmatrix}_{\mathcal{B}}=(-7,1,3)
  \]
  Luego
  \[
    \begin{aligned}
      T(-4,4,6) &= T(-7(1,0,-1))+1(0,1,-1)+3(1,1,0)) \\
                &= -7T(1,0,-1)+1T(0,1,-1)+3T(1,1,0) \\
                &= -7(0,1,2)+1(-3,1,-2)+3(-1,2,-2) \\
                &= \begin{bmatrix}[llllll] -7 \cdot 0 & + & 1 \cdot (-3) & +& 3 \cdot (-1) \\ -7 \cdot 1 & +& 1 \cdot 1 & +& 3 \cdot 2 \\ -7 \cdot 2 & + & 1 \cdot (-2) & + & 3 \cdot (-2) \end{bmatrix} = \begin{bmatrix} T (v) \end{bmatrix}_{\mathcal{B'}}
    \end{aligned}
  \]
  Repasando \[
    \begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B'}}=\begin{bmatrix}0 & -3 & 1 \\ 1 & 1 & 2 \\ -2 & -2 & -2 \end{bmatrix},
  \]
  \[
    \begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}-7 \\ 1 \\ 3 \end{bmatrix}
    \]\[
\begin{bmatrix} T (v) \end{bmatrix}_{\mathcal{B'}}
= \begin{bmatrix}[llllll] -7 \cdot 0 & + & 1 \cdot (-3) & +& 3 \cdot (-1) \\ -7 \cdot 1 & +& 1 \cdot 1 & +& 3 \cdot 2 \\ -7 \cdot 2 & + & 1 \cdot (-2) & + & 3 \cdot (-2) \end{bmatrix}   \]
Es decir \[
  \underbrace{\begin{bmatrix}0 & -3 & 1 \\ 1 & 1 & 2 \\ -2 & -2 & -2 \end{bmatrix}}_{\begin{bmatrix}
T
\end{bmatrix}_{\mathcal{B}\mathcal{B'}}} 
  \underbrace{\begin{bmatrix}-7 \\ 1 \\ 3 \end{bmatrix}
}_{\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}}
=
\underbrace{\begin{bmatrix}[llllll] -7 \cdot 0 & + & 1 \cdot (-3) & +& 3 \cdot (-1) \\ -7 \cdot 1 & +& 1 \cdot 1 & +& 3 \cdot 2 \\ -7 \cdot 2 & + & 1 \cdot (-2) & + & 3 \cdot (-2) \end{bmatrix} }_{\begin{bmatrix} T (v) \end{bmatrix}_{\mathcal{B'}}} 
\]
\pagebreak

\begin{obs}
\end{obs}
Con solo conocer cuanto vale la transformacion en una base conocemos cuanto vale en todo el espacio.\\\\ En efecto, la matriz de la transformacionj la armamos calculando la transformacion en los vectores de una base. Y la proposicion anterior nos dice que para calcular la transformacion en un vector cualquiera debemos multiplicar por esa matriz. \\\\ 
Tambien vale lo siguiente. 
\begin{teo}
  Sean $V$ un espacio vectorial de dimension finita sobre el cuerpo $\mathbb{K}$ y $\{v_1, \dots ,v_n\}$ una base ordenada de $V$. Sean $W$ un espacio vectorial sobre el mismo cuerpo y $\{w_1, \dots ,w_n\}$, vectores cualesquiera de $W$. Entonces existe una unica transformacion lineal $T$ de $V$ en $W$ tal que \[
T(v_j)=w_j, \quad j=1, \dots ,n.
  \]
\end{teo}
\begin{corol}[Prop. 68.]
  Sea $V$ un espacio vectorial de dimension finita sobre el cuerpo $\mathbb{K}$, sean $\mathcal{B}$, $\mathcal{B
  }$ bases ordenadas de $V$. Entonces \[
    \begin{bmatrix}
v
\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}'\mathcal{B}}\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}'}, \quad \forall v \in V.
  \]
\end{corol}
\begin{demo}
  Por la Proposicion 68 tenemos que \[
    \begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}'\mathcal{B}}\begin{bmatrix}v \end{bmatrix}_{\mathcal{B'}}=\begin{bmatrix}Id(v)\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}.
  \]
  \qed
\end{demo}
\begin{defi}
  Se $V$ un espacio vectorial de dimension finita sobre el cuerpo $\mathbb{K}$ y sean $\mathcal{B}$ y $\mathcal{B}'$ bases ordenadas de $V$. La matriz $P=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}$ es llamada la matriz de cambio de base de la base $\mathcal{B'}$ a la base $\mathcal{B}$.
\end{defi}\pagebreak 

\begin{teo}
Sean $V,W$ y $Z$ espacios vectoriales de dimension finita con bases $\mathcal{B}$, $\mathcal{B'}$ y $\mathcal{B''}$, respectivamente. \\\\
Sean $T: V \to W$ y $U : W \to Z$ transformaciones lineales. \\\\ Entonces la matriz de la transformacion lineal \[
  UT : V \to Z, \quad \underset{\mathcal{B}}{V} \overset{T}{\to} \underset{\mathcal{B'}}{W} \overset{U}{\to}\underset{\mathcal{B''}}{Z}
\]
es decir la composicion de $T$ con $U$, satisface \[
  \begin{bmatrix}UT\end{bmatrix}_{\mathcal{B}{\mathcal{B''}}}=\begin{bmatrix}U\end{bmatrix}_{\mathcal{B'}\mathcal{B''}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B'}}
\]
(multiplicacion de matrices)
\end{teo} 
\begin{corol}
  Sea $V$ un espacio vectorial de dimension finita sobre el cuerpo $\mathbb{K}$ y sean $\mathcal{B}$ y $\mathcal{B'}$ bases ordenadas de $V$. La matriz de cambio de base $P=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}'\mathcal{B}}$ es invertible y su inversa es $P^{-1}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B'}}$.
\end{corol}
\begin{demo}
  \[
    \begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B'}} P = \begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B'}} \cdot \begin{bmatrix}Id \end{bmatrix}_{\mathcal{B'}\mathcal{B}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B'}}=Id
  \]
  \qed
\end{demo}
\textbf{Notacion}\\\\ Una transformacion lineal que va de un espacio en si mismo, es decir $T: V \to V$, se llama operador lineal. \\\\ Si $\mathcal{B}$ es una base de $V$, $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}$ denota la matriz de $T$ en la base $\mathcal{B}$ y $\mathcal{B}$, o sea la base de salida y llegada es la misma, entonces usamos un solo subindice.
\begin{corol}
Sea $V$ un espacio vectorial de dimension finita con bases $\mathcal{B}$ y $U,T : V \to V$ dos transformaciones lineales. Entonces \begin{enumerate}[label=(\arabic*)]
  \item $\begin{bmatrix}UT\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}U\end{bmatrix}_{\mathcal{B}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}$
  \item $T$ es un ismorfismo si y solo si $\begin{bmatrix}T\end{bmatrix}_\mathcal{B}$ es una matriz invertible. En tal caso \[
    \begin{bmatrix}T^{-1}\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}T\end{bmatrix}^{-1}_{\mathcal{B}}
  \]
\end{enumerate}
\end{corol} \pagebreak

Los espacios vectoriales no tienen una base ``natural'' es decir una que es mas importante que otras. Cuando trabajamos con bases estamos haciendo una eleccion y hay infinitas elecciones posibles.\\\\ El siguiente teorema nos dice como se relacionan las matrices de una transformacion lineal respecto a distintas bases. \\\\ Sea $V$ un espacio vectorial de dimension finita sobre el cuerpo $\mathbb{K}$ y saean \[
  \mathcal{B}=\{v_1, \dots ,v_n\}, \quad \quad \mathcal{B'}=\{w_1, \dots ,w_n\}
  \]bases ordenadas de $V$. Sea $T$ un operador lineal sobre $V$. Entonces, si $P$ es la matriz de cambio de base de $\mathcal{B'}$ a $\mathcal{B}$, se cumple que \[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{B}'}=P^{-1}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}P.
\]
Es decir \[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}.
\] \; 
\begin{demo}
  Tenemos que $T=Id \; T$ \; y \; $T=T \; Id$, luego \begin{align}
    \begin{bmatrix}T \end{bmatrix}_{\mathcal{B'}\mathcal{B'}} &= \begin{bmatrix}Id \; T \end{bmatrix}_{\mathcal{B'}\mathcal{B'}} \notag  \\
                 &= \begin{bmatrix}Id \end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}\mathcal{B}}\tag{teorema 70} \\
                 &= \begin{bmatrix}Id \end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T \; Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}} \notag \\
                 &= \begin{bmatrix}Id \end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}'\mathcal{B}} \tag{teorema 70}. 
  \end{align} \\
  Por lo tanto $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}=P^{-1}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B}}P$.
\end{demo}
\qed \\\\ 
Las formulas \begin{align}
  &\begin{bmatrix}UT\end{bmatrix}_{\mathcal{B}{\mathcal{B''}}}=\begin{bmatrix}U\end{bmatrix}_{\mathcal{B'}\mathcal{B''}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B'}} \tag{0} \\
  &\begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}} = \begin{bmatrix}Id \end{bmatrix}_{\mathcal{B}\mathcal{B'}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}{\mathcal{B}}} \tag{*} \\
  &\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{B}'}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}{\mathcal{B}}}=Id \tag{**} \\
  &\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}\begin{bmatrix}v\end{bmatrix}_{\mathcal{B'}} \tag{***}
\end{align}
son importantes por si mismas y debemos recordarlas. \\\\ Como ya dijimos, la matriz $P = \begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}$ es llamada la matriz de cambio de base de la base $\mathcal{B}'$ a la base $\mathcal{B}$. \\\\ La matriz de cambio de base nos permite calcular los cambios de coordenadas de los vectores y los cambios de base de las transformaciones lineales.
\begin{obs}
  Sea $T : \mathbb{K}^n \to \mathbb{K}^n$ operador lineal, $\mathcal{B}=\{v_1, \dots ,v_n\}$ base ordenada y $\mathcal{C}$ la base canonica, entonces \[
    \begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{C}}=\begin{bmatrix}T(v_1) & T(v_2) & \cdots & T(v_n) \end{bmatrix}.
  \]
\end{obs}
\begin{obs}
  Pudimos probar el teorema de cambio de base usando adecuadamente el Teorema 70, es decir la formula \[
    \begin{bmatrix}UT\end{bmatrix}_{\mathcal{B}\mathcal{B''}} = \begin{bmatrix}U\end{bmatrix}_{\mathcal{B'}\mathcal{B''}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}\mathcal{B'}}
  \]
  Con igual argumento podemos deducir otras igualdades que son utiles para armar todas las matrices a partir de matrices asociadas a bases canonicas, que, como dijimos en la observacion anterior, es facil calcularlas.
\end{obs}
\begin{obs}
  Sea $T : \mathbb{R}^n \to \mathbb{R}^n$ una transformacion lineal. \\\\ Sean $\mathcal{B}$ y $\mathcal{B'}$ bases de $\mathbb{R}^n$. \\\\ Entonces \[
    \begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}\mathcal{B}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{C}\mathcal{B}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{C}\mathcal{C}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{C}}=\begin{bmatrix}Id\end{bmatrix}^{-1}_{\mathcal{B}\mathcal{C}}\begin{bmatrix}T\end{bmatrix}_{\mathcal{C}\mathcal{C}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{C}}
  \]
  En palabras, ``para ir de $\mathcal{B'}$ a $\mathcal{B}$ con $T$, primero vamos de $\mathcal{B'}$ a $\mathcal{C}$, despues de $\mathcal{C}$ a $\mathcal{C}$ con $T$ y finalmente vamos de $\mathcal{C}$ a $\mathcal{B''}$''.\\\\ Las matrices $\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B}\mathcal{C}}$ y $\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{C}}$ son faciles de calcular, ponemos los vectores de $\mathcal{B}$ y $\mathcal{B'}$ como columnas. Similarmente, la matriz de $T$ en la base canonica tambien es facil de calcular.
\end{obs}
\begin{obs}
  Sean $\mathcal{B}$ y $\mathcal{B'}$ dos bases de $\mathbb{R}^n$. \\\\ Entonces la matriz de cambio de base de $\mathcal{B'}$ a $\mathcal{B}$ es \[
    \begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'}\mathcal{B}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{C}\mathcal{B}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'C}}=\begin{bmatrix}Id\end{bmatrix}^{-1}_\mathcal{BC}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B'C}}
  \]En palabras, ``para ir de $\mathcal{B'}$ a $\mathcal{B}$, primero vamos de $\mathcal{B'}$ a $\mathcal{C}$ y despues vamos de $\mathcal{C}$ a $\mathcal{B}$'' \\\\ Las matrices $\begin{bmatrix}Id\end{bmatrix}_{BC}$ y $\begin{bmatrix}Id\end{bmatrix}_{B'C}$ son faciles de calcular, ponemos los vectores de $\mathcal{B}$ y $\mathcal{B'}$ como columnas.
\end{obs}
\pagebreak

\section{Diagonalizacion} 
\begin{enumerate}[label=$\circ$]
  \item A continuacion veremos los autovalores y autovectores desde una perspectiva de las transformaciones lineales.
  \item Muchos conceptos y demostraciones se repiten o son similares al caso de las matrices. 
  \item Sea $V$ espacio vectorial de dimension finita. Un operador lineal en $V$ es diagonalizable si existe una base ordenada $\mathcal{B}=\{v_1, \dots ,v_n\}$ de $V$ y $\lambda_1, \dots ,\lambda_n \in \mathbb{K}$ tal que \[
T(v_i)=  \lambda_i v_i , \quad \quad 1 \leq i \leq n.
    \]
    ($T$ diagonalizable $\Leftrightarrow \exists$ base de autovectores)
  \item En general, los operadores diagonalizables permiten hacer calculos sobre ellos en forma sencilla, por ejemplo el nucleo del operador anterior es $Nu(T)=\langle v_i : \lambda_i = 0 \rangle $ y su imagen es $Im(T)=\langle v_i : \lambda_i \neq 0 \rangle $. 
  \item Otra propiedad importante de los operadores diagonalizables es que la matriz de la transformacion lineal en una base adecuada es diagonal (de alli viene el nombre de diagonalizable). 
    \[
      \begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix} \lambda_1 & 0 & 0 & \cdots & 0 \\ 0 & \lambda_2 & 0 & \cdots & 0 \\ 0 & 0 & \lambda_3 & \cdots & 0 \\ \vdots & \vdots & \vdots && \vdots \\ 0 & 0 & 0 & \cdots & \lambda_n \end{bmatrix}.
    \]
\end{enumerate}
\begin{defi}
  Sea $T : V \to V$ una transformacion lineal. Un autovalor de $T$ es un escalar $\lambda \in \mathbb{R}$ tal que existe un vector no nulo $v \in V$ con \[
T(v)=\lambda v
  \]
  En tal caso, se dice que $v$ es un autovector (asociado a $\lambda$). \\\\ El autoespacio asociado a $\lambda$ es \[
    V_\lambda = \{v \in V \;|\; T (v) = \lambda v\} = \{ \text{autovectores asociados a }\lambda \} \cup \{0\}
  \]
\end{defi}\pagebreak

\begin{lema}
  Sea $T : V \to V$ una transformacion lineal y $\mathcal{B}$ una base de $V$. Las siguientes afirmaciones son equivalentes \begin{enumerate}[label=$\circ$]
    \item $\lambda \in \mathbb{R}$ y $v \in V$ son autovalor y autovector de $T$
    \item $v \in Nu(T-\lambda \; Id)$
    \item $\lambda \in \mathbb{R}$ y $\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}$ son autovalor y autovector de $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}$
  \end{enumerate}
\end{lema}
\begin{demo}
  \[
    \begin{aligned}
      T(v) &= \lambda v \Leftrightarrow (Tv)-\lambda v=(T-\lambda \; Id)v=0 \\\\
      T(v) &= \lambda v \Leftrightarrow \lambda \begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}\lambda v\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}T(v)\end{bmatrix}_{\mathcal{B}}=\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}\begin{bmatrix}v\end{bmatrix}_{\mathcal{B}}
    \end{aligned}
  \]\qed
\end{demo}
\textbf{Consecuencia} \begin{enumerate}[label=$\circ$]
  \item Para calcular los autovalores y autovectores de una transformacion $T$, elegimos una base $\mathcal{B}$ y calculamos los autovalores y autovectores de $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}$. 
  \item No importa que base elijamos porque $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B'}}=P^{-1}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}P$ y en el Ejercicio 8 del 1er TP vimos que estas matrices tienen igual autovalores.
\end{enumerate}\pagebreak

\begin{teo}
  Sea $T : V \to V$ una transformacion lineal. Entonces $V_\lambda$ es un subespacio vectorial.  \end{teo}
  La demostracion es como en el caso de matrices.\\\\
Notar que podemos definir $V_\lambda$ para cualquier $\lambda \in \mathbb{R}$. Asi, $\lambda$ es autovalor si y solo si $V_\lambda \neq 0$ 
\begin{teo}
  Sea $T : V \to V$ una transformacion lineal. \\\\ Sean $v_1, \dots ,v_m$ autovectores de $T$ con autovalores $\lambda_1, \dots ,\lambda_m$, respectivamente. \\\\ Si todos los autovalores son distintos, entonces los vectores $v_1, \dots ,v_m$ son $LI$.
\end{teo}
\begin{demo}
  La demostracion es por induccion. \\\\ Caso base. Si $m=1$, entonces vale porque los autovectores son no nulos por definicion y en tal caso el conjunto $\{v_1\}$ es $LI$. \\\\ Paso inductivo. Para $m+1$ procedemos como sigue. \\\\ Sean $c_1, \dots ,c_{m+1}$ escalares tales que \begin{equation}
    c_1 v_1 + \cdots + c_m v_m + c_{m+1} v_{m+1}=0 \tag{*}
\end{equation}
Si aplicamos $T$ y multiplicamos por $\lambda_{m+1}$ esta igualdad obtenemos \[
  \begin{aligned}
    T(*) &\leadsto c_1 \lambda_1 v_1 + \cdots + c_m \lambda_m v_m + c_{m+1}\lambda_{m+1} v_{m+1} = 0 \\\\
    \lambda_{m+1} \cdot (*) & \leadsto c_1 \lambda_{m+1} v_{1} + \cdots + c_{m} \lambda_{m+1}v_{m}+c_{m+1}\lambda_{m+1}v_{m+1}=0
  \end{aligned}
\]
Restando miembro a miembro obtenemos \[
c_1(\lambda_1 - \lambda_{m+1})v_1+\cdots + c_m (\lambda_m - \lambda_{m+1})v_m = 0
\]
Por (HI), $c_i(\lambda_i - \lambda_{m+1})=0$ para todo $1 \leq i \leq m$. \\\\ dado que los autovalores son distintos, $c_i=0$ para todo $1 \leq i \leq m$. Por lo tanto $c_{m+1}=0$ y los vectores son $LI$.
\end{demo}
\qed
\\\\
Se preguntaran como saber si una transformacion lineal es diagonalizable. \\\\ La primera respueta es: calcular todos los autovalores y autovectores. \\\\ A continuacion veremos algunos criterios a tener en cuenta. 
\begin{corol}\; \\\\
  Si $V$ es un espacio vectorial de dimension $n$ y $T : V \to V$ es una transformacion lineal con $n$ autovalores distintos entonces $T$ es diagonalizable.
\end{corol}
\begin{demo}
  En efecto, cada autovalor tiene al menos un autovector. \\\\ Elijamos un autovector $v_1, \dots ,v_n$ para cada autovalor. \\\\ Por el Teorema 72 estos vectores son $LI$. \\\\ Dado que son tantos como la dimension de $V$ forman una base. \\ \qed
\end{demo}
\begin{corol}
  Si $V$ es un espacio vectorial de dimension $n$ y $T : V \to V$ es una transformacion lineal con autovalores $\lambda_1, \dots , \lambda_m$. Si \[
    dim \; V = dim \; V_{\lambda_{1}}+ \cdots + dim \; V_{\lambda_{m}}
  \]
\end{corol}
\begin{demo}
  La demostracion es similar a la anterior. \\\\ Primero elegimos una base para cada autoespacio. \\\\ Despues vemos que la union de estas bases es un conjunto $LI$ gracias al Teorema 72. \\\\ Finalmentel a union es una base de $V$ por 
lo que forman una base del espacio total. \\ \qed \end{demo}
\begin{prop}
  Se $T : V \to V$ con $\mathcal{B}=\{v_1, \dots ,v_n \}$ una base de autovectores con autovalores $\lambda_1, \dots ,\lambda_n$. \\\\ Entonces $Nu(T) = \langle v_i : \lambda_i = 0 \rangle $ e $Im(T)=\langle v_i : \lambda_i \neq 0 \rangle $.
\end{prop}
\begin{demo}
  Recordemos: tal que $|lambda_i = 0$ para $1 \leq i \leq k $ y $\lambda_i \neq 0$ para $ k < 1 \leq n$. \[
    v \in V \quad \Rightarrow \quad v=x_1 v_1 + \cdots + x_k v_k + x_{k+1} v_{k+1}+ \cdots + x_n v_n ,
  \]
  y entonces \begin{equation}
    T(v)=\lambda_{k+1}x_{k+1}v_{k+1}+\cdots + \lambda_{n}x_nv_n. \tag{1}
  \end{equation}
  Luego \[
    \begin{aligned}
      T(v)=0 \quad &\Leftrightarrow \quad x_{k+1} = \cdots = x_n = 0 \quad \Leftrightarrow \quad v=x_1 v_1 + \cdots + x_kv_k \\
                   &\Leftrightarrow \quad v \in \langle v_i : \lambda_i = 0 \rangle .
    \end{aligned}
  \]
  Tambien es claro por la ecuacion (1) que \[
    \begin{aligned}
      Im(T) &= \{\lambda_{k+1}x_{k+1}v_{k+1}+ \cdots + \lambda_{n}x_{n}v_n : x_i \in \mathbb{K}\} \\ &= \{\mu_{k+1}v_{k+1}+\cdots + \mu_nv_n : \mu_i \in \mathbb{K}\} \\
            &= \langle v_i : \lambda_i \neq 0 \rangle .
    \end{aligned}
  \]
  \qed
\end{demo}
\begin{defi}
  Sea $V$ un espacio vectorial de dimension finita y $T : V \to V$ una transformacion lineal. El polinomio caracteristico de $T$ es el polinomio caracteristico de la matriz $\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}$ donde $\mathcal{B}$ es una base de $V$. Es decir, \[
  \chi_{T}(x)=det\left(\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}-x \; Id\right)
\]
\end{defi}
\begin{obs}
  Notar que no importa que base usemos para calcular el polinomio caracteristico dado que $\begin{bmatrix}T\end{bmatrix}_{B'}=P^{-1}\begin{bmatrix}T\end{bmatrix}_{\mathcal{B}}P$.
\end{obs}
\begin{prop}
  Sea $T : V \to V$ una transformacion lineal. Entonces $\lambda v $ es autovalor de $T$ si y solo si $\lambda$ es raiz del polinomio caracteristico.
\end{prop}
\begin{corol}
  Sea $T : V \to V$ una transformacion lineal. Supongamos que \[
    \chi_{T}(x)=(-1)^n(x-\lambda_1)^{d_1}\cdots (x-\lambda_m)^{d_{m}}
  \]
  Entonces \begin{enumerate}[label=(\arabic*)]
    \item $1 \leq dim \; V_{\lambda_{i}} \leq d_i$ para todo $i$. 
    \item $T$ es diagonalizable si y solo si $Dim \; V_{\lambda_{i}}=d_i$ para todo $i$.
  \end{enumerate}
\end{corol}
El punto de partida de esta seccion es la siguiente simple observacion. \begin{obs}
  Sea $T : V \to W$ una transformacion lineal. Si conocemos cuanto vale $T(v_i)$ para todos los vectores de una base $\mathcal{B}=\{v_1, \dots ,v_n\}$ de $V$, entonces podemos calcular $T(v)$ para todo $v \in V$.\\\\ Pues al ser $\mathcal{B}$ una base, si $v \in V$ entonces $v= x_1v_1 + \cdots + x_n v_n$ y por lo tanto \[
T(v)=x_1T(v_1) + \cdots + x_n T(v_n)  \]
Mas aun, una transformacion queda determinada por cuanto vale en una base.
\end{obs}\pagebreak

\begin{teo}
  Sea $V$ un espacio vectorial con base $\mathcal{B}=\{v_1, \dots ,v_n\}.$ \\\\ Sea $W$ un espacio vectorial y $\{w_1, \dots ,w_n\}$ vectores de $W$. \\\\ Entonces existe una unica transformacion lineal $T : V \to W$ tal que $T(v_i)=w_i$ para todo $i$. \\\\ Notar que los $w_i$'s pueden ser cualesquiera vectores y se pueden repetir. \\\\ Este teorema nos permite construir transformaciones lineales con propiedades especificas (ver los ejercicios del Practico 8)
\end{teo}
``Se extiende $T$ por linearidad.''
\begin{obs}
  Lo que nos dice el teorema es que para definir una transformacion lineal es suficiente que definamos cuanto vale en una base. \\\\ Si bien estamos acostumbrados a definir funfiones usando formulas en el caso de las transformaciones lineales no es necesario. \\\\ Si quisieramos dar una formula podemos usar las coordenadas en la base. \\\\Y si estamos en $\mathbb{R}^n$ podemos usar las coordenadas usuales usando la matriz de la transformacion y el teorema que nos dice como se relacionan las matrices en distintas bases.
\end{obs}
\begin{teo}
  Sea $A$ matriz $n \times n$ en $\mathbb{R}$ simetrica. Sea $A : \mathbb{R}^n \to \mathbb{R}^n$ definida $A(v)=Av$. Entonces $A$ es diagonalizable.
\end{teo}
(Se usa el teorema fundamental del Algebra: 
Si $p(x)$ polinomio a coeficientes en $\mathbb{C}$, $p(x)$ tiene al menos una raiz.)
\section{Extensiones lineales.}
El punto de partida de esta seccion es la siguiente simple observacion que ya hemos notado anteriormente. \begin{obs}
 Sea $T : V \to W$ una transformacion lineal. Si conocemos cuanto vale $T(v_i)$ para todos los vectores de una base $\mathcal{B}=\{v_1, \dots ,v_n\}$ de $V$, entonces podemos calcular $T(v)$ para todo $v \in V$.
\end{obs}
Pues al ser $\mathcal{B}$ una base, si $v \in V$ entonces $v=x_1 v_1 + \cdots + x_n v_n$ y por lo tanto \[
  T(v)=x_1 T(v_1) + \cdots + x_n T(v_n).
\]
¿De cuantas maneras puedo escribir a $v$ como combinacion lineal de los vectores de la base $\mathcal{B}$?\pagebreak

Mas aun, una transformacion queda univocamente determinada por cuanto vale en una base. \begin{teo}
 Sea $V$ u nespacio vectorial con base $\mathcal{B}=\{v_1, \dots ,v_n \}$. \\\\ Sea $W$ un espacio vectorial y $\{w_1, \dots ,w_n\}$ vectores de $W$. \\\\ Entonces existe una unica transformacion lineal $T : V \to W$ tal que $T (v_i) = w_i$ para todo $i$.
\end{teo}
Notar que los $w_i$'s pueden ser cualesquiera vectores y se pueden repetir. (No es necesario que sean una base. Pueden ser todos el vector cero.)\\\\ Este teorema nos permite construir transformaciones lineales con propiedades especificas (ver los ejercicios del Practico 8) 
\begin{demo}
  Si $ v \in V$, entonces existen unicos excalares $x_1, \dots ,x_n$ (las coordenadas) tal que \[
v=x_1v_1 + \cdots + x_n v_n 
  \]
  pues $\mathcal{B}=\{v_1,\dots ,v_n\}$ es una base de $V$. Entonces podemos definir la funcion $T : V \to W$ asi \[
T(v)=x_1w_1 + \cdots + x_n w_n.
  \]
  Esta funcion resulta ser una transformacion lineal porque las coordenadas respetan las operaciones de suma y producto por escalares.  \\\\ Claramente, $T(v_1)=w_1, T(v_2)=w_2, \dots , T(v_n)=w_n$.
\end{demo}
\qed
\begin{ej}
  Si tomamos $V = \mathbb{K}^3$ con la base canonica $\mathcal{C}=\{e_1, e_2, e_3\}$, \\\\ $W=\mathbb{K}^3$ y $\{(1,2,3),(-1,0,5),(-2,3,1)\}$ entonces existe una unica transformacion lineal $T : \mathbb{K}^3 \to \mathbb{K}^3$ tal que \[
T(e_1)=(1,2,3), \quad T(e_2) =(-1,0,5), \quad T(e_3) = (-2,3,1)
  \]
\end{ej}

Dado $(x,y,z) \in \mathbb{K}^3$ ¿Cuanto vale $T(x,y,z)$? \\\\ Para responder a esta pregunta usar las coordenadas como en la demostracion del teorema. En este caso \[
  (x,y,z)=xe_1 + ye_2 + ze_3.
\]
Entonces \[
  \begin{aligned}
    T(x,y,z) &= T(xe_1+ye_2+ze_3) \\
             &= xT(e_1) +yT(e_2)+zT(e_3) \\
             &= x(1,2,3)+y(-1,0,5)+z(-2,3,1) \\
             &= x(1,2,3) + y(-1,0,5)+z(-2,3,1) \\
             &= (x-y-2z,2x+3z,3x+5y+z).
  \end{aligned}
\]
\begin{ej}
  Si tomamos $V = \mathbb{K}^3$ con la base canonica $\mathcal{C}= \{e_1, e_2, e_3\}$, \\ $W =\mathbb{K}^3$ y $\{(1,2,3),(-1,5,5),(-2,3,1)\}$ entonces existe una unica transformacion lineal $T : \mathbb{K}^3 \to \mathbb{K}^3$ tal que \[
T(e_1)=(1,2,3), \quad T(e_2)=(-1,0,5), \quad T(e_3) = (-2,3,1).
  \]
  Explicitamente esta transformacion lineal es dada por la formula: \[
T(x,y,z)=(x-y-2z,2x+3z,3x+5y+z).
  \]
  Tambien podriamos definirla como la multiplicacion por \[
    A=\begin{bmatrix}T\end{bmatrix}_{\mathcal{C}}=\begin{bmatrix}1 & -1 & -2 \\ 2 & 0 & 3 \\ 3 & 5 & 1 \end{bmatrix}
  \]
\end{ej}
\begin{ej}
  Si tomamos $V = \mathbb{K}^3$ con la base $\mathcal{B}=\{(1,1,1),(0,1,1),(0,0,1)\}$, \\$W= \mathbb{K}^3$ y $\{(1,2,3) ,(-1,0,5),(-2,3,1)\}$ entonces existe una unica transformacion lineal $T : \mathbb{K}^3 \to \mathbb{K}^3$ tal que \[
T(1,1,1)=(1,2,3), \quad T(0,1,1)=(-1,0,5), \quad T(0,0,1)=(-2,3,1).
  \]
\end{ej}
Dado $(x,y,z) \in \mathbb{K}^3$ ¿Cuanto vale $T(x,y,z)?$ \\\\ Para responder a esta pregunta usar las coordenadas como en la demostracion del teorema. Lo que queremos es la matriz de $T$ en la base canonica. \\\\ Por la teoria sabemos que \[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{C,C}}=\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B,C}} \begin{bmatrix}T\end{bmatrix}_{\mathcal{B,B}} \begin{bmatrix}Id\end{bmatrix}_{\mathcal{C,B}} =\begin{bmatrix}T\end{bmatrix}_{\mathcal{B,C}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B,C}}^{-1}
\]
\[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{B,C}}=\begin{bmatrix}[rrrrr]1 & -1 & -2 \\ 2 & 0 & 3 \\ 3 & 5 & 1 \end{bmatrix}
\]
\[
  \begin{bmatrix}Id\end{bmatrix}_{\mathcal{C,B}}=\begin{bmatrix}1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{bmatrix}^{-1}=\begin{bmatrix}[rrrrr]1 & 0 & 0 \\ -1 & 1 & 0 \\ 0 & -1 & 1 \end{bmatrix} 
\]
Entonces \[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{C,C}}=\begin{bmatrix}T\end{bmatrix}_{\mathcal{B,C}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B,C}}^{-1}=\begin{bmatrix}[rrrrr]2 & 1 & -2 \\ 2 & -3 & 3 \\ -2 & 4 & 1 \end{bmatrix}
\]
Por lo tanto \[
  T(x,y,z)=\begin{bmatrix}[rrrr]2 & 1 & -2 \\ 2 & -3 & 3 \\ -2 & 4 & 1 \end{bmatrix}\begin{bmatrix}x \\ y \\ z \end{bmatrix}= \begin{bmatrix}2x+y-2z \\2x-3y+3y \\ -2x+4y+z \end{bmatrix}
\]
Podemos verificar que esta transformacion satisface que \[
T(1,1,1)=(1,2,3),T(0,1,1)=(-1,0,5),T(0,0,1)=(-2,3,1).
\]
\begin{ej}
Dar una transformacion lineal $T : \mathbb{K}^2 \to \mathbb{K}^2$ tal que \\ $(1,1) \in Nu(T)$ y $(1,2) \in Im(T)$. \\\\
Para resolver este tipo de problemas podemos recurrir al Teorema 75 para ello debemos verificar la hipotesis del mismo.
\end{ej}
Primero, necesitamos elegir una base del espacio de salida. Como una de las condiciones es que $(1,1) \in Nu(T)$ elegimos una base que contenga a este vector. Por ejemplo, $\mathcal{B}=\{(1,1),(0,1)\}$ es una base de $\mathbb{K}^2$. \\\\ Luego elegimos los vectores del espacio de llegada. Como la otra condicion es que $(1,2) \in Im(T)$ eegimos $\{(0,0),(1,2)\}$.\\\\ Entonces el Teorema 75 nos dice que existe una unica transformacion lineal $T : \mathbb{K}^2 \to \mathbb{K}^2$ tal que \[
T(1,2) =(0,0), \quad T(0,1)=(1,2).
\]
Esta transformacion satisface lo que queriamos. \\\\ Si queremos una formula mas explicita de esta transformacion calculamos la matriz de $T$ e nla base canonica como lo hicimos en el ejemplo anterior. \\\\ Explicitamente, \[
  \begin{bmatrix}T\end{bmatrix}_{\mathcal{C,C}}=\begin{bmatrix}T\end{bmatrix}_{\mathcal{B,C}}\begin{bmatrix}Id\end{bmatrix}_{\mathcal{B,C}}^{-1}=\begin{bmatrix}0 & 1 \\ 0 & 2 \end{bmatrix}\begin{bmatrix}1 & 0 \\ 1 & 1 \end{bmatrix}^{-1}=\begin{bmatrix}[rr] -1 & 1 \\ -2 & 2 \end{bmatrix}
\]
Entonces la transformacion $T(x,y)=(-x+y,-2x+2y)$ satisface que \[
  (1,1)\in Nu(T), \quad (1,2) \in Im(T)
\]
\qed\\\\
\textbf{Problema}\\\\ Dar una transformacion lineal $T : \mathbb{K}^2 \to \mathbb{K}^2$ tal que $(1,1)\in Nu(T)$ y $ (1,2) \in Im(T)$. \\\\
\textbf{Una respuesta es} \\\\ $T(x,y)=(-x+y,-2x+2y)$ \begin{obs}
  Existen infinitas transformaciones con la propiedad solicitada. Esto es asi porque en el transcurso de la construccion hemos hecho varias elecciones. Si cambiamos nuestra eleccion construiremos otra transformacion lineal. \pagebreak
  
  Por ejemplo, podriamos elegir la base $\mathcal{B}=\{(1,1),(1,-1)\}$ y como conjunto de llegada $\{(0,0),(2,4)\}$. Luego, por el Teorema 75, existiria una unica transformacion lineal tal que \[
T(1,1)=(0,0), \quad T(1,-1)=(2,4).
  \]
  Entonces $(1,1) \in Nu(T)$ y $(1,2)=\frac{1}{2}T(1,-1)\in Im(T)$. \\\\ La otra transformacion que construimos es la unica que satisface $T(1,1)=(0,0)$ y $T(0,1)=(1,2)$.
\end{obs}
\begin{obs}
  Lo que nos dice el teorema es que para definir una transformacion lineal es suficiente que definamos cuanto vale en una base. \\\\ Si bien estamos acostumbrados a definir funciones usando formulas en el caso de las transformaciones lineales no es necesario. \\\\ Si quisieramos dar una formula podemos usar las coordenadas en la base. \\\\ Y si estamos en $\mathbb{K}^n$ podemos usar las coordenadas usuales usando la matriz de la transformacion y el teorema que nos dice como se relacionan las matrices en distintas bases.
\end{obs}
\end{document} 
